{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86cd4440",
   "metadata": {},
   "source": [
    "# **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #GENERIC\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from random import random\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from joblib import load\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "    #EIKON SPECIFIC\n",
    "import eikon as ek\n",
    "import refinitiv.dataplatform as rdp \n",
    "    #DATA EXPLORATION SPECIFIC\n",
    "import holidays\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "    #PRE PROCESSING\n",
    "import re\n",
    "import nltk  \n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "    #Embedding + Classification\n",
    "from gensim.models import KeyedVectors\n",
    "import operator\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "    #Investment\n",
    "from itertools import product\n",
    "from scipy.stats import kurtosis, skew, norm, chi2\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09adca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change Directory to the OS Folder\n",
    "os.chdir(r\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b657c2",
   "metadata": {},
   "source": [
    "# **EIKON & WRDS Data Retrieval**\n",
    "\n",
    "Links: https://community.developers.refinitiv.com/questions/32320/how-to-set-sources-for-get-news-headlines.html || https://community.developers.refinitiv.com/questions/28201/how-to-get-list-of-leaver-companies-of-the-asx-100.html || "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181e743",
   "metadata": {},
   "source": [
    "## **Credential configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to eikon with acess key\n",
    "with open('0)Eikon_Credentials/Eikon_Api_Key_1.txt', 'r') as f:\n",
    "    api_key = f.read()\n",
    "ek.set_app_key(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1666acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to eikon with acess key (Library)\n",
    "with open('0)Eikon_Credentials/Eikon_Library_Api_Key.txt', 'r') as f:\n",
    "    api_library_key = f.read()\n",
    "rdp.open_desktop_session(api_library_key) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa525b",
   "metadata": {},
   "source": [
    "## **S&P Historical List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b33647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current S&P List\n",
    "constituents_fields = ['TR.IndexConstituentRIC','TR.IndexConstituentName','TR.IndexConstituentComName']\n",
    "Current_SP, e = ek.get_data(['.SPX'], fields=constituents_fields, parameters={'SDate':'2022-04-01'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes to S&P\n",
    "SP_changes_fields = ['TR.IndexJLConstituentChangeDate','TR.IndexJLConstituentRIC.change','TR.IndexJLConstituentName', 'TR.IndexJLConstituentRIC','TR.IndexJLConstituentComName']\n",
    "SP_changes_df, e = ek.get_data(['.SPX'], fields=SP_changes_fields, parameters={'SDate':'0D', 'EDate':'-2CY', 'IC':'B'})\n",
    "SP_changes_df[\"Date\"] =  pd.to_datetime(SP_changes_df[\"Date\"]).apply(lambda x: x.date())\n",
    "SP_changes_df = SP_changes_df[(pd.to_datetime(SP_changes_df[\"Date\"])<=datetime(2022,4,1)) & (pd.to_datetime(SP_changes_df[\"Date\"])>=datetime(2020,9,1))].copy()\n",
    "#List of Leavers and Joiners\n",
    "leavers_RIC = list(SP_changes_df[SP_changes_df[\"Change\"]==\"Leaver\"][\"Constituent RIC\"])\n",
    "joiners_RIC = list(SP_changes_df[SP_changes_df[\"Change\"]!=\"Leaver\"][\"Constituent RIC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d687f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for repeated changes to same company\n",
    "company_change_frequency = SP_changes_df.groupby([\"Constituent RIC\", \"Constituent Name\"]).size()\n",
    "display(company_change_frequency[company_change_frequency > 1])\n",
    "display( SP_changes_df[SP_changes_df[\"Constituent RIC\"]==\"ZIMV.OQ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f25059",
   "metadata": {},
   "source": [
    "> **Notes**\n",
    "\n",
    "Company changes where checked for the cases in which a company changed multiple times and found to be companies that entered for a short time in the S&P and then left. So no instances of a company exiting and entering multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f9bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historical S&P (by joining list of changes and current constituents)\n",
    "SP_changes_df_noduplicates = SP_changes_df[[\"Constituent RIC\",\"Constituent Name\"]].drop_duplicates()\n",
    "Historical_SP = pd.concat([Current_SP[[\"Constituent RIC\",\"Constituent Name\"]], SP_changes_df_noduplicates], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbda21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Duplicate RIC\n",
    "list_duplicates = list(Historical_SP[Historical_SP.duplicated(\"Constituent RIC\")][\"Constituent RIC\"].unique())\n",
    "for dup in list_duplicates:\n",
    "    #print(Historical_SP[Historical_SP[\"Constituent RIC\"]==dup])\n",
    "    pass\n",
    "    \n",
    "#Drop duplicates\n",
    "Historical_SP.drop_duplicates(subset=[\"Constituent RIC\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e17a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for repeated changes to same company (to highlight inconsistencies)\n",
    "company_frequency = Historical_SP.groupby([\"Constituent RIC\", \"Constituent Name\"]).size()\n",
    "display(company_frequency[company_frequency > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52436335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check NANS\n",
    "display(Historical_SP[Historical_SP['Constituent RIC'].isna()])\n",
    "#Correct NANS by hand\n",
    "Historical_SP.loc[266, \"Constituent RIC\"] = \"EMBC.O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abcb4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (timeline as defined by user)\n",
    "recent_date_chosen = datetime(2022,4,1)\n",
    "oldest_date_chosen = datetime(2020,9,1)\n",
    "#REgister times of joining and leaving\n",
    "Historical_SP[\"leave\"] = Historical_SP.apply(lambda x : SP_changes_df[ (SP_changes_df[\"Constituent RIC\"]==x[\"Constituent RIC\"]) & (SP_changes_df[\"Change\"]==\"Leaver\") ][\"Date\"].values[0] if x[\"Constituent RIC\"] in leavers_RIC else recent_date_chosen, axis=1)\n",
    "Historical_SP[\"join\"] = Historical_SP.apply(lambda x : SP_changes_df[ (SP_changes_df[\"Constituent RIC\"]==x[\"Constituent RIC\"])  & (SP_changes_df[\"Change\"]==\"Joiner\") ][\"Date\"].values[0] if x[\"Constituent RIC\"] in joiners_RIC else oldest_date_chosen, axis=1)\n",
    "#Save Excel version\n",
    "#Historical_SP.to_excel(\"1)Eikon_Data/Historical_SP_Constituents.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(Historical_SP))\n",
    "Historical_SP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa204a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## **Request Headlines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43223693",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Historical_SP_Full = pd.read_excel(\"1)Eikon_Data/Historical_SP_Constituents.xlsx\", index_col=0) \n",
    "display(Historical_SP_Full.shape)\n",
    "display(Historical_SP_Full.head(2))\n",
    "    #Changes (according to needs)\n",
    "    #Historical_SP = Historical_SP_Full[(Historical_SP_Full[\"Constituent RIC\"]!=\"MCO.N\") & (Historical_SP_Full[\"Constituent RIC\"]!=\"IVZ.N\")].copy()\n",
    "display(Historical_SP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ba78a",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    #Current date\n",
    "datestring_time = datetime.strftime(datetime.now(),\"%m_%d_%Y__%H_%M\")\n",
    "start_time = time.time()#TIMER of code\n",
    "safety_start_time = time.time() #Safety TIMER \n",
    "\n",
    "company_headlines = []\n",
    "for index, row in tqdm(Historical_SP.iterrows(), total=Historical_SP.shape[0]):\n",
    "    #(auxiliary)\n",
    "    retriving_data =  True # reser individual company loop \n",
    "    company_request_loop = 0 # tracer reset\n",
    "    #Setting up timeline of data to retrieve (based on leave and join datess and if not existing fixed dates)\n",
    "    join_date = row[\"join\"]\n",
    "    leave_date = row[\"leave\"]\n",
    "    print(\"Start Date\",join_date,\"  End Date\",leave_date)\n",
    "    while retriving_data :   #Loop to bypass request limit\n",
    "        try:             \n",
    "            #Timer to avoid overload of requests\n",
    "            time.sleep(random() * 3)\n",
    "            # Make request of news (Note: 100 is maximum per request)\n",
    "            df = ek.get_news_headlines('R:'+str(row[\"Constituent RIC\"])+' AND Language:LEN', date_from = str(join_date), date_to=str(leave_date), count=100)\n",
    "            #Add RIC to headline retrieved\n",
    "            df[\"Constituent RIC\"]= row[\"Constituent RIC\"]\n",
    "            df[\"Constituent Name\"]= row[\"Constituent Name\"]\n",
    "            # Add healines to main list\n",
    "            company_headlines.append(df)\n",
    "            company_request_loop +=1 #tracker  add       \n",
    "            # Condition to break loop (if either no news available anymore or if the oldest date matches the start date of the timeline stipulated)\n",
    "            if (df.shape[0]==0) or (leave_date == df.tail(1).index.item()):\n",
    "                retriving_data=False\n",
    "            else:\n",
    "                # change date of the request of news to end when the existing data already retrieved beggins\n",
    "                leave_date = df.tail(1).index.item() - timedelta(milliseconds=1)\n",
    "                \n",
    "        except Exception as exc:\n",
    "            print(f\"Encountered an unknown error: {exc}\")\n",
    "            pd.concat(company_headlines).to_csv(\"1)Eikon_Data/\" + datestring_time + \"_Eikon_headlines.csv\")\n",
    "            print(f\"Current end date is: {leave_date}\")\n",
    "            \n",
    "            #Safety timer every 2 hours asks if code is to be kept running or not (to account for timeout errors from eikon)\n",
    "            lapsed_time = time.time() - safety_start_time\n",
    "            if lapsed_time>=3600:\n",
    "                safety_start_time = time.time()\n",
    "                safety_question = input(\"Keep the code running?\")\n",
    "            \n",
    "    # Traker of iterations\n",
    "    print(\"Done with company:\", str(row[\"Constituent RIC\"]),\"total loops:\",str(company_request_loop))\n",
    "    \n",
    "#Savind data\n",
    "pd.concat(company_headlines).to_csv(\"1)Eikon_Data/\" + datestring_time + \"_Eikon_headlines.csv\")\n",
    "print(\"Code ran for --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f758a6f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Check Data Retrieved (& Combine hadlines from diferent retrieval processes)\n",
    "EIKON_Headlines = pd.DataFrame()\n",
    "data_files_list = os.listdir(\"1)Eikon_Data/Headlines_partitioned_versions/\")\n",
    "for file_name in data_files_list:\n",
    "    temp_df = pd.read_csv(\"1)Eikon_Data/Headlines_partitioned_versions/\" + file_name) \n",
    "    print(f\" \\t File {file_name} has a shape of {temp_df.shape}\")\n",
    "    display(temp_df.head(2))\n",
    "    EIKON_Headlines = pd.concat([EIKON_Headlines, temp_df])\n",
    "display(EIKON_Headlines.head(5))\n",
    "#(cleaning)\n",
    "EIKON_Headlines.drop_duplicates(inplace=True)\n",
    "#EIKON_Headlines = EIKON_Headlines[(EIKON_Headlines[\"Unnamed: 0\"]>=\"2020-12-31\") & (EIKON_Headlines[\"Unnamed: 0\"]<=\"2022-04-01\")].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de1892",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## **Request Lookup Table with Ticker, RIc,..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2a2d1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Get symbols equivalence\n",
    "Historical_SP_Full_0 = pd.read_excel(\"1)Eikon_Data/Historical_SP_Constituents.xlsx\", index_col=0) \n",
    "RIC_list = list(Historical_SP_Full_0[\"Constituent RIC\"].unique())\n",
    "symbols_df = rdp.convert_symbols(RIC_list, from_symbol_type=rdp.SymbolTypes.RIC)\n",
    "display(symbols_df.head(5))\n",
    "#Add Relevant Data to Existing Historical S&P table (CUSIP)\n",
    "lookup_symbol_table = Historical_SP_Full_0.merge(symbols_df[[\"RIC\",\"TickerSymbol\",\"CUSIP\"]], how=\"left\", left_on=\"Constituent RIC\", right_on=\"RIC\")\n",
    "lookup_symbol_table.drop_duplicates(inplace=True)\n",
    "#lookup_symbol_table[[\"RIC\",\"TickerSymbol\",\"CUSIP\",\"Constituent Name\"]].to_excel(\"1)Eikon_Data/Symbols_lookup.xlsx\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d6c9d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* WRDS Library of Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1df680",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "WRDS_raw_data = pd.read_csv(\"2)WRDS_Data/WRDS_symbols_vlookup.csv\")\n",
    "    # Type, Col name and other corretion\n",
    "WRDS_raw_data[\"date\"] = pd.to_datetime(WRDS_raw_data[\"date\"].astype(str))   \n",
    "WRDS_raw_data.rename(columns={'PERMNO': 'PERMNO_W', 'date': 'Date', 'TICKER': 'Ticker_W', 'COMNAM': 'Company_W','CUSIP':'CUSIP_W'}, inplace=True)\n",
    "WRDS_raw_data.drop_duplicates(subset=[\"Ticker_W\",\"Company_W\",\"CUSIP_W\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af029d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Check changes in ticker & cusip combo (merge points)\n",
    "ticker_cusip_frequency = WRDS_raw_data.groupby([\"Ticker_W\",\"CUSIP_W\"]).size()\n",
    "display(ticker_cusip_frequency[ticker_cusip_frequency > 1])\n",
    "display( WRDS_raw_data[WRDS_raw_data[\"Ticker_W\"]==\"AFK\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798f4da",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#WRDS_raw_data.to_excel(\"2)WRDS_Data/WRDS_symbols_vlookup.xlsx\", index=0)\n",
    "    # Note: Some company name changes occourred, after merging data check relevant S&P cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ad9af",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Check Against WRDS ticker & CUSIP (VERSION 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd78ef22",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #Check if tickers match the ones in WRDS\n",
    "        #wrds data\n",
    "WRDS_symbols_df = pd.read_excel(\"2)WRDS_Data/WRDS_symbols_vlookup.xlsx\") \n",
    "WRDS_symbols_df[\"CUSIP_W\"] = WRDS_symbols_df[\"CUSIP_W\"].astype(str)\n",
    "        #eikon (SP) data\n",
    "symbols_df = pd.read_excel(\"1)Eikon_Data/Symbols_lookup__ManualRevision.xlsx\") \n",
    "symbols_df = symbols_df.rename(columns={'CUSIP': 'CUSIP_E', 'Constituent Name': 'Company_E','RIC':'RIC_E', 'TickerSymbol': 'Ticker_E'})\n",
    "\n",
    "    #Filtering Data (based on ticker & cusip)  - Only to make code faster\n",
    "#SP_ticker_list = list(symbols_df[\"Ticker_E\"].unique())\n",
    "#SP_cusip_list = [str(cusip)[:8] for cusip in symbols_df[\"CUSIP_E\"].unique()]\n",
    "#WRDS_symbols_SP = WRDS_symbols_df[(WRDS_symbols_df[\"Ticker_W\"].isin(SP_ticker_list)) | (WRDS_symbols_df.apply(lambda x: x[\"CUSIP_W\"] in SP_cusip_list,axis=1))].copy()\n",
    "\n",
    "    #Aggregating the RIC based on ticker and CUSIP\n",
    "        #adapting cusip (to same length as WRDS)\n",
    "symbols_df_w_cussip_header = symbols_df.copy()\n",
    "symbols_df_w_cussip_header[\"CUSIP_E\"] = symbols_df_w_cussip_header.apply(lambda x: str(x[\"CUSIP_E\"])[:8], axis=1) \n",
    "        #aggregating\n",
    "merged_symbols = symbols_df_w_cussip_header.merge(WRDS_symbols_df, how=\"left\", left_on=[\"Ticker_E\",\"CUSIP_E\"], right_on=[\"Ticker_W\",\"CUSIP_W\"])\n",
    "merged_symbols = merged_symbols[['RIC_E', 'Ticker_E', 'Ticker_W', 'CUSIP_E', 'CUSIP_W', 'Company_E','Company_W']]\n",
    "    #Column signaling match of values (ticker & cusip)\n",
    "merged_symbols[\"CUSIPTicker_Match\"] = np.where(((merged_symbols[\"Ticker_E\"]==merged_symbols[\"Ticker_W\"]) & (merged_symbols[\"CUSIP_E\"]==merged_symbols[\"CUSIP_W\"])),1,0)\n",
    "    #Checking no match\n",
    "merged_symbols[merged_symbols[\"CUSIPTicker_Match\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60621c7d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Save Ticker list\n",
    "SP_ticker_list = list(symbols_df[\"Ticker_E\"].unique())\n",
    "with open('2)WRDS_Data/tickers_for_returns_wrds.txt', 'w') as f:\n",
    "    for item in SP_ticker_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff1841",
   "metadata": {},
   "source": [
    "## GICS Sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44094174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get RIC symbols\n",
    "symbols_df = pd.read_excel(\"1)Eikon_Data/Symbols_lookup__ManualRevision.xlsx\") \n",
    "print(\"Shape of Eikon Symbol Lookup Table\", symbols_df.shape)\n",
    "#For each get corresponding sector\n",
    "GICS_df = pd.DataFrame()\n",
    "for ric in list(symbols_df[\"RIC\"].unique()):\n",
    "    df, err = ek.get_data(ric,\n",
    "                      ['TR.RICCode',\n",
    "                      'TR.CompanyName',\n",
    "                      'TR.GICSSector', \n",
    "                      'TR.GICSIndustryGroup'])\n",
    "    GICS_df = pd.concat([GICS_df,df])\n",
    "display(GICS_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ef57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GICS_df.to_excel(\"1)Eikon_Data/Sectors_lookup.xlsx\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe54e39",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## RETURNS (Wrds)\n",
    "\n",
    "Return Data > https://wrds-www.wharton.upenn.edu/\n",
    "\n",
    "Using the List of Tickers Compiled in 2.4.1 make a request for the relevant timeline (with some leeway) + quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564409ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Get Return Data (& eikon revised symbol data to check against)\n",
    "WRDS_RET_Raw_data = pd.read_csv(\"2)WRDS_Data/WRDS_returns_RAW.csv\") \n",
    "EIKON_symbols_check = pd.read_excel(\"1)Eikon_Data/Symbols_lookup__ManualRevision.xlsx\") \n",
    "print(\"WRDS Raw Data Size\", WRDS_RET_Raw_data.shape)\n",
    "\n",
    "#Safety Measures and Corrections for Comparability betwenn Eikon Wrds\n",
    "    #(adjust CUSIP of Eikon to same Wrds Size)\n",
    "EIKON_symbols_check[\"CUSIP\"] = EIKON_symbols_check.apply(lambda x: str(x[\"CUSIP\"])[:8], axis=1) \n",
    "\n",
    "#Merged Table\n",
    "WrdsRet_EikonSymbol = WRDS_RET_Raw_data.merge(EIKON_symbols_check[[\"RIC\",\"TickerSymbol\",\"CUSIP\",\"Constituent Name\"]], how=\"left\", left_on=[\"TICKER\",\"CUSIP\"], right_on=[\"TickerSymbol\",\"CUSIP\"])\n",
    "WrdsRet_EikonSymbol = WrdsRet_EikonSymbol[~WrdsRet_EikonSymbol.isnull()[[\"RIC\",\"TickerSymbol\",\"CUSIP\",\"TICKER\"]].any(axis=1)].copy()\n",
    "print(\"WRDS Clean Data Size\", WrdsRet_EikonSymbol.shape)\n",
    "print(\"TIckers in Returns Not in S&P\", list(set(WRDS_RET_Raw_data[\"TICKER\"]) - set(EIKON_symbols_check[\"TickerSymbol\"])))\n",
    "print(\"\")\n",
    "\n",
    "# Calculating Market Value  (outstanding shares*price)\n",
    "WrdsRet_EikonSymbol[\"MktCap\"] = WrdsRet_EikonSymbol[\"SHROUT\"]*WrdsRet_EikonSymbol[\"PRC\"]\n",
    "    #(check nans)\n",
    "nan_chcek_mkt = WrdsRet_EikonSymbol[WrdsRet_EikonSymbol.isnull()[[\"MktCap\"]].any(axis=1)] \n",
    "print(\"Mkt Cap nan check\", len(nan_chcek_mkt))\n",
    "display(nan_chcek_mkt)\n",
    "#Note: Fill nans with last valid observations and if not available thenext valid one \n",
    "    #(correct nan)\n",
    "WrdsRet_EikonSymbol.sort_values(by=['RIC','date'], inplace=True)\n",
    "WrdsRet_EikonSymbol[\"MktCap\"] = WrdsRet_EikonSymbol.groupby(\"RIC\")[\"MktCap\"].transform(lambda x: x.fillna(method=\"ffill\"))\n",
    "WrdsRet_EikonSymbol[\"MktCap\"] = WrdsRet_EikonSymbol.groupby(\"RIC\")[\"MktCap\"].transform(lambda x: x.fillna(method=\"bfill\"))\n",
    "print(\"Mkt Cap nan check\", len(WrdsRet_EikonSymbol[WrdsRet_EikonSymbol.isnull()[[\"MktCap\"]].any(axis=1)] ))\n",
    "\n",
    "#Save Clean Data\n",
    "WrdsRet_EikonSymbol.to_csv(\"2)WRDS_Data/WRDS_returns_Clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc70666",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check not keppt values\n",
    "not_keept = pd.concat([WRDS_RET_Raw_data,WrdsRet_EikonSymbol[list(WRDS_RET_Raw_data.columns)]]).drop_duplicates(keep=False)\n",
    "print(\"Statistics on return data drooped\")\n",
    "print(\"\\t Companies\")\n",
    "display(pd.DataFrame(not_keept.groupby([\"TICKER\"]).size()))\n",
    "print(\"\\t Date\")\n",
    "display(pd.DataFrame(not_keept.groupby([\"date\"]).size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edc946",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check companies and date availability\n",
    "Wrds_Companies_Available = WrdsRet_EikonSymbol.groupby([\"RIC\",\"TICKER\",\"CUSIP\",\"COMNAM\",\"Constituent Name\"]).agg(\n",
    "                                                        **{'Max date':pd.NamedAgg(column='date', aggfunc='max'),\n",
    "                                                        'Min date':pd.NamedAgg(column='date', aggfunc='min'),})\n",
    "unique_wrds_companies = Wrds_Companies_Available.reset_index()[\"TICKER\"].unique()\n",
    "unique_eikon_companies = EIKON_symbols_check[\"TickerSymbol\"].unique()\n",
    "print(f\"Companies Available {len(unique_wrds_companies)}  &  Required {len(unique_eikon_companies)} \")\n",
    "print()\n",
    "# Check missing companies\n",
    "print(\" \\t Missing Ticker\")\n",
    "print(set(EIKON_symbols_check[\"TickerSymbol\"]) - set(Wrds_Companies_Available.reset_index()[\"TICKER\"]))\n",
    "display(Wrds_Companies_Available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2deb04a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check Extreme dates available against the Ones needed\n",
    "    #Relevant DaAta\n",
    "Historical_SP_Full = pd.read_excel(\"1)Eikon_Data/Historical_SP_Constituents.xlsx\", index_col=0) #leave/join dates\n",
    "Wrds_Companies_Available_Checks = Wrds_Companies_Available.reset_index().merge(Historical_SP_Full[['Constituent RIC', 'leave', 'join']], how=\"left\", left_on=[\"RIC\"], right_on=[\"Constituent RIC\"])\n",
    "    #Datetime Conversions for coparision\n",
    "Wrds_Companies_Available_Checks[['Max date', 'Min date']] = Wrds_Companies_Available_Checks[['Max date', 'Min date']].apply(lambda x : pd.to_datetime(x.astype(str)))\n",
    "#Check column 1 means GOOD 0 means BAD\n",
    "Wrds_Companies_Available_Checks[\"Leave Check\"] = np.where(Wrds_Companies_Available_Checks[\"Max date\"]+timedelta(days=1)>=Wrds_Companies_Available_Checks[\"leave\"],1,0)\n",
    "Wrds_Companies_Available_Checks[\"Join Check\"] = np.where(Wrds_Companies_Available_Checks[\"Min date\"]<=Wrds_Companies_Available_Checks[\"join\"],1,0)\n",
    "#Check Cases with mismatces\n",
    "Wrds_Companies_Available_Checks[(Wrds_Companies_Available_Checks[\"Leave Check\"]!=1) | (Wrds_Companies_Available_Checks[\"Join Check\"]!=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5256e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Check specific Case\n",
    "ticker_to_check = 'AGN'\n",
    "print(\" \\t Infro From EIKON\")\n",
    "display(EIKON_symbols_check[EIKON_symbols_check[\"TickerSymbol\"].str.contains(ticker_to_check)])\n",
    "print(\" \\t Results From WRDS\")\n",
    "display(WRDS_RET_Raw_data[WRDS_RET_Raw_data[\"TICKER\"].str.contains(ticker_to_check)].drop_duplicates(subset=[\"TICKER\",\"CUSIP\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746b196",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc89ff8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Headline Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd7655",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Add Dates to Headline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac5715",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " #Dataset from Eikon\n",
    "eikon_news_data_0 = pd.read_csv(\"1)Eikon_Data/Eikon_headlines_1Set2020_31Mar2022.csv\")\n",
    "display(eikon_news_data_0.shape)\n",
    "eikon_news_data_0.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8faef3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CHECK for inconsistencies\n",
    "    #Merge Data of Headlines with entry S&P entry/exit dates\n",
    "Historical_SP_Full = pd.read_excel(\"1)Eikon_Data/Historical_SP_Constituents.xlsx\", index_col=0) #leave/join dates\n",
    "eikon_news_data_1_check = eikon_news_data_0.merge(Historical_SP_Full, how=\"left\", on=[\"Constituent RIC\"], suffixes=('_Headlines', '_SP'))\n",
    "print(\"Companies Available in NEWS\" , eikon_news_data_1_check[\"Constituent RIC\"].nunique(), \" &  Required\", Historical_SP_Full[\"Constituent RIC\"].nunique() )\n",
    "print(\" \\t Extra or Non Matching:\")\n",
    "extra_RIC = list(set(eikon_news_data_1_check[\"Constituent RIC\"].unique()) - set(Historical_SP_Full[\"Constituent RIC\"].unique()))\n",
    "missing_RIC = list(set(Historical_SP_Full[\"Constituent RIC\"].unique()) -  set(eikon_news_data_1_check[\"Constituent RIC\"].unique()))\n",
    "print(\"RICS In News not in S&P\", sorted(extra_RIC))\n",
    "print(\"RICS In S&P not in News\", sorted(missing_RIC))\n",
    "\n",
    "#Check NANs\n",
    "cols_to_check_null = [col for col in eikon_news_data_1_check.columns if col not in ['Constituent Name_Headlines', 'Constituent Name_SP']]\n",
    "nans_check_headline = eikon_news_data_1_check[eikon_news_data_1_check.isnull()[cols_to_check_null].any(axis=1)]\n",
    "print(\" \\t Companies with no correspondence\")\n",
    "RIC_list_for_adjustments = nans_check_headline.drop_duplicates(subset=[\"Constituent RIC\"])\n",
    "display(RIC_list_for_adjustments)\n",
    "\n",
    "#Save\n",
    "#RIC_list_for_adjustments[[\"Constituent RIC\",\"Constituent Name_Headlines\"]].to_excel(\"1)Eikon_Data/Headlines_RIC_for_Correction.xlsx\", index=0)\n",
    "\n",
    "#Help in checking specific cases (for the Nans)\n",
    "eikon_unique_ComRic = eikon_news_data_1_check.groupby([\"Constituent RIC\",\"Constituent Name_Headlines\"]).agg(\n",
    "                                                **{ 'Max date':pd.NamedAgg(column='versionCreated', aggfunc='max'),\n",
    "                                                    'Min date':pd.NamedAgg(column='versionCreated', aggfunc='min')}).reset_index()\n",
    "\n",
    "display(eikon_unique_ComRic[eikon_unique_ComRic[\"Constituent Name_Headlines\"].str.contains(\"VIA\")])\n",
    "display(eikon_unique_ComRic[eikon_unique_ComRic[\"Constituent RIC\"].str.contains(\"PAR\")])\n",
    "\n",
    "#check headlines of Nan  RIC\n",
    "study_df = eikon_news_data_1_check[eikon_news_data_1_check[\"Constituent RIC\"]=='HP.N']\n",
    "for index, row in study_df.iterrows():\n",
    "    print(row[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94631e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ACTUAL MERGE\n",
    "    #Data REquired\n",
    "eikon_news_data_01 = eikon_news_data_0.copy()\n",
    "Historical_SP_Full = pd.read_excel(\"1)Eikon_Data/Historical_SP_Constituents.xlsx\", index_col=0) #leave/join dates\n",
    "Headlines_Corrected_RIC = pd.read_excel(\"1)Eikon_Data/Headlines_RIC_for_Correction__ManualRevision.xlsx\") #ric corrected\n",
    "ric_for_correction = list(Headlines_Corrected_RIC[~Headlines_Corrected_RIC.isnull()[[\"Corrected_RIC\",\"Note\"]].any(axis=1)][\"Constituent RIC\"].unique())\n",
    "    #Correction to Headline Data\n",
    "eikon_news_data_01[\"Constituent RIC\"] = eikon_news_data_01.apply(lambda x: Headlines_Corrected_RIC[Headlines_Corrected_RIC[\"Constituent RIC\"]==x[\"Constituent RIC\"]][\"Corrected_RIC\"].values[0] if x[\"Constituent RIC\"] in ric_for_correction else x[\"Constituent RIC\"], axis=1)\n",
    "    #Merge\n",
    "eikon_news_data_02 = eikon_news_data_01.merge(Historical_SP_Full, how=\"inner\", on=[\"Constituent RIC\"], suffixes=('_Headlines', '_SP'))\n",
    "    \n",
    "# CHECK RIC Time Boundaries in S&P\n",
    "eikon_news_data_03 = eikon_news_data_02[(pd.to_datetime(eikon_news_data_02[\"versionCreated\"]).dt.tz_localize(None)>=pd.to_datetime(eikon_news_data_02[\"join\"])) & (pd.to_datetime(eikon_news_data_02[\"versionCreated\"]).dt.tz_localize(None)<=pd.to_datetime(eikon_news_data_02[\"leave\"]))].copy()\n",
    "\n",
    "# INFO\n",
    "eikon_news_data_03\n",
    "display(eikon_news_data_03.shape)\n",
    "print(\"Unique RIC\", eikon_news_data_03[\"Constituent RIC\"].nunique())\n",
    "print(\"RICS Not Available\", list(set(Historical_SP_Full[\"Constituent RIC\"].unique())-set(eikon_news_data_03[\"Constituent RIC\"].unique())))\n",
    "eikon_news_data_03.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d1df6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Create Auxiliary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676d903",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Adjust data for better interpretation\n",
    "eikon_news_data_1 = eikon_news_data_03.copy()\n",
    "  #Create Date and Datetime columns based on versionCreated\n",
    "eikon_news_data_1[\"DateTime\"] = pd.to_datetime(eikon_news_data_1[\"versionCreated\"])\n",
    "eikon_news_data_1[\"Date\"] =  pd.to_datetime(eikon_news_data_1['versionCreated']).dt.normalize()\n",
    "eikon_news_data_1[\"leave\"] = pd.to_datetime(eikon_news_data_1[\"leave\"])\n",
    "eikon_news_data_1[\"join\"] = pd.to_datetime(eikon_news_data_1[\"join\"])\n",
    "  #Add days in S&P\n",
    "eikon_news_data_1[\"days\"] = (eikon_news_data_1[\"leave\"] - eikon_news_data_1[\"join\"]).dt.days\n",
    "  #Renaming columns\n",
    "eikon_news_data_1.rename(columns = {\"Constituent RIC\": \"RIC\", \"Constituent Name_SP\": \"Company\"}, inplace = True) \n",
    "  #Cleaning the sources\n",
    "eikon_news_data_1[\"sourceCode\"] = eikon_news_data_1[\"sourceCode\"].apply(lambda x: x[3:])\n",
    "#Creating additional columns to identify holidays and weekends\n",
    "us_holidays = holidays.US()\n",
    "eikon_news_data_1[\"Holiday\"] = eikon_news_data_1.apply(lambda x : 1 if x[\"DateTime\"] in us_holidays else 0, axis=1)\n",
    "eikon_news_data_1[\"Weekday\"]= eikon_news_data_1[\"Date\"].dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4eaab4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Statistics on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d82c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " #Checking timeline \n",
    "print(\"Data is available from\", eikon_news_data_1[\"Date\"].min(), \"to\", eikon_news_data_1[\"Date\"].max())\n",
    "fig, ax1 = plt.subplots()\n",
    "eikon_news_data_1.groupby([(eikon_news_data_1[\"DateTime\"])])['DateTime'].count().plot(ax=ax1, kind='line')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7f9b1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " #Checking Sources\n",
    "fig, ax2 = plt.subplots()\n",
    "eikon_news_data_1.groupby([(eikon_news_data_1[\"sourceCode\"])])['sourceCode'].count().nlargest(10).plot(ax=ax2, kind='bar')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708d796",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " #Checking Top companies\n",
    "fig, ax3 = plt.subplots()\n",
    "eikon_news_data_1.groupby([(eikon_news_data_1[\"Company\"])])['Company'].count().nlargest(10).plot(ax=ax3, kind='bar')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a3874",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " #Checking Weekdays & holidays\n",
    "print(\"Percentage of news in holidays is {:.2%} \".format(len(eikon_news_data_1[eikon_news_data_1[\"Holiday\"]==1])/len(eikon_news_data_1)))\n",
    "fig, ax3 = plt.subplots()\n",
    "eikon_news_data_1.groupby([(eikon_news_data_1[\"Weekday\"])])['Weekday'].count().nlargest(10).plot(ax=ax3, kind='bar')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75050d5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Checking suspicious data Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a9210a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#eikon_news_data_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e1b4b",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#study_df = eikon_news_data_1[eikon_news_data_1[\"sourceCode\"]=='TRANS']    #EDG    #GLFILE   #TRANS\n",
    "#for index, row in study_df.iterrows():\n",
    "#    print(row[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711c1df",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#study_df = eikon_news_data_1[eikon_news_data_1[\"RIC\"]=='MCO.N']\n",
    "#for index, row in study_df.iterrows():\n",
    "#    print(row[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f68dacc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "> **Notes**\n",
    "\n",
    "*   Sources\n",
    "\n",
    "The headlines provided by **EDG** (Edgar Fillings News) & **GLFILE** (Global Fillings) & **TRANS** (Event Transcript News) are just informative of the day in which a company´s fillings, earnings, other events occor having relevant infromation (***Remove***).\n",
    "\n",
    "*   Companies\n",
    "\n",
    "The headlines provided by **MCO** (Moody´s) is primarly about other companies (***Remove***)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574bd717",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Final Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006d069",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Remove Irrelevant Sources ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa4532",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eikon_news_data_2 = eikon_news_data_1.copy()\n",
    "print(\"Initial News\", eikon_news_data_2.shape)\n",
    "#Eliminating irrelevant news sources\n",
    "eikon_news_data_2 = eikon_news_data_2[(eikon_news_data_2[\"sourceCode\"]!=\"EDG\") & (eikon_news_data_2[\"sourceCode\"]!=\"GLFILE\") & (eikon_news_data_2[\"sourceCode\"]!=\"TRANS\")].copy()\n",
    "print(\"News after Source Elimination\", eikon_news_data_2.shape)\n",
    "#Eliminating irrelevant companies\n",
    "eikon_news_data_2 = eikon_news_data_2[(eikon_news_data_2[\"RIC\"]!=\"MCO.N\")].copy()\n",
    "print(\"News after Company Elimination\", eikon_news_data_2.shape)\n",
    "\n",
    "#Elimianting duplicate (cases with same headline, for same company), keeping the first time instance in which they show up\n",
    "eikon_news_data_3 = eikon_news_data_2.sort_values(by=['DateTime'])  #from older to newer\n",
    "    #Keep only the first  repeated new on any given company\n",
    "eikon_news_data_final = eikon_news_data_3.drop_duplicates(subset=['text', 'RIC'], keep='first') #,'sourceCode'\n",
    "print(\"News after Duplicate Elimination (by Ric and text)\", eikon_news_data_final.shape)\n",
    "    \n",
    "#Signal Headlines common to multiple companies (e.g: industry level deadlines)\n",
    "eikon_news_data_final['Duplicate_Title'] = eikon_news_data_final.groupby('text')['text'].transform('count')\n",
    "print(\"Unique NEws\", len(eikon_news_data_final[eikon_news_data_final['Duplicate_Title']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2ff9c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_data = input(\"Save the Data?  \")\n",
    "if save_data.lower() == \"yes\":\n",
    "    eikon_news_data_final.to_pickle(\"1)Eikon_Data/Clean_Eikon_headlines_1Set2020_31Mar2022.pkl\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f7136",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275fe8a5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Clean Dataframe\n",
    "eikon_news_data_final = pd.read_pickle(\"1)Eikon_Data/Clean_Eikon_headlines_1Set2020_31Mar2022.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df815fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"News Data Size\", eikon_news_data_final.shape)\n",
    "#Checking timeline \n",
    "print(\"Data is available from\", eikon_news_data_final[\"Date\"].min(), \"to\", eikon_news_data_final[\"Date\"].max())\n",
    "fig, ax1 = plt.subplots()\n",
    "eikon_news_data_final.groupby([(eikon_news_data_final[\"DateTime\"])])['DateTime'].count().plot(ax=ax1, kind='line')\n",
    "plt.plot()\n",
    " #Checking Sources\n",
    "fig, ax2 = plt.subplots()\n",
    "eikon_news_data_final.groupby([(eikon_news_data_final[\"sourceCode\"])])['sourceCode'].count().nlargest(10).plot(ax=ax2, kind='bar')\n",
    "plt.plot()\n",
    " #Checking Top companies\n",
    "fig, ax3 = plt.subplots()\n",
    "eikon_news_data_final.groupby([(eikon_news_data_final[\"RIC\"])])['RIC'].count().nlargest(10).plot(ax=ax3, kind='bar')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75483f8b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Headlines Size statistics\n",
    "print('Average word length of Headlines is {0:.0f}.'.format(np.mean(eikon_news_data_final[\"text\"].apply(lambda x: len(x.split())))))\n",
    "print('Std Dev word length of Headlines is {0:.1f}.'.format(np.std(eikon_news_data_final[\"text\"].apply(lambda x: len(x.split())))))\n",
    "print('Max word length of Headlines is {0:.0f}.'.format(np.max(eikon_news_data_final[\"text\"].apply(lambda x: len(x.split())))))\n",
    "print('Average character length of Headlines is {0:.0f}.'.format(np.mean(eikon_news_data_final[\"text\"].apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d8c4f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Headline top words (removing redundant ones)\n",
    "headline_words = [''.join(filter(str.isalnum, word.lower())) for head in eikon_news_data_final[\"text\"].str.split().values.tolist() for word in head if ''.join(filter(str.isalnum, word.lower()))!=\"\"]\n",
    "counter=Counter(headline_words)\n",
    "most=counter.most_common()\n",
    "stop=set(stopwords.words('english'))\n",
    "x, y= [], []\n",
    "for word,count in most[:35]:\n",
    "    if (word not in stop):\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "sns.barplot(x=y,y=x, )\n",
    "sns.set(rc={'figure.figsize':(11,12)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0466b4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Auxiliary tables to all statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b500e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "join_leave_by_ric_final = eikon_news_data_final[[\"RIC\",\"days\",\"leave\",\"join\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3c5f0",
   "metadata": {
    "hidden": true
   },
   "source": [
    ">> **Statistics Saving Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3c89d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#SAVE DATA\n",
    "save_data_file_exploration = input(\"Create File to Save the Data?\")\n",
    "if save_data_file_exploration ==\"yes\":\n",
    "    datestring_time = datetime.strftime(datetime.now(),\"%m_%d_%Y\")\n",
    "    writer = pd.ExcelWriter(\"3)Tables_PLots/\" + str(datestring_time) + \"__Data_Exploration.xlsx\", engine='xlsxwriter')\n",
    "    \n",
    "save_data_exploration = input(\"Save the Data?\")\n",
    "if save_data_exploration ==\"yes\":\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb4e92",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Daily Company Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ddc13",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    #Aggregate\n",
    "news_company_level = eikon_news_data_final.groupby(['RIC']).agg(**{'# news':pd.NamedAgg(column='text', aggfunc='count'),\n",
    "                                                                 '# days with news':pd.NamedAgg(column='Date', aggfunc='nunique')})\n",
    "news_company_level_DCS = pd.merge(news_company_level, join_leave_by_ric_final[[\"RIC\",\"days\"]], left_index=True, right_on='RIC').set_index('RIC')\n",
    "    #Main metircs\n",
    "news_company_level_DCS['avg # news'] = news_company_level_DCS['# news']/news_company_level_DCS['days']\n",
    "news_company_level_DCS['% days without news'] = (news_company_level_DCS['days']-news_company_level_DCS['# days with news'])/news_company_level_DCS['days']*100\n",
    "percentiles = [0,10,20,30,40,50,60,70,80,90,100]\n",
    "new_decile_company_level = news_company_level_DCS.groupby(pd.cut(news_company_level_DCS[\"avg # news\"], np.percentile(news_company_level_DCS[\"avg # news\"], percentiles), include_lowest=True)).agg(\n",
    "                                                                  **{'avg # news':pd.NamedAgg(column='avg # news', aggfunc='mean'),\n",
    "                                                                     'median # news':pd.NamedAgg(column='avg # news', aggfunc='median'),\n",
    "                                                                     '# companies':pd.NamedAgg(column='avg # news', aggfunc='count'),\n",
    "                                                                      '% days without news':pd.NamedAgg(column='% days without news', aggfunc=\"mean\")})\n",
    "new_level = news_company_level_DCS.agg(**{'avg # news':pd.NamedAgg(column='avg # news', aggfunc='mean'),\n",
    "                                                                     'median # news':pd.NamedAgg(column='avg # news', aggfunc='median'),\n",
    "                                                                     '# companies':pd.NamedAgg(column='avg # news', aggfunc='count'),\n",
    "                                                                      '% days without news':pd.NamedAgg(column='% days without news', aggfunc=\"mean\")})\n",
    "display(new_decile_company_level)\n",
    "display(new_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7a261",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_data = input(\"Save the Data?\")\n",
    "if save_data.lower() == \"yes\":\n",
    "    # Position the dataframes in the worksheet.\n",
    "    new_decile_company_level.to_excel(writer, sheet_name='Daily_Company_Stats')  \n",
    "    new_level.to_excel(writer, sheet_name='Daily_Company_Stats',startrow=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b71334",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Extreme Companies statistics (according to total news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b295b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #Prepare auxiliary statistics\n",
    "        #Company level (Total news & auxiliary  # days with news)\n",
    "news_company_level = eikon_news_data_final.groupby(['RIC']).agg(**{'total # news':pd.NamedAgg(column='text', aggfunc='count'),\n",
    "                                                                 '# days with news':pd.NamedAgg(column='Date', aggfunc='nunique')})\n",
    "        #Company Date level (auxiliary list of news count for median, max, min)\n",
    "news_company_date_level = eikon_news_data_final.groupby(['RIC',\"Date\"]).agg(**{'text_count':pd.NamedAgg(column='text', aggfunc='count')})\n",
    "news_company_date_level_countlist = pd.DataFrame(news_company_date_level.groupby('RIC').text_count.apply(list))\n",
    "news_company_date_level_max_min = news_company_date_level.groupby(['RIC']).agg(**{ 'max # news':pd.NamedAgg(column='text_count', aggfunc='max'),\n",
    "                                                                                  'min # news':pd.NamedAgg(column='text_count', aggfunc='min')})\n",
    "    #Aggregate tables\n",
    "news_ECS_0= pd.merge(news_company_level, join_leave_by_ric_final[[\"RIC\",\"days\"]].drop_duplicates(subset=[\"RIC\"]), how=\"left\", on='RIC').set_index(\"RIC\")\n",
    "news_ECS_1 = pd.merge(news_ECS_0, news_company_date_level_max_min, left_index=True, right_index=True)\n",
    "news_ECS = pd.merge(news_ECS_1, news_company_date_level_countlist, left_index=True, right_index=True)\n",
    "    #Main metircs\n",
    "news_ECS['avg # news'] = news_ECS['total # news']/news_ECS['days']\n",
    "news_ECS['% days without news'] = (news_ECS['days']-news_ECS['# days with news'])/news_ECS['days']*100\n",
    "news_ECS['median # news'] = news_ECS.apply(lambda x: np.median([0]*(x['days']-x['# days with news']) +x['text_count']), axis=1)\n",
    "\n",
    "\n",
    "    #Select top and bottom companies\n",
    "top_x_companies = news_ECS.nlargest(10,[\"total # news\"])[['total # news','avg # news','median # news','max # news','min # news','% days without news']]\n",
    "bottom_x_companies = news_ECS.nsmallest(5,[\"total # news\"])[['total # news','avg # news','median # news','max # news','min # news','% days without news']]\n",
    "\n",
    "display(top_x_companies)\n",
    "display(bottom_x_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b410b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_data = input(\"Save the Data?\")\n",
    "if save_data.lower() == \"yes\":\n",
    "    # Position the dataframes in the worksheet.\n",
    "    top_x_companies.to_excel(writer, sheet_name='Extreme_Company_Stats')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea363f97",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Sector Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3e0fd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #Sector names (GICS)\n",
    "GICS_Sector_lookup = pd.read_excel(\"1)Eikon_Data/Sectors_lookup__ManualRevision.xlsx\")\n",
    "GICS_Sector_lookup = GICS_Sector_lookup.rename(columns={'Instrument': 'RIC', 'Company Name': 'Company_E(simplified)', 'GICS Sector Name': 'GICS_Sector', 'GICS Industry Group Name': 'GICS_Industry'})\n",
    "GICS_Sector_lookup = GICS_Sector_lookup[['RIC','Company_E(simplified)','GICS_Sector','GICS_Industry']].copy()\n",
    "display(\"Sector Info (GICS)\", GICS_Sector_lookup.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2f7c4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #Aggregate\n",
    "news_company_level = eikon_news_data_final.groupby(['RIC']).agg(**{'# news':pd.NamedAgg(column='text', aggfunc='count'),\n",
    "                                                                 '# days with news':pd.NamedAgg(column='Date', aggfunc='nunique')})\n",
    "news_company_level_DCS = pd.merge(news_company_level, join_leave_by_ric_final[[\"RIC\",\"days\"]], left_index=True, right_on='RIC').set_index('RIC')\n",
    "    #Main metircs\n",
    "news_company_level_DCS['avg # news'] = news_company_level_DCS['# news']/news_company_level_DCS['days']\n",
    "news_company_level_DCS['% days without news'] = (news_company_level_DCS['days']-news_company_level_DCS['# days with news'])/news_company_level_DCS['days']*100\n",
    "news_company_level_Sector = pd.merge(news_company_level_DCS.reset_index(), GICS_Sector_lookup[[\"RIC\",\"GICS_Sector\"]], on='RIC', how=\"left\")\n",
    "new_decile_sector_level = news_company_level_Sector.groupby([\"GICS_Sector\"]).agg(\n",
    "                                                                  **{'avg # news':pd.NamedAgg(column='avg # news', aggfunc='mean'),\n",
    "                                                                     'median # news':pd.NamedAgg(column='avg # news', aggfunc='median'),\n",
    "                                                                     '# companies':pd.NamedAgg(column='avg # news', aggfunc='count'),\n",
    "                                                                      '% days without news':pd.NamedAgg(column='% days without news', aggfunc=\"mean\")})\n",
    "display(new_decile_sector_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fba728",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# PFE.N  AMZN.OQ  FB.OQ  TSLA.OQ  MSFT.OQ  AAPL.OQ\n",
    "#GICS_Sector_lookup[GICS_Sector_lookup[\"RIC\"]==\"AAPL.OQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c31cbd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_data = input(\"Save the Data?\")\n",
    "if save_data.lower() == \"yes\":\n",
    "    # Position the dataframes in the worksheet.\n",
    "    new_decile_sector_level.to_excel(writer, sheet_name='Sector_Stats')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60642e51",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Source Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6080f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_news = len(eikon_news_data_final)\n",
    "new_source_level = eikon_news_data_final.groupby(['sourceCode']).agg(**{'total # news':pd.NamedAgg(column='text', aggfunc='count'),\n",
    "                                                                        '% news from source': pd.NamedAgg(column='text', aggfunc=lambda x: len(x)/total_news)})\n",
    "print(\"Total news\", total_news)\n",
    "display(new_source_level.sort_values(by=['total # news'], ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa520b3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_data = input(\"Save the Data?\")\n",
    "if save_data.lower() == \"yes\":\n",
    "    # Position the dataframes in the worksheet.\n",
    "    new_source_level.to_excel(writer, sheet_name='Source_Stats')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce9148",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbc597",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Functions for generic pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2aef2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def text_processing(text, multiexpression_dic=[], stopwords=None, special_char=False, numbers=True, lowercase=False, stemm=False, lemm=False):\n",
    "    '''Preprocess a string. All transformation can be chosen through arguments.\n",
    "    :parameter\n",
    "    :param text_input: string - sentence/corpus to be processed\n",
    "    :param numbers: bool - whether numbers are removed or not\n",
    "    :param special_char: bool - whether special characters and punctuation are removed or not\n",
    "    :param lowercase: bool - whether  words are converted to lowercase or not   \n",
    "    :param stopwords: list - list of stopwords to remove\n",
    "    :param stemm: bool - whether stemming is to be applied\n",
    "    :param lemm: bool - whether lemmitisation is to be applied\n",
    "    :param min_size: int - minimum size of words included (inclusive)\n",
    "    :param tokenize: bool - whether it is to tokenize the final text\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # MultiExpression Words (1st step)\n",
    "    for key, value in multiexpression_dic.items():\n",
    "        if key in text:\n",
    "            text = text.replace(key,value)\n",
    "    # Word Tokenize (2st step)\n",
    "    text = nltk.word_tokenize(text)    \n",
    "    #StopWord Removal (3rd step)\n",
    "    if stopwords is not None:\n",
    "        text = [word for word in text if word not in stopwords]    \n",
    "    # Punctuation & Special Character removal (except those between numbers and those in multiexpressions)\n",
    "    if special_char == True:\n",
    "        multiexpressions = list(multiexpression_dic.values())\n",
    "        text = [re.sub(r\"(?<!\\d)[.,;:](?!\\d)\", \"\", word) if word not in multiexpressions else word for word in text] \n",
    "        text = [word for word in text if word!=\"\"]   \n",
    "    # Numbers removal\n",
    "    if numbers == True:\n",
    "        text = [re.sub('\\d', '#', str(word)) for word in text] \n",
    "    # Conversion to lowercase\n",
    "    if lowercase == True:\n",
    "        text =  [word.lower() for word in text]       \n",
    "    # Stemming (remove -ing, -ly, ...)\n",
    "    if stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        text = [ps.stem(word) for word in text.split()]\n",
    "    # Lemmatisation (convert the word into root word)\n",
    "    if lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text.split()]     \n",
    "    return text\n",
    "\n",
    "def build_vocab(headlines):\n",
    "    \"\"\"REturns dictionary with cout of occurence of each word in the full dataset.\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in headlines:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def embedding_coverage(vocab,model):\n",
    "    '''Percentage of words in vocabulary that are in model selected. And percentage of tottal text.\n",
    "    :param vocab: dictionary with count of words in data to be vectorized\n",
    "    :param model: pretrained model used\n",
    "    '''\n",
    "    common_words = {}\n",
    "    specific_vocab = {}\n",
    "    n_words_common = 0\n",
    "    n_specific_vocab = 0\n",
    "    for word in (vocab):\n",
    "        try:\n",
    "            common_words[word] = model[word]\n",
    "            n_words_common += vocab[word]\n",
    "        except:\n",
    "            specific_vocab[word] = vocab[word]\n",
    "            n_specific_vocab += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(common_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(n_words_common / (n_words_common + n_specific_vocab)))\n",
    "    sorted_x = sorted(specific_vocab.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_x\n",
    "\n",
    "def FinBERT_tokenization_outputlayer(sentence):\n",
    "    '''A 3x1 layer is outputed of which the first value represents neutral the second positive and the third negative sentiment  {0:'neutral', 1:'positive',2:'negative'}'''\n",
    "    try:\n",
    "        #Tokenizing\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n",
    "        #Output Layes\n",
    "        outputs = finbert(**inputs)[0]\n",
    "        array_output = outputs.detach().numpy() \n",
    "        return array_output\n",
    "    except Exception as ex:\n",
    "        print(\"Error {} for sentence >  {}\".format(ex,sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513df9b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def classify_vector(vector, best_model):\n",
    "    try:\n",
    "        classification = best_model.predict(vector.reshape(1, -1))[0]\n",
    "        return classification\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def classify_vector_confidence(vector, best_model):\n",
    "    try:\n",
    "        classification = best_model.decision_function(vector.reshape(1, -1))[0]\n",
    "        return classification\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def classify_vector_probability(vector, best_model):\n",
    "    try:\n",
    "        classification = best_model.predict_proba(vector.reshape(1, -1))[0]\n",
    "        return classification\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2b160",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Loading pre-trained model and tokenizer\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef6c9f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Classification (BEST MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40754b07",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Processing (+ finbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781044e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Clean Dataframe\n",
    "SP_clean_0 = pd.read_pickle(\"1)Eikon_Data/Clean_Eikon_headlines_1Set2020_31Mar2022.pkl\") \n",
    "\n",
    "#RESAVE Finbert output layer OR load existing\n",
    "resave_output = input(\"ReSave Finbert Output?\")\n",
    "if resave_output == \"yes\":\n",
    "    SP_clean_0[\"Finbert_output\"] = SP_clean_0.progress_apply(lambda x: FinBERT_tokenization_outputlayer(x[\"text\"]), axis=1)\n",
    "    with open(\"4)Eikon_Classification/Processed_Eikon_headlines_1Set2020_31Mar2022.pkl\", \"wb\") as fp:   \n",
    "        pickle.dump(SP_clean_0, fp)  \n",
    "else:\n",
    "    with open(\"4)Eikon_Classification/Processed_Eikon_headlines_1Set2020_31Mar2022.pkl\", \"rb\") as fp:  \n",
    "        SP_clean_0 = pickle.load(fp)\n",
    "print(SP_clean_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c935067",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#CHECK DUPLICATE OCCURENCES ACROSS COMPANIES TO SEE IF OPPOSING SENTIMENT\n",
    "perform_dupl_check = input(\"Perform Check of Repeated News for each Company:  \")\n",
    "if perform_dupl_check==\"yes\":\n",
    "    print(\"Repetitions of text\", set(list(SP_clean_0[\"Duplicate_Title\"])))\n",
    "\n",
    "    temp_check = SP_clean_0[SP_clean_0[\"Duplicate_Title\"]==2][[\"text\",\"Company\"]]\n",
    "    temp_check[\"Companies_Involved\"] = temp_check.groupby([\"text\"])['Company'].transform(lambda x: ', '.join(x))\n",
    "    temp_check.drop_duplicates(subset=[\"text\",\"Companies_Involved\"], inplace=True)\n",
    "\n",
    "    for index, row in temp_check.iterrows():\n",
    "        print(row[\"Companies_Involved\"], \">\", row[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281b22e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Classify News for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629fe2a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load best model \n",
    "Best_Finbert = load(r\"C:\\Users\\Utilizador\\Desktop\\MasterDissertation_2021_code\\IS_Files\\2)Sentiment_Models\\RbfSVC_2class_wProb_Finbert.joblib\")\n",
    "reclasify_check = input(\"Finish Classification of news:  \")\n",
    "if reclasify_check==\"yes\":\n",
    "    # Classify according to binary  (1 for positive and -1 for negative)\n",
    "    SP_clean_0[\"Sent_Binary\"] = SP_clean_0.progress_apply(lambda x: classify_vector(x[\"Finbert_output\"], Best_Finbert),axis=1)\n",
    "    # Retrieve probabilities  (vector with probabilty of negative and positive respectively)\n",
    "    print(f\"Classes probability are in the same order as {Best_Finbert.classes_}\")\n",
    "    SP_clean_0[\"Prob_Vector\"] = SP_clean_0.progress_apply(lambda x: classify_vector_probability(x[\"Finbert_output\"], Best_Finbert),axis=1)\n",
    "    # Use one of the probabilites as Sent, recentering it around 0   (= range from -0.5 to 0.5, with 0 being neutral)\n",
    "    SP_clean_0[\"SentProb\"] = SP_clean_0.progress_apply(lambda x: x[\"Prob_Vector\"][1]-0.5 ,axis=1)\n",
    "else:\n",
    "    with open(\"4)Eikon_Classification/Classified_Eikon_headlines_1Set2020_31Mar2022\", \"rb\") as fp:  \n",
    "        SP_clean_0 = pickle.load(fp)\n",
    "print(SP_clean_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e6ea59",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Safety Check----------------------------------------------\n",
    "safety_check = input(\"Run Safety Check? \")\n",
    "if safety_check == \"yes\":\n",
    "    label = {0:-1,1:1}  # position to classification\n",
    "    safety_check = SP_clean_0[SP_clean_0.progress_apply(lambda x: True if x[\"Sent_Binary\"]==label[np.argmax(x[\"Prob_Vector\"])] else False, axis=1)]\n",
    "    print(f\"Full Dataset is {len(SP_clean_0)}. Safety check keeps {len(safety_check)}\")\n",
    "#-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4aff7a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_output = input(\"Save classification: \")\n",
    "if save_output == \"yes\":\n",
    "    with open(\"4)Eikon_Classification/Classified_Eikon_headlines_1Set2020_31Mar2022\", \"wb\") as fp:   \n",
    "        pickle.dump(SP_clean_0, fp)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b738eb0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Daily Complete Data Compilation\n",
    "\n",
    "Sentiment + Returns + Sector Info + (Auxilairy > Ticker, Cusip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd5803",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0e215",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def news_traiding_daily_timeline_adj(date_value):\n",
    "    date_time = date_value.replace(tzinfo=None)\n",
    "    date_ymd = datetime(date_time.year,date_time.month,date_time.day)\n",
    "    lower_date = datetime(date_ymd.year,date_ymd.month,date_ymd.day,9,30,0)\n",
    "    upper_date = lower_date + timedelta(hours=23,minutes=30)\n",
    "    if (date_time>=lower_date) & (date_time<=upper_date):\n",
    "        return date_ymd\n",
    "    elif (date_time<= (lower_date - timedelta(minutes=30))):\n",
    "        return date_ymd - timedelta(days=1)\n",
    "    else:\n",
    "        return \"Not Considered\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1fa51",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Compilations Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59344fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with IntraDay Sentiment classifications\n",
    "with open(\"4)Eikon_Classification/Classified_Eikon_headlines_1Set2020_31Mar2022\", \"rb\") as fp:  \n",
    "    SP_clean_1 = pickle.load(fp)\n",
    "print(SP_clean_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27449585",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load Returns Data\n",
    "WRDS_returns_Clean = pd.read_csv(\"2)WRDS_Data/WRDS_returns_Clean.csv\", index_col=0)\n",
    "WRDS_returns_Clean[\"DateAdj\"] = pd.to_datetime(WRDS_returns_Clean[\"date\"].astype(str))\n",
    "WRDS_returns = WRDS_returns_Clean[['DateAdj','RIC', 'COMNAM', 'RET', 'sprtrn', 'MktCap']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a1afd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Create Daily Sentiment For Each Company (And Derived Versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b879232",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating Dates for Daily Aggregation (as explained in paper) > This date contains sentiment actionable in next day\n",
    "SP_clean_1[\"DateAdj\"] = SP_clean_1.apply(lambda x: news_traiding_daily_timeline_adj(x[\"DateTime\"]), axis=1)\n",
    "# Remove News between the period of 9:00 AM and 9:30 AM\n",
    "SP_clean_2 = SP_clean_1[SP_clean_1[\"DateAdj\"]!=\"Not Considered\"].copy()\n",
    "print(SP_clean_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc96cc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SentVariableChosen = \"AbsoluteRobust\"   # \"Original\"   \"RelativeRobust\"   \"AbsoluteRobust\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d331d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Original**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142e12b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if SentVariableChosen==\"Original\":\n",
    "    # Daily Aggregated News\n",
    "    dict_aggregations = { 'Days':pd.NamedAgg(column='days', aggfunc = lambda x: x.unique()),\n",
    "                         'Leave':pd.NamedAgg(column='leave', aggfunc = lambda x: x.unique()),\n",
    "                         'Join':pd.NamedAgg(column='join', aggfunc = lambda x: x.unique()),\n",
    "                         'Company':pd.NamedAgg(column='Company', aggfunc= lambda x: x.unique()),\n",
    "                         'DailyTotalSent':pd.NamedAgg(column='SentProb', aggfunc='sum'),  # = overall sentiment score  (using confidence values)\n",
    "                         'DailyAvgSent':pd.NamedAgg(column='SentProb', aggfunc='mean'),                  \n",
    "                         'News_vol':pd.NamedAgg(column='SentProb', aggfunc=\"count\")}     # = number of positive news per day \n",
    "    # Daily (by RIC) > Note: companies change for some companies so RIC is the most reliable as there was a manual check on values\n",
    "    SP_Sent_RIC_daily_scores = SP_clean_2.groupby([\"RIC\",\"DateAdj\"]).agg(**dict_aggregations).reset_index()\n",
    "    print(\"Daily sentiment Data\", SP_Sent_RIC_daily_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34af03",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Robust***\n",
    "\n",
    "Relative Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a71535",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def coerce_values_rel(values):\n",
    "    sent_aux = (1+np.abs(np.sum(values[values>0])))/(1+np.abs(np.sum(values[values<0])))\n",
    "    if sent_aux>0:\n",
    "        return  np.log(sent_aux) \n",
    "    else:\n",
    "        print(\"Impossible Calc for \", values)\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845fe64c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if SentVariableChosen==\"RelativeRobust\":\n",
    "    # Daily Aggregated News\n",
    "    dict_aggregations = { 'Days':pd.NamedAgg(column='days', aggfunc = lambda x: x.unique()),\n",
    "                         'Leave':pd.NamedAgg(column='leave', aggfunc = lambda x: x.unique()),\n",
    "                         'Join':pd.NamedAgg(column='join', aggfunc = lambda x: x.unique()),\n",
    "                         'Company':pd.NamedAgg(column='Company', aggfunc= lambda x: x.unique()),\n",
    "                         'DailyTotalSent':pd.NamedAgg(column='SentProb', aggfunc='sum'),  # = overall sentiment score  (using confidence values)\n",
    "                         'DailyAvgSent':pd.NamedAgg(column='SentProb', aggfunc=lambda x: coerce_values_rel(x)),                  \n",
    "                         'News_vol':pd.NamedAgg(column='SentProb', aggfunc=\"count\")}     # = number of positive news per day \n",
    "    # Daily (by RIC) > Note: companies change for some companies so RIC is the most reliable as there was a manual check on values\n",
    "    SP_Sent_RIC_daily_scores = SP_clean_2.groupby([\"RIC\",\"DateAdj\"]).agg(**dict_aggregations).reset_index()\n",
    "    print(\"Daily sentiment Data\", SP_Sent_RIC_daily_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3eff7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Robust****\n",
    "\n",
    "Absolute Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b37381",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if SentVariableChosen==\"AbsoluteRobust\":\n",
    "    # Daily Aggregated News\n",
    "    dict_aggregations = { 'Days':pd.NamedAgg(column='days', aggfunc = lambda x: x.unique()),\n",
    "                         'Leave':pd.NamedAgg(column='leave', aggfunc = lambda x: x.unique()),\n",
    "                         'Join':pd.NamedAgg(column='join', aggfunc = lambda x: x.unique()),\n",
    "                         'Company':pd.NamedAgg(column='Company', aggfunc= lambda x: x.unique()),\n",
    "                         'DailyTotalSent':pd.NamedAgg(column='SentProb', aggfunc='sum'),  # = overall sentiment score  (using confidence values)\n",
    "                         'DailyAvgSent':pd.NamedAgg(column='SentProb', aggfunc=lambda x: np.abs(np.sum(x[x>0]))-np.abs(np.sum(x[x<0]))),                  \n",
    "                         'News_vol':pd.NamedAgg(column='SentProb', aggfunc=\"count\")}     # = number of positive news per day \n",
    "    # Daily (by RIC) > Note: companies change for some companies so RIC is the most reliable as there was a manual check on values\n",
    "    SP_Sent_RIC_daily_scores = SP_clean_2.groupby([\"RIC\",\"DateAdj\"]).agg(**dict_aggregations).reset_index()\n",
    "    print(\"Daily sentiment Data\", SP_Sent_RIC_daily_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1252a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Organize Sent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee6b8a4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Generate A Dataframe with all date & ticker/RIC Combinations (guarantee the shifted returns & sentiment are correct)\n",
    "    # Lists of values\n",
    "complete_date_list = list(WRDS_returns[\"DateAdj\"].unique())\n",
    "complete_ric_list = list(SP_Sent_RIC_daily_scores[\"RIC\"].unique())\n",
    "    # Baseline Dataframe\n",
    "Sent_Headlines =  pd.DataFrame(list(product(complete_date_list, complete_ric_list)), columns=['DateAdj', 'RIC'])\n",
    "Sent_Headlines = pd.merge(Sent_Headlines,SP_Sent_RIC_daily_scores, how=\"left\", on=['DateAdj', 'RIC'])\n",
    "    # Sort To ensure Date shift are correctl done\n",
    "Sent_Headlines = Sent_Headlines.sort_values([\"RIC\",\"DateAdj\"])\n",
    "    #Adjust Baseline Dataframe to the timeline for which actual sentiment data exists\n",
    "Sent_Headlines = Sent_Headlines[(Sent_Headlines[\"DateAdj\"]>=SP_Sent_RIC_daily_scores[\"DateAdj\"].min()) & (Sent_Headlines[\"DateAdj\"]<=SP_Sent_RIC_daily_scores[\"DateAdj\"].max())].copy()  \n",
    "\n",
    "    #Dealing with Nans\n",
    "nan_dealing = input(\"Treat Nans as neutral signals for Delayed Sentiment Value?: \")\n",
    "if nan_dealing==\"yes\":\n",
    "    Sent_Headlines[\"DailyAvgSent\"].fillna(0, inplace=True) \n",
    "    \n",
    "    #Calculate shifted versions (MAIN SENTIMENT MEASURE)\n",
    "for var in ['DailyAvgSent']:    \n",
    "    for i in range(1,2):\n",
    "        Sent_Headlines[var+'(t-' + str(i) +')'] = Sent_Headlines.groupby('RIC')[var].shift(periods=i)\n",
    "        \n",
    "Sent_Data_Full = Sent_Headlines.copy()\n",
    "display(Sent_Data_Full.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2a2c1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Check amount of nans\n",
    "check_nans = input(\"Perform Nan check? \")\n",
    "if check_nans == \"yes\":\n",
    "    check_Nans_Sent_Headlines = Sent_Data_Full.copy()\n",
    "    initial_size=len(check_Nans_Sent_Headlines)\n",
    "    check_Nans_Sent_Headlines = check_Nans_Sent_Headlines[[\"DateAdj\",\"RIC\"]+[col for col in check_Nans_Sent_Headlines.columns for i in range (1,2 ) if \"t-\"+str(i) in col]]\n",
    "    Sent_Headlines_NAN = check_Nans_Sent_Headlines[check_Nans_Sent_Headlines.isnull().sum(axis=1)<3]\n",
    "    print(f\"From {initial_size} observations {len(Sent_Headlines_NAN)/initial_size:.2%} ({len(Sent_Headlines_NAN)}) have reduced number of Nans\")\n",
    "    Sent_Headlines.replace(np.inf, np.nan, inplace=True)\n",
    "    Sent_Headlines.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2cf9b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Auxiliary, Informative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a451cee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    # GICS Code\n",
    "GICS_Sectors = pd.read_excel(\"1)Eikon_Data/Sectors_lookup__ManualRevision.xlsx\", index_col=0)\n",
    "GICS_Sectors.rename(columns = {'RIC Code':'RIC','GICS Sector Name':'GICS_Sector'}, inplace = True)\n",
    "Sent_GICS = Sent_Data_Full.merge(GICS_Sectors[['RIC', 'GICS_Sector']], how=\"left\", on=\"RIC\")\n",
    "print(\"Daily sentiment Data With Ticker and Cusip nd GICS\", Sent_GICS.shape)\n",
    "display(Sent_GICS.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98778514",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded5a41",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Generate a Dataframe will all date & ticker/RIC Combinations (guarantee the shifted returns & sentiment are correct)\n",
    "    #Lists of values\n",
    "complete_date_list = list(WRDS_returns[\"DateAdj\"].unique())\n",
    "complete_ric_list = list(SP_Sent_RIC_daily_scores[\"RIC\"].unique())\n",
    "    #Baseline Dataframe\n",
    "WRDS_returns_Final =  pd.DataFrame(list(product(complete_date_list, complete_ric_list)), columns=['DateAdj', 'RIC'])\n",
    "WRDS_returns_Final = pd.merge(WRDS_returns_Final,WRDS_returns, how=\"left\", on=['DateAdj', 'RIC'])\n",
    "WRDS_returns_Final['RET'] = pd.to_numeric(WRDS_returns_Final['RET'], errors='coerce')\n",
    "WRDS_returns_Final = WRDS_returns_Final.sort_values([\"RIC\",\"DateAdj\"])\n",
    "\n",
    "    #Calculate excess returns \n",
    "WRDS_returns_Final[\"Excess_RET\"] = WRDS_returns_Final[\"RET\"] - WRDS_returns_Final[\"sprtrn\"]\n",
    "\n",
    "    #Calculate Shifted market cap for Value weighted analysis\n",
    "for i in range(1,2):\n",
    "    WRDS_returns_Final['MktCap(t-' + str(i) +')'] = WRDS_returns_Final.groupby('RIC')['MktCap'].shift(periods=i)\n",
    " \n",
    "    #Eliminate instances were all returns null\n",
    "WRDS_returns_Final = WRDS_returns_Final[WRDS_returns_Final.isnull().sum(axis=1)<5]\n",
    "WRDS_returns_Final.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95380c1b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Sentiment (and Aux)  +  Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebc704",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Size Sent Data\", len(Sent_GICS), \" | Size Returns\", len(WRDS_returns_Final))\n",
    "DailySentimentReturn = pd.merge(Sent_GICS,WRDS_returns_Final, how=\"inner\", on=['DateAdj', 'RIC'])\n",
    "print(\"size Merged Data\", len(DailySentimentReturn))\n",
    "print(\"----------\")\n",
    "print(\"Dropped Dates (from Sent)\", list(set(Sent_GICS[\"DateAdj\"].unique())-set(DailySentimentReturn[\"DateAdj\"].unique())))\n",
    "print(\"Dropped Dates (from Ret)\", list(set(WRDS_returns_Final[\"DateAdj\"].unique())-set(DailySentimentReturn[\"DateAdj\"].unique())))\n",
    "print(\"----------\")\n",
    "print(\"Dropped RIC (from Sent)\", list(set(Sent_GICS[\"RIC\"].unique())-set(DailySentimentReturn[\"RIC\"].unique())))\n",
    "print(\"Dropped RIC (from Ret)\", list(set(WRDS_returns_Final[\"RIC\"].unique())-set(DailySentimentReturn[\"RIC\"].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36e66f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Safety Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5a6e1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Duplicate Check\n",
    "print( \"N of duplicates (on RIC and Date):\", len(DailySentimentReturn[DailySentimentReturn.duplicated(subset=['DateAdj','RIC'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6395b7d0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Check presence of full return timeline for each ric\n",
    "if False:\n",
    "    abnormal_values = []\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    us_holidays = cal.holidays(start=DailySentimentReturn[\"DateAdj\"].min(), end=DailySentimentReturn[\"DateAdj\"].max()).to_pydatetime()\n",
    "    us_holidays = np.array([x.date() for x in us_holidays], dtype='datetime64[D]')\n",
    "    for ric_aux in list(DailySentimentReturn[\"RIC\"].unique()):\n",
    "        temp = DailySentimentReturn[DailySentimentReturn[\"RIC\"]==ric_aux].copy()\n",
    "        n_business_days = np.busday_count(temp[\"DateAdj\"].min().date(), temp[\"DateAdj\"].max().date(), holidays=us_holidays)\n",
    "        unique_days = temp[\"DateAdj\"].nunique()\n",
    "        days = len(temp[\"DateAdj\"])\n",
    "        if not (n_business_days==unique_days==days):\n",
    "            abnormal_values.append({\"ric\":ric_aux, \"Bdays\":n_business_days, \"Adays\": [unique_days,days]})\n",
    "    print(abnormal_values)        \n",
    "    #Further check missing dates ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b55888",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Check Abnormal Values\n",
    "Infinite_check = DailySentimentReturn[DailySentimentReturn.isin([np.inf, -np.inf]).any(axis=1)]\n",
    "print(f\"From {len(DailySentimentReturn)} observations {len(Infinite_check)/len(DailySentimentReturn):.2%} ({len(Infinite_check)}) have infinite values\")\n",
    "print(\"Infinite Values are present in\", Infinite_check.columns[Infinite_check.isin([np.inf, -np.inf]).any(axis=0)])\n",
    "\n",
    "force_inf = input(\"Convert Infinite Values to Nan? \")\n",
    "if force_inf==\"yes\":\n",
    "    DailySentimentReturn.replace(np.inf, np.nan, inplace=True)\n",
    "    DailySentimentReturn.replace(-np.inf, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d727983",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Nan Checks\n",
    "Nan_check = DailySentimentReturn[[\"DateAdj\",\"RIC\"]+[col for col in DailySentimentReturn.columns for i in range (1,2 ) if \"t-\"+str(i) in col]]\n",
    "Sent_Headlines_NAN = Nan_check[Nan_check.isnull().sum(axis=1)<3]\n",
    "print(f\"From {len(DailySentimentReturn)} observations {len(Sent_Headlines_NAN)/len(DailySentimentReturn):.2%} ({len(Sent_Headlines_NAN)}) have reduced number of Nans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395cd8f4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ba452",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_output = input(\"Save Complete Daily Data: \")\n",
    "\n",
    "if save_output == \"yes\":\n",
    "    if SentVariableChosen==\"Original\":\n",
    "        with open(\"4)Eikon_Classification/Daily_Complete_HeadlineReturn\", \"wb\") as fp:   \n",
    "            pickle.dump(DailySentimentReturn, fp)  \n",
    "    elif SentVariableChosen==\"RelativeRobust\":    \n",
    "        with open(\"4)Eikon_Classification/Daily_Complete_HeadlineReturn_RelativeRobust\", \"wb\") as fp:   \n",
    "            pickle.dump(DailySentimentReturn, fp)  \n",
    "    elif SentVariableChosen==\"AbsoluteRobust\":  \n",
    "        with open(\"4)Eikon_Classification/Daily_Complete_HeadlineReturn_AbsoluteRobust\", \"wb\") as fp:   \n",
    "            pickle.dump(DailySentimentReturn, fp)  \n",
    "    print(\"Saved File for Variable\", SentVariableChosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd149021",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Compilations Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a9c0d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with IntraDay Sentiment classifications\n",
    "with open(\"4)Eikon_Classification/Classified_Eikon_headlines_1Set2020_31Mar2022\", \"rb\") as fp:  \n",
    "    SP_clean_1 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2885d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load Returns Data\n",
    "WRDS_returns_Clean = pd.read_csv(\"2)WRDS_Data/WRDS_returns_Clean.csv\", index_col=0)\n",
    "WRDS_returns_Clean[\"DateAdj\"] = pd.to_datetime(WRDS_returns_Clean[\"date\"].astype(str))\n",
    "WRDS_returns = WRDS_returns_Clean[['DateAdj','RIC', 'COMNAM', 'RET', 'sprtrn', 'MktCap']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf780a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Create Weekly Sentiment For Each Company (And Derived Versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e606384",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Dealing with Nans\n",
    "nan_dealing_extra = input(\"Treat Nans as neutral signals for Extra Variables based on Tendencies?: \")\n",
    "    \n",
    "#Calculate Sent Schock Score (attributing a score just in the cases for which at least half of the lookback period is available)\n",
    "lookback = 7\n",
    "final_temp = pd.DataFrame()\n",
    "    #(create for each RIC a list with the necesssary lookback period for calculations)\n",
    "for tic in Sent_Headlines['RIC'].unique():\n",
    "    temp = Sent_Headlines[Sent_Headlines['RIC']==tic].copy()\n",
    "    temp.sort_values(by=['DateAdj'], inplace=True)\n",
    "    if nan_dealing_extra == \"yes\":  \n",
    "        temp['List_SentShock(t-1)'] = [[0 if math.isnan(value) else value for value in window.to_list()] for window in temp[\"DailyAvgSent(t-1)\"].rolling(window=lookback)]\n",
    "    else:\n",
    "        temp['List_SentShock(t-1)'] = [window.to_list() for window in temp[\"DailyAvgSent(t-1)\"].rolling(window=lookback)]\n",
    "    final_temp = pd.concat([final_temp,temp])\n",
    "    #(auxiliary variables)\n",
    "final_temp[\"Average(\"+str(lookback)+\" days)\"] = final_temp.apply(lambda x : np.nanmean(x['List_SentShock(t-1)']), axis=1)\n",
    "final_temp[\"Std(\"+str(lookback)+\" days)\"] = final_temp.apply(lambda x : np.std(x['List_SentShock(t-1)']), axis=1)\n",
    "final_temp[\"Count(\"+str(lookback)+\" days)\"] = final_temp.apply(lambda x : np.sum(~np.isnan(x['List_SentShock(t-1)'])) if type(x['List_SentShock(t-1)'])==list else 0, axis=1)\n",
    "    #(attributing a score just in the cases for which at least 1/3 of the lookback period is available)\n",
    "final_temp[\"Sent_Shock_\"+str(lookback)+\"(t)\"] = np.where(final_temp[\"Count(\"+str(lookback)+\" days)\"]>lookback/3,(final_temp[\"DailyAvgSent\"] - final_temp[\"Average(\"+str(lookback)+\" days)\"])/final_temp[\"Std(\"+str(lookback)+\" days)\"], np.nan)\n",
    "final_temp[\"Sent_Shock_\"+str(lookback)+\"(t-1)\"] = final_temp.groupby('RIC')[\"Sent_Shock_\"+str(lookback)+\"(t)\"].shift(periods=1)\n",
    "    #(show key info)\n",
    "count_valid_observations = np.sum(~np.isnan(final_temp[\"Sent_Shock_\"+str(lookback)+\"(t-1)\"]))\n",
    "print(f\"From {len(final_temp)} observations {count_valid_observations/len(final_temp):.2%} ({count_valid_observations}) have valid Sentiment schock scores\")\n",
    "\n",
    "#Calculate Sent Trend score\n",
    "lookback = 7\n",
    "    #(sentiment delta)\n",
    "if nan_dealing_extra == \"yes\":\n",
    "    final_temp[\"ΔSent(t)\"] = final_temp[\"DailyAvgSent\"].fillna(0) - final_temp[\"DailyAvgSent(t-1)\"].fillna(0)\n",
    "else:\n",
    "    final_temp[\"ΔSent(t)\"] = final_temp[\"DailyAvgSent\"] - final_temp[\"DailyAvgSent(t-1)\"]\n",
    "final_temp_2 = pd.DataFrame()\n",
    "    #(create for each RIC a list with the necesssary lookvack period fro calculations)\n",
    "for tic in final_temp['RIC'].unique():\n",
    "    temp = final_temp[final_temp['RIC']==tic].copy()\n",
    "    temp.sort_values(by=['DateAdj'], inplace=True)\n",
    "    temp['List_ΔSent(t)'] = [window.to_list() for window in temp[\"ΔSent(t)\"].rolling(window=lookback)]\n",
    "    final_temp_2 = pd.concat([final_temp_2,temp])\n",
    "    #(auxiliary variables)\n",
    "final_temp_2[\"Sum_Trend(\"+str(lookback)+\" days)\"] = final_temp_2.apply(lambda x : np.sum(x['List_ΔSent(t)']), axis=1)\n",
    "final_temp_2[\"Count_Trend(\"+str(lookback)+\" days)\"] = final_temp_2.apply(lambda x : np.sum(~np.isnan(x['List_ΔSent(t)'])) if type(x['List_ΔSent(t)'])==list else 0, axis=1)\n",
    "    #(attributing a score just in the cases for which at least 1/3 of the lookback period is available)\n",
    "final_temp_2[\"Sent_Trend_\"+str(lookback)+\"(t)\"] = np.where(final_temp_2[\"Count_Trend(\"+str(lookback)+\" days)\"]>lookback/3, final_temp_2[\"Sum_Trend(\"+str(lookback)+\" days)\"] , np.nan)\n",
    "final_temp_2[\"Sent_Trend_\"+str(lookback)+\"(t-1)\"] = final_temp_2[\"Sent_Trend_\"+str(lookback)+\"(t)\"].shift(periods=1)\n",
    "    #(show key info)\n",
    "count_valid_observations = np.sum(~np.isnan(final_temp_2[\"Sent_Trend_\"+str(lookback)+\"(t-1)\"]))\n",
    "print(f\"From {len(final_temp_2)} observations {count_valid_observations/len(final_temp_2):.2%} ({count_valid_observations}) have valid Sentiment Trend scores\")\n",
    "\n",
    "#Save in Variable\n",
    "Sent_Data_Full = final_temp_2.copy()\n",
    "display(Sent_Data_Full.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd83399",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#SAme as other but filter in the end for the signals on monday (exclusive) so that portfolio can be adjusted in that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4063c1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b47903d0",
   "metadata": {},
   "source": [
    "# Data Exploration (OS Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59690c4c",
   "metadata": {},
   "source": [
    "## Summary Statistics (Sentiment by News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3313cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_input = input(\"Use daily data?\")\n",
    "\n",
    "if daily_input!=\"yes\":\n",
    "    #Load Saved Dataframe with IntraDay Sentiment classifications\n",
    "    with open(\"4)Eikon_Classification/Classified_Eikon_headlines_1Set2020_31Mar2022\", \"rb\") as fp:  \n",
    "        Os_Data_Explore = pickle.load(fp)\n",
    "    print(Os_Data_Explore.shape)\n",
    "else:\n",
    "    #Load Saved Dataframe with Daily Sentiment classifications\n",
    "    with open(\"4)Eikon_Classification/Daily_Complete_HeadlineReturn\", \"rb\") as fp:  \n",
    "        Os_Data_Explore = pickle.load(fp)\n",
    "        Os_Data_Explore[\"SentProb\"] = Os_Data_Explore[\"DailyAvgSent\"]\n",
    "    print(Os_Data_Explore.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbee07",
   "metadata": {},
   "source": [
    "* Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b56878",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall Distribution\")\n",
    "display(Os_Data_Explore[\"SentProb\"].describe())\n",
    "display(\"Kurtosis:\", Os_Data_Explore[\"SentProb\"].kurtosis())\n",
    "display(\"Skewness:\", Os_Data_Explore[\"SentProb\"].skew())\n",
    "#Displays\n",
    "fig, axes = plt.subplots(nrows=1,ncols=2)\n",
    "fig.set_size_inches(18, 10, forward=True)\n",
    "display(Os_Data_Explore[\"SentProb\"].hist(bins=50, ax= axes[0]))\n",
    "ax1 = plt.gca()\n",
    "display(Os_Data_Explore.boxplot(column=\"SentProb\", ax= axes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc526d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'figure.figsize':(10,5), 'figure.dpi':100})\n",
    "fig, ax = plt.subplots()\n",
    "# Hide the top and right spines of the axis\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "#plt.xticks(np.arange(-6, 9, 1.0))\n",
    "\n",
    "# Plot Histogram on x\n",
    "x = Os_Data_Explore[\"SentProb\"]\n",
    "plt.hist(x, bins=40)\n",
    "plt.gca().set(#title='Histogram', \n",
    "              ylabel='News Volume',xlabel=\"Sentiment\")\n",
    "#Save\n",
    "if save_data_file_exploration.lower() == \"yes\":\n",
    "    plt.savefig('3)Tables_PLots/Sentiment Distribution.png',dpi=300, transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf2e8c",
   "metadata": {},
   "source": [
    "* Headline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1649f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "company = \"AMZN.OQ\"\n",
    "company_table = Os_Data_Explore[Os_Data_Explore[\"RIC\"]== company]\n",
    "company_extreme_news = pd.concat([pd.DataFrame(company_table.nlargest(10,\"SentProb\")),\n",
    "                          pd.DataFrame(company_table.nsmallest(20,\"SentProb\"))])\n",
    "\n",
    "company_table[company_table.columns] = company_table[company_table.columns].astype(str)\n",
    "company_extreme_news[company_extreme_news.columns] = company_extreme_news[company_extreme_news.columns].astype(str) \n",
    "company_extreme_news.to_excel(\"3)Tables_Plots/SentimentExplore\"+ company + \"_Headlines.xlsx\")\n",
    "company_table.to_excel(\"3)Tables_Plots/SentimentExploreFull\"+ company + \"_Headlines.xlsx\")\n",
    "company_table.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce688ac",
   "metadata": {},
   "source": [
    ">> **Statistics Saving Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE DATA\n",
    "save_data_file_exploration = input(\"Create File to Save the Data?\")\n",
    "if save_data_file_exploration ==\"yes\":\n",
    "    datestring_time = datetime.strftime(datetime.now(),\"%m_%d_%Y\")\n",
    "    writer_sent = pd.ExcelWriter(\"3)Tables_PLots/\" + str(datestring_time) + \"__Data_ExplorationSENT.xlsx\", engine='xlsxwriter')\n",
    "    \n",
    "save_data_exploration = input(\"Save the Data?\")\n",
    "if save_data_exploration ==\"yes\":\n",
    "    writer_sent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bdc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with IntraDay Sentiment classifications\n",
    "#  Daily_Complete_HeadlineReturn\n",
    "#  Daily_Complete_HeadlineReturn_AbsoluteRobust\n",
    "#  Daily_Complete_HeadlineReturn_RelativeRobust\n",
    "with open(\"4)Eikon_Classification/Daily_Complete_HeadlineReturn_RelativeRobust\", \"rb\") as fp:  \n",
    "    InvestData_Explore = pickle.load(fp)\n",
    "print(InvestData_Explore.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a32c84",
   "metadata": {},
   "source": [
    "* Plot Graph Sent versus returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns list \n",
    "col_used = ['DailyAvgSent(t-1)','RET',\"MktCap(t-1)\",'sprtrn']\n",
    "\n",
    "# data\n",
    "    #frequency adjustment\n",
    "InvestData_Explore_W[\"frequency\"] = InvestData_Explore_W['DateAdj'].dt.strftime('%Y-%U')   # WEEK\n",
    "#InvestData_Explore_W[\"frequency\"] = InvestData_Explore_W['DateAdj']   # DAY\n",
    "news_day_level = InvestData_Explore_W.groupby(['frequency',\"RIC\"]).mean().reset_index()\n",
    "    #separate by section\n",
    "daily_top_mkt = news_day_level.groupby(['frequency']).apply(lambda x: x[(x[\"MktCap(t-1)\"]>=np.percentile(x[\"MktCap(t-1)\"],95))])\n",
    "daily_top_mkt = daily_top_mkt[~daily_top_mkt.isnull()[col_used].any(axis=1)].copy()\n",
    "daily_bottom_mkt = news_day_level.groupby(['frequency']).apply(lambda x: x[(x[\"MktCap(t-1)\"]<=np.percentile(x[\"MktCap(t-1)\"],5))])\n",
    "daily_bottom_mkt = daily_bottom_mkt[~daily_bottom_mkt.isnull()[col_used].any(axis=1)].copy()\n",
    "\n",
    "# plot pos and negative in differnt colors\n",
    "plt.scatter(daily_top_mkt[\"DailyAvgSent(t-1)\"], daily_top_mkt[\"RET\"], color='green', label=\"Pos Sent\", alpha=0.2) #pos/top\n",
    "plt.scatter(daily_bottom_mkt[\"DailyAvgSent(t-1)\"], daily_bottom_mkt[\"RET\"], color='red', label=\"Neg Sent\", alpha=0.2)  #neg/bottom\n",
    "#settings\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649cbae2",
   "metadata": {},
   "source": [
    "* Plot with Sent, return and Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Aggregate\n",
    "InvestData_Explore_W =  InvestData_Explore.copy() \n",
    "InvestData_Explore_W[\"frequency\"] = InvestData_Explore_W['DateAdj'].dt.strftime('%Y-%U')   # WEEK\n",
    "#InvestData_Explore_W[\"frequency\"] = InvestData_Explore_W['DateAdj']   # DAY\n",
    "news_day_level = InvestData_Explore_W.groupby(['frequency']).agg(**{'avgSent':pd.NamedAgg(column='DailyAvgSent', aggfunc=lambda x: x.mean()),\n",
    "                                                                'volume':pd.NamedAgg(column='News_vol', aggfunc=lambda x: x.sum()),\n",
    "                                                                's&p':pd.NamedAgg(column='sprtrn', aggfunc=lambda x: x.mean()),\n",
    "                                                                'date':pd.NamedAgg(column='DateAdj', aggfunc=lambda x: x.max())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700bf202",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax2 = ax.twinx()  # set up the 2nd axis\n",
    "ax3 = ax.twinx() # set up 3rd axis\n",
    "alphaVal = 1\n",
    "linethick=2\n",
    "\n",
    "#sentiment\n",
    "lg1= ax.plot(news_day_level.date,\n",
    "        news_day_level[\"avgSent\"],\n",
    "        color=\"#00008b\",\n",
    "        alpha=0.7,\n",
    "        linestyle = '-',\n",
    "        lw=linethick,\n",
    "        label=\"Average Sentiment\",\n",
    "       # alpha=alphaVal\n",
    "       )\n",
    "# Return S&P\n",
    "lg2= ax2.plot(news_day_level.date,\n",
    "        news_day_level[\"s&p\"],\n",
    "        color=\"green\",\n",
    "        linestyle = '-',\n",
    "        alpha=0.7,\n",
    "        lw=linethick,\n",
    "        label=\"S&P 500 Index\",\n",
    "       # alpha=alphaVal\n",
    "       )\n",
    "# Volume\n",
    "ax3.bar(news_day_level.date,\n",
    "        news_day_level[\"volume\"],\n",
    "        color=\"grey\",\n",
    "        alpha=0.2, \n",
    "        width=5,\n",
    "        #color=\"blue\",\n",
    "        label=\"News Vol.\",\n",
    "        #alpha=alphaVal\n",
    "       )\n",
    "\n",
    "\n",
    "\n",
    "# Set Axis\n",
    "ax3.get_yaxis().set_visible(False)\n",
    "myFmt = matplotlib.dates.DateFormatter('%Y-%m')\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "#Anotate Volume extreme value\n",
    "    #(max)\n",
    "y_max = news_day_level[\"volume\"].max()\n",
    "x_max = news_day_level[news_day_level[\"volume\"]==news_day_level[\"volume\"].max()][\"date\"].values[0]\n",
    "ax3.annotate(\"#News={0:,.0f}\".format(y_max), xy=(x_max, y_max),bbox=dict(pad=5, facecolor=\"none\", edgecolor=\"none\"))\n",
    "    #(min)\n",
    "y_min = news_day_level[\"volume\"].min()\n",
    "x_min = news_day_level[news_day_level[\"volume\"]==news_day_level[\"volume\"].min()][\"date\"].values[0]\n",
    "ax3.annotate(\"#News={0:,.0f}\".format(y_min), xy=(x_min, y_min),bbox=dict(pad=5, facecolor=\"none\", edgecolor=\"none\"))\n",
    "\n",
    "# Set Labels\n",
    "ax.set_xlabel('')\n",
    "ax.set_xlabel(\"Month\",rotation=0)\n",
    "#plt.ylabel(\"Average Sentiment Confidence\",rotation=90)\n",
    "ax.set_ylabel('Average Sentiment')\n",
    "ax2.set_ylabel('Average S&P 500 Index Return')\n",
    "\n",
    "#Legend\n",
    "lns = lg1+lg2\n",
    "labs = [l.get_label() for l in lg1+lg2]\n",
    "ax.legend(lns, labs, frameon=False,  loc='lower left',ncol=1,handlelength=1,framealpha=1,facecolor=\"w\")\n",
    "\n",
    "#Save\n",
    "if save_data_file_exploration.lower() == \"yes\":\n",
    "    plt.savefig('3)Tables_PLots/Weekly_Sent_Return_Volume.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079be605",
   "metadata": {},
   "source": [
    "* Sent by Decile (average news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f6c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Aggregate\n",
    "news_company_level = InvestData_Explore.groupby(['RIC']).agg(**{'avgSent':pd.NamedAgg(column='DailyAvgSent', aggfunc=lambda x: x.mean()),\n",
    "                                                                'news_vol':pd.NamedAgg(column='News_vol', aggfunc=lambda x: x.sum()),\n",
    "                                                                 'days':pd.NamedAgg(column='Days', aggfunc=lambda x: x.max())})\n",
    "news_company_level['avg # news'] = news_company_level['news_vol']/news_company_level['days']\n",
    "    #Main metircs\n",
    "percentiles = [0,10,20,30,40,50,60,70,80,90,100]\n",
    "new_decile_company_level = news_company_level.groupby(pd.cut(news_company_level[\"avg # news\"], np.nanpercentile(news_company_level[\"avg # news\"], percentiles), include_lowest=True)).agg(\n",
    "                                                                  **{'avg Sent':pd.NamedAgg(column='avgSent', aggfunc='mean'),\n",
    "                                                                     'median Sent':pd.NamedAgg(column='avgSent', aggfunc='median'),\n",
    "                                                                     'max Sent':pd.NamedAgg(column='avgSent', aggfunc='max'),\n",
    "                                                                     'min Sent':pd.NamedAgg(column='avgSent', aggfunc='min'),\n",
    "                                                                     'ske Sent':pd.NamedAgg(column='avgSent', aggfunc='skew'),\n",
    "                                                                     'kurt Sent':pd.NamedAgg(column='avgSent', aggfunc='min'),\n",
    "                                                                    })\n",
    "display(new_decile_company_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c0975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position the dataframes in the worksheet.\n",
    "if save_data_file_exploration ==\"yes\":\n",
    "    new_decile_company_level.reset_index().to_excel(writer_sent, sheet_name='Decile_Stats')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce72f89",
   "metadata": {},
   "source": [
    "* Sent by Sector(average sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae51fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "GICS_Sector_lookup = pd.read_excel(\"1)Eikon_Data/Sectors_lookup__ManualRevision.xlsx\", index_col=0)\n",
    "GICS_Sector_lookup.rename(columns = {'RIC Code':'RIC','GICS Sector Name':'GICS_Sector'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e723e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BY SECTOR\n",
    "    #Aggregate\n",
    "news_company_level = InvestData_Explore.groupby(['RIC']).agg(**{'avgSent':pd.NamedAgg(column='DailyAvgSent', aggfunc=lambda x: x.mean())})\n",
    "    #Main metircs\n",
    "news_company_level_Sector = pd.merge(news_company_level.reset_index(), GICS_Sector_lookup[[\"RIC\",\"GICS_Sector\"]], on='RIC', how=\"left\")\n",
    "new_decile_sector_level = news_company_level_Sector.groupby([\"GICS_Sector\"]).agg(\n",
    "                                                                  **{'avg Sent':pd.NamedAgg(column='avgSent', aggfunc='mean'),\n",
    "                                                                     'median Sent':pd.NamedAgg(column='avgSent', aggfunc='median'),\n",
    "                                                                     'max Sent':pd.NamedAgg(column='avgSent', aggfunc='max'),\n",
    "                                                                     'min Sent':pd.NamedAgg(column='avgSent', aggfunc='min'),\n",
    "                                                                     'ske Sent':pd.NamedAgg(column='avgSent', aggfunc='skew'),\n",
    "                                                                     'kurt Sent':pd.NamedAgg(column='avgSent', aggfunc='min'),\n",
    "                                                                    })\n",
    "display(new_decile_sector_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043de141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position the dataframes in the worksheet.\n",
    "if save_data_file_exploration ==\"yes\":\n",
    "    new_decile_sector_level.reset_index().to_excel(writer_sent, sheet_name='Sector_Stats')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62142fb4",
   "metadata": {},
   "source": [
    "# Investment Strats  (daily)\n",
    "\n",
    "Fama/French 3 Factor >  https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0224a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ba016",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8730b8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "MAX Drawdaown\n",
    "https://stackoverflow.com/questions/52539335/how-to-calculate-maximumdrawdown-using-returns-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e8e97",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Turnover\n",
    "https://quant.stackexchange.com/questions/19229/calculate-turnover-for-portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35946f78",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Annualized results\n",
    "def advanced_stats_table(Strat_Returns):\n",
    "    #Main Metrics\n",
    "    Strat_Statistics.loc[\"Count\"] = len(Strat_Returns)\n",
    "    Strat_Statistics.loc[\"\"] = \"\"\n",
    "    Strat_Statistics.loc[\"Mean %\"] = np.average(Strat_Returns)*100*12\n",
    "    Strat_Statistics.loc[\"(mean) t-stat\"] = np.average(Strat_Returns) / (np.std(Strat_Returns)/ (Strat_Statistics.loc[\"Count\"].values[0]**0.5)) \n",
    "    Strat_Statistics.loc[\"(mean) p-value\"] = (1-norm.cdf(Strat_Statistics.loc[\"(mean) t-stat\"].values[0]))*2\n",
    "    Strat_Statistics.loc[\"-\"] = \"-\"\n",
    "    Strat_Statistics.loc[\"Std. Dev. (%)\"] = np.round(np.std(Strat_Returns)*100*(12**0.5),2)\n",
    "    Strat_Statistics.loc[\"--\"] = \"--\"\n",
    "    Strat_Statistics.loc[\"Sharpe ratio\"] = Strat_Statistics.loc[\"Mean %\"]/ Strat_Statistics.loc[\"Std. Dev. (%)\"]\n",
    "    Strat_Statistics.loc[\"(SR) t-stat\"] = (Strat_Statistics.loc[\"Sharpe ratio\"].values[0]/(12**0.5))/np.sqrt((1+0.5*(Strat_Statistics.loc[\"Sharpe ratio\"].values[0]/(12**0.5))**2)/Strat_Statistics.loc[\"Count\"].values[0])\n",
    "    Strat_Statistics.loc[\"(SR) p-value\"] = (1-norm.cdf(Strat_Statistics.loc[\"(SR) t-stat\"].values[0]))*2\n",
    "    Strat_Statistics.loc[\"---\"] = \"---\"\n",
    "    Strat_Statistics.loc[\"Skewness\"] = skew(Strat_Returns)\n",
    "    Strat_Statistics.loc[\"Kurtosis\"] = kurtosis(Strat_Returns)\n",
    "    Strat_Statistics.loc[\"----\"] = \"----\"\n",
    "    Strat_Statistics.loc[\"JB test statistic\"] = (Strat_Statistics.loc[\"Count\"].values[0]/6)*(Strat_Statistics.loc[\"Skewness\"].values[0]**2+0.25*(Strat_Statistics.loc[\"Kurtosis\"].values[0]**0.5))\n",
    "    Strat_Statistics.loc[\"(JB) p-value\"] = 1-chi2.cdf(Strat_Statistics.loc[\"JB test statistic\"].values[0], 2)\n",
    "    Strat_Statistics.loc[\"-----\"] = \"-----\"\n",
    "    Strat_Statistics.loc[\"Minimum (%)\"] = Strat_Returns.min()\n",
    "    Strat_Statistics.loc[\"Percentile 25 (%)\"] = np.percentile(Strat_Returns, 25)\n",
    "    Strat_Statistics.loc[\"Median (%)\"] = Strat_Returns.median()\n",
    "    Strat_Statistics.loc[\"Percentile 75 (%)\"] = np.percentile(Strat_Returns, 75)\n",
    "    Strat_Statistics.loc[\"Maximum (%)\"] = Strat_Returns.max()\n",
    "    Strat_Statistics.loc[\"------\"] = \"------\"\n",
    "    Strat_Statistics.loc[\"AR(1) (%)\"] = Strat_Returns.corr(Strat_Returns.shift(1))\n",
    "    Strat_Statistics.loc[\"p-value\"] = 2*(1-norm.cdf(Strat_Statistics.loc[\"Count\"].values[0]**0.5*np.abs(Strat_Statistics.loc[\"AR(1) (%)\"].values[0])))\n",
    "    return Strat_Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f75092",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def paper_stats_table(STRAT, ret_col,cumret_col, data):\n",
    "    #(auxiliary)\n",
    "    Strat_Statistics = pd.DataFrame()\n",
    "    ret = data[ret_col]\n",
    "    cum_ret = data[cumret_col]\n",
    "    #(average)\n",
    "    Strat_Statistics.loc[STRAT, \"Average Return (%)\"] = np.average(ret)*100*12\n",
    "    Strat_Statistics.loc[\"(mean) t-stat\", \"Average Return (%)\"] = np.average(ret) / (np.std(ret)/ (len(ret)**0.5)) \n",
    "    Strat_Statistics.loc[\"(mean) p-value\", \"Average Return (%)\"] = (1-norm.cdf(np.abs(Strat_Statistics.loc[\"(mean) t-stat\", \"Average Return (%)\"])))*2\n",
    "    #(sharpe ratio)\n",
    "    Strat_Statistics.loc[STRAT, \"Sharpe Ratio\"] = (np.average(ret)*12)/ (np.std(ret)*(12**0.5)) \n",
    "    Strat_Statistics.loc[\"(mean) t-stat\", \"Sharpe Ratio\"] = (Strat_Statistics.loc[STRAT, \"Sharpe Ratio\"]/(12**0.5))/np.sqrt((1+0.5*(Strat_Statistics.loc[STRAT, \"Sharpe Ratio\"]/(12**0.5))**2)/len(ret))\n",
    "    Strat_Statistics.loc[\"(mean) p-value\", \"Sharpe Ratio\"] = (1-norm.cdf(np.abs(Strat_Statistics.loc[\"(mean) t-stat\", \"Sharpe Ratio\"])))*2\n",
    "    #(max drawdown)\n",
    "    aux_drawdown = pd.DataFrame(ret*1000).rename(columns={ret_col:\"Dollar\"})\n",
    "    aux_drawdown[\"Dollar\"].iloc[0] += 1000\n",
    "    aux_drawdown[\"DollarEvol\"] = aux_drawdown[\"Dollar\"].cumsum()\n",
    "    aux_drawdown[\"Roll_Max\"] = aux_drawdown['DollarEvol'].rolling(300, min_periods=1).max()\n",
    "    aux_drawdown[\"Daily_Drawdown\"] = aux_drawdown[\"DollarEvol\"]/aux_drawdown[\"Roll_Max\"] - 1.0\n",
    "    Strat_Statistics.loc[STRAT, \"Max Drawdown (%)\"] = (aux_drawdown[\"Daily_Drawdown\"].min()) *100\n",
    "    #(3 Factor)\n",
    "    X_Fama3Factors = data[['Mkt-RF', 'SMB', 'HML']]\n",
    "    Y_ExcessReturns = data[ret_col] - data['RF']\n",
    "    X = sm.add_constant(X_Fama3Factors)\n",
    "    ff_model = sm.OLS(Y_ExcessReturns, X).fit()\n",
    "    Strat_Statistics.loc[\"(mean) t-stat\", \"FF3\"] = ff_model.tvalues[0]\n",
    "    Strat_Statistics.loc[\"(mean) p-value\", \"FF3\"] = (1-norm.cdf(np.abs(Strat_Statistics.loc[\"(mean) t-stat\", \"FF3\"])))\n",
    "    Strat_Statistics.loc[\"alpha\", \"FF3\"] = ff_model.params[0] * 12 * 100\n",
    "    Strat_Statistics.loc[\"R^2\", \"FF3\"] = ff_model.rsquared * 100\n",
    "    #(5 Factor)\n",
    "    X_Fama5Factors = data[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "    Y_ExcessReturns = data[ret_col] - data['RF']\n",
    "    X = sm.add_constant(X_Fama5Factors)\n",
    "    ff_model = sm.OLS(Y_ExcessReturns, X).fit()\n",
    "    Strat_Statistics.loc[\"(mean) t-stat\", \"FF5\"] = ff_model.tvalues[0]\n",
    "    Strat_Statistics.loc[\"(mean) p-value\", \"FF5\"] = (1-norm.cdf(np.abs(Strat_Statistics.loc[\"(mean) t-stat\", \"FF5\"])))\n",
    "    Strat_Statistics.loc[\"alpha\", \"FF5\"] = ff_model.params[0] * 12 * 100\n",
    "    Strat_Statistics.loc[\"R^2\", \"FF5\"] = ff_model.rsquared * 100\n",
    "    #(5 Factor+MOM)\n",
    "    X_Fama5FactorsMOM = data[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'Mom']]\n",
    "    Y_ExcessReturns = data[ret_col] - data['RF']\n",
    "    X = sm.add_constant(X_Fama5FactorsMOM)\n",
    "    ff_model = sm.OLS(Y_ExcessReturns, X).fit()\n",
    "    Strat_Statistics.loc[\"(mean) t-stat\", \"FF5 + MOM\"] = ff_model.tvalues[0]\n",
    "    Strat_Statistics.loc[\"(mean) p-value\", \"FF5 + MOM\"] = (1-norm.cdf(np.abs(Strat_Statistics.loc[\"(mean) t-stat\", \"FF5 + MOM\"])))\n",
    "    Strat_Statistics.loc[\"alpha\", \"FF5 + MOM\"] = ff_model.params[0] * 12 * 100\n",
    "    Strat_Statistics.loc[\"R^2\", \"FF5 + MOM\"] = ff_model.rsquared * 100\n",
    "    return Strat_Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c78b9cd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def paper_stats_table_Costs(STRAT, ret_col,net_col, data):\n",
    "    #(auxiliary)\n",
    "    Strat_Statistics = pd.DataFrame()\n",
    "    ret = data[ret_col]\n",
    "    net_ret = data[net_col]\n",
    "    #GROSS\n",
    "        #(average)\n",
    "    Strat_Statistics.loc[STRAT, \"GROSS Return (%)\"] = np.average(ret)*100*12\n",
    "    Strat_Statistics.loc[\"(mean) t-stat\", \"GROSS Return (%)\"] = np.average(ret) / (np.std(ret)/ (len(ret)**0.5)) \n",
    "    Strat_Statistics.loc[\"(mean) p-value\", \"GROSS Return (%)\"] = (1-norm.cdf(np.abs(Strat_Statistics.loc[\"(mean) t-stat\", \"GROSS Return (%)\"])))*2\n",
    "        #(sharpe ratio)\n",
    "    Strat_Statistics.loc[STRAT, \"GROSS SR\"] = (np.average(ret)*12)/ (np.std(ret)*(12**0.5)) \n",
    "    Strat_Statistics.loc[\"(mean) t-stat\", \"GROSS SR\"] = (Strat_Statistics.loc[STRAT, \"GROSS SR\"]/(12**0.5))/np.sqrt((1+0.5*(Strat_Statistics.loc[STRAT, \"GROSS SR\"]/(12**0.5))**2)/len(ret))\n",
    "    Strat_Statistics.loc[\"(mean) p-value\", \"GROSS SR\"] = (1-norm.cdf(np.abs(Strat_Statistics.loc[\"(mean) t-stat\", \"GROSS SR\"])))*2\n",
    "    #NET\n",
    "        #(average)\n",
    "    Strat_Statistics.loc[STRAT, \"NET Return (%)\"] = np.average(net_ret)*100*12\n",
    "    Strat_Statistics.loc[\"(mean) t-stat\", \"NET Return (%)\"] = np.average(net_ret) / (np.std(net_ret)/ (len(net_ret)**0.5)) \n",
    "    Strat_Statistics.loc[\"(mean) p-value\", \"NET Return (%)\"] = (1-norm.cdf(np.abs(Strat_Statistics.loc[\"(mean) t-stat\", \"NET Return (%)\"])))*2\n",
    "        #(sharpe ratio)\n",
    "    Strat_Statistics.loc[STRAT, \"NET SR\"] = (np.average(net_ret)*12)/ (np.std(net_ret)*(12**0.5)) \n",
    "    Strat_Statistics.loc[\"(mean) t-stat\", \"NET SR\"] = (Strat_Statistics.loc[STRAT, \"NET SR\"]/(12**0.5))/np.sqrt((1+0.5*(Strat_Statistics.loc[STRAT, \"NET SR\"]/(12**0.5))**2)/len(net_ret))\n",
    "    Strat_Statistics.loc[\"(mean) p-value\", \"NET SR\"] = (1-norm.cdf(np.abs(Strat_Statistics.loc[\"(mean) t-stat\", \"NET SR\"])))*2\n",
    "    return Strat_Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c719333",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def FamaFrench(data, y_var):\n",
    "    X_Fama3Factors = data[['Mkt-RF', 'SMB', 'HML']]\n",
    "    Y_ExcessReturns = data[y_var] - data['RF']\n",
    "    X = sm.add_constant(X_Fama3Factors)\n",
    "    ff_model = sm.OLS(Y_ExcessReturns, X).fit()\n",
    "    print(ff_model.summary())\n",
    "    intercept, b1, b2, b3 = ff_model.params\n",
    "    return intercept, b1, b2, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065850d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Portfolio Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37660891",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_ric_pos_dict(strat_,   Ncomp,   date_, row_sent,   df_check):\n",
    "    '''Dictionary of RIC and respective position 1 (long) or -1 (short), Corrected for Data Availability'''\n",
    "    #Create dictionary with RIC : Position Sig basedon Strat\n",
    "    if strat_==\"L\":\n",
    "        ric_pos_dict =  {r: 1 for r in list(row_sent[row_sent>0].nlargest(Ncomp).index)}\n",
    "    elif strat_==\"S\":\n",
    "        ric_pos_dict = {r: -1 for r in list(row_sent[row_sent<0].nsmallest(Ncomp).index)}\n",
    "    elif strat_==\"L-S\":\n",
    "        ric_pos_dict = {** {r: 1 for r in list(row_sent[row_sent>0].nlargest(int(Ncomp/2)).index)} ,  **{r: -1 for r in list(row_sent[row_sent<0].nsmallest(int(Ncomp/2)).index)}}\n",
    "    #Clean list based on data availability\n",
    "    for df in df_check:\n",
    "        date_aux_df = df.loc[date_]\n",
    "        non_nan_rics = list(date_aux_df[~date_aux_df.isna()].index)\n",
    "        ric_pos_dict = {key:value  for key, value in ric_pos_dict.items() if key in non_nan_rics}\n",
    "    #Determine what to return \n",
    "    if ric_pos_dict!=[]:\n",
    "        return ric_pos_dict\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a3ee4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_ric_value_dict(date_, C_ric_pos_dict, Value_df):\n",
    "    '''Get the list of values (can be Mkt, RET,... depends on the Value_df chosen) for each RIC preserving order of dict'''\n",
    "    day_values = Value_df.loc[date_]\n",
    "    sorted_values = [day_values.loc[ric] for ric in C_ric_pos_dict]\n",
    "    return sorted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a7b7b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_universe_weight_dict(unique_rics, C_ric_pos_dict, C_weights=None):\n",
    "    '''Dictionary of all RIC with the respective positions for that point in time, either equal weighted or value weighted'''\n",
    "    if C_weights is None:\n",
    "        size_pos = len(C_ric_pos_dict)\n",
    "        universe_weights = {ric:(C_ric_pos_dict[ric]/size_pos if ric in C_ric_pos_dict else 0) for ric in unique_rics}\n",
    "    else:\n",
    "        size_pos = np.sum(C_weights)\n",
    "        C_weights_dict = {ric:C_weights[index] for index, ric in enumerate(C_ric_pos_dict)}\n",
    "        universe_weights = {ric:(C_ric_pos_dict[ric]*C_weights_dict[ric]/size_pos if ric in C_ric_pos_dict else 0) for ric in unique_rics}\n",
    "    return universe_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795e70e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_daily_turnover(date_, weights_df, Return_df,tipo=\"adj\"):\n",
    "    '''Based on date get current and previous wight lists for the all the RICs and compute abs difference \n",
    "    adjusted based on returns to get turnover,  the first position is assumed to be zero on all rics'''\n",
    "    #Weight Universes\n",
    "    loc_row = weights_df.index.get_loc(date_)\n",
    "    C_weights = weights_df.iloc[loc_row]\n",
    "    P_weights = weights_df.iloc[loc_row-1] if loc_row>0 else {key:0 for key in C_weights}\n",
    "    #Return Universes\n",
    "    day_values = Return_df.loc[date_]\n",
    "    C_returns = {key:(0 if pd.isnull(day_values.loc[key]) else day_values.loc[key]) for key in C_weights}\n",
    "    return_if_hold = np.sum([P_weights[key]*C_returns[key] for key in C_returns])\n",
    "    #Adjusted Calc\n",
    "    if tipo==\"adj\":\n",
    "        cost_list = [np.abs(C_weights[key]-(P_weights[key]*(1+C_returns[key]))/(1+return_if_hold)) for key in C_weights]\n",
    "        return np.sum(cost_list)/2\n",
    "    else:\n",
    "        cost_list = [np.abs(C_weights[key]-P_weights[key]) for key in C_weights]\n",
    "        return np.sum(cost_list)/2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff4a70",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_transaction_costs(date_, weight_df):\n",
    "    '''Based on date get current and previous wight lists for the all the RICs and compute abs difference * cost,\n",
    "    the first position is assumed to be zero on all rics'''\n",
    "    #COST USER VALUES\n",
    "    cost_small_cap = 0.0020 \n",
    "    cost_big_cap = 0.0010\n",
    "    #CAlC\n",
    "    loc_row = weight_df.index.get_loc(date_)\n",
    "    C_weights = weight_df.iloc[loc_row]\n",
    "    P_weights = weight_df.iloc[loc_row-1] if loc_row>0 else {key:0 for key in C_weights}\n",
    "    cost_list = [np.abs(C_weights[key]-P_weights[key])*cost_big_cap for key in C_weights]\n",
    "    return np.sum(cost_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabc509",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getcurrent_optimal_universe_weight_dict(strat_, P_universe_w, C_universe_w, start_type, λ):\n",
    "    '''Adjust weights of universe based on fixed value to optimize trading costs\n",
    "    λ determines how much is changed each time (keeping 1-λ fixed)''' \n",
    "    #Define Starting Point  (3 scenarios, EW based on sentiment, EW random x companies)\n",
    "    if P_universe_w=={}:\n",
    "        if start_type[0]==\"current\":   # use current allocation as dictated by sentiment\n",
    "            return C_universe_w\n",
    "        elif start_type[0]==\"random\":   # use EW across random number of elements in universe\n",
    "            ric_universe = list(C_universe_w)\n",
    "            size_pos = len(C_universe_w) if start_type[1]<0 else start_type[1]\n",
    "            selected_rics = list(set(np.random.choice(ric_universe, size=size_pos, replace=start_type[2])))\n",
    "            size_pos = len(selected_rics) #adjustment as same ric can be sample multiple times\n",
    "            if strat_==\"L\":\n",
    "                return {ric: 1/size_pos for ric in C_universe_w if ric in selected_rics}\n",
    "            if strat_==\"S\":\n",
    "                return {ric: -1/size_pos for ric in C_universe_w if ric in selected_rics}            \n",
    "            if strat_==\"L\":\n",
    "                long_list = set(random.sample(selected_rics, int(size_pos/2)))\n",
    "                short_list = selected_rics - long_list\n",
    "                return Counter({ric: 1/size_pos for ric in C_universe_w if ric in long_list}) + Counter({ric: -1/size_pos for ric in C_universe_w if ric in short_list})\n",
    "    else:\n",
    "        Universe_wOptimal = {ric: C_universe_w[ric]*(λ)+P_universe_w[ric]*(1-λ) for ric in C_universe_w}\n",
    "        return Universe_wOptimal\n",
    "\n",
    "def getFULL_optimal_universe_weight_dict(Return_by_Strat_date,n_companies,   strat_,start_type=[\"current\",0,False], λ_=1):\n",
    "    optimal_weight_list_ofDict, iter_n = [], 0\n",
    "    for index, row in Return_by_Strat_date.iterrows():\n",
    "        Pre_OP_dict = optimal_weight_list_ofDict[iter_n-1] if (iter_n-1)>=0 else {}\n",
    "        New_op_dict = getcurrent_optimal_universe_weight_dict(strat_, Pre_OP_dict, row[strat_+\"(\"+str(n_companies)+\")Weight_Universe[λ=\"+str(λ_)+\"]\"], start_type_, λ_)\n",
    "        optimal_weight_list_ofDict.append(New_op_dict)\n",
    "        iter_n +=1\n",
    "    return optimal_weight_list_ofDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb9ac6e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "def get_optimal_universe_weight_dict(strat_, date_, WUniverse_df, start_type=[\"current\",0,False], λ=1):\n",
    "    '''Adjust weights of universe based on fixed value to optimize trading costs\n",
    "    λ determines how much is changed each time (keeping 1-λ fixed)'''\n",
    "    # Get Main Data\n",
    "    loc_row = WUniverse_df.index.get_loc(date_)\n",
    "    C_universe_w = WUniverse_df.iloc[loc_row]\n",
    "    P_universe_w = WUniverse_df.iloc[loc_row-1] \n",
    "    #Define Starting Point  (3 scenarios, EW based on sentiment, EW random x companies)\n",
    "    if loc_row==0:\n",
    "        if start_type[0]==\"current\":   # use current allocation as dictated by sentiment\n",
    "            return C_universe_w\n",
    "        elif start_type[0]==\"random\":   # use EW across random number of elements in universe\n",
    "            ric_universe = list(C_universe_w)\n",
    "            size_pos = len(C_universe_w) if start_type[1]<0 else start_type[1]\n",
    "            selected_rics = list(set(np.random.choice(ric_universe, size=size_pos, replace=start_type[2])))\n",
    "            size_pos = len(selected_rics) #adjustment as same ric can be sample multiple times\n",
    "            if strat_==\"L\":\n",
    "                return {ric: 1/size_pos for ric in C_universe_w if ric in selected_rics}\n",
    "            if strat_==\"S\":\n",
    "                return {ric: -1/size_pos for ric in C_universe_w if ric in selected_rics}            \n",
    "            if strat_==\"L\":\n",
    "                long_list = set(random.sample(selected_rics, int(size_pos/2)))\n",
    "                short_list = selected_rics - long_list\n",
    "                return Counter({ric: 1/size_pos for ric in C_universe_w if ric in long_list}) + Counter({ric: -1/size_pos for ric in C_universe_w if ric in short_list})\n",
    "    else:\n",
    "        Universe_wOptimal = Counter({ric:p_w*(1-λ) for ric, p_w in P_universe_w.items()}) + Counter({ric:c_w*(λ) for ric, c_w in C_universe_w.items()})\n",
    "        return Universe_wOptimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd3cd98",
   "metadata": {},
   "source": [
    "## Data Complements for Statistics\n",
    "\n",
    "Fama/French 3 Factor >  https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654a91b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Full Data Joined & without consistent beggining and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbab75",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SentProxyChosen = \"AbsRobust\"    # Original   RelRobust   AbsRobust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af85b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#  Original >    \"4)Eikon_Classification/Daily_Complete_HeadlineReturn\"\n",
    "#  Robustness Relative Measure 1 >   \"4)Eikon_Classification/Daily_Complete_HeadlineReturn_RelativeRobust\"\n",
    "#  Robustness Absolute Measure 2 >   \"4)Eikon_Classification/Daily_Complete_HeadlineReturn_AbsoluteRobust\"\n",
    "\n",
    "#Load Saved Dataframe with IntraDay Sentiment classifications\n",
    "if SentProxyChosen == \"Original\":\n",
    "    with open(\"4)Eikon_Classification/Daily_Complete_HeadlineReturn\", \"rb\") as fp:  \n",
    "        InvestData = pickle.load(fp)\n",
    "    print(\"OG\")\n",
    "elif SentProxyChosen == \"RelRobust\":\n",
    "    with open(\"4)Eikon_Classification/Daily_Complete_HeadlineReturn_RelativeRobust\", \"rb\") as fp:  \n",
    "        InvestData = pickle.load(fp)\n",
    "    print(\"R1\")\n",
    "elif SentProxyChosen == \"AbsRobust\":\n",
    "    with open(\"4)Eikon_Classification/Daily_Complete_HeadlineReturn_AbsoluteRobust\", \"rb\") as fp:  \n",
    "        InvestData = pickle.load(fp)\n",
    "    print(\"R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022baec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fama French Data 5 FACTOR      Note: Data is in percentage\n",
    "FF5_Data = pd.read_csv(\"5)InvestmentStrat/F-F_Research_Data_5_Factors_daily.csv\", header=2)\n",
    "FF5_Data.rename(columns={\"Unnamed: 0\":\"Date\"}, inplace=True)\n",
    "\n",
    "# Fama French Data MOMENTUM     Note: Data is in percentage\n",
    "MOM_Data = pd.read_csv(\"5)InvestmentStrat/F-F_Momentum_Factor_daily.csv\", header=11)\n",
    "MOM_Data.rename(columns={\"Unnamed: 0\":\"Date\",'Mom   ':'Mom'}, inplace=True)\n",
    "MOM_Data.drop(labels=[25171],axis=0, inplace=True)\n",
    "MOM_Data[\"Date\"] = pd.to_numeric(MOM_Data[\"Date\"])\n",
    "\n",
    "#Merge FF Data\n",
    "FamaFrench_Data = pd.merge(FF5_Data,MOM_Data, how=\"inner\", on=\"Date\")\n",
    "print(\"Size 5Factor\", len(FF5_Data), \"  | Size MomFactor\", len(MOM_Data), \" | JOINED\", len(FamaFrench_Data))\n",
    "display(FamaFrench_Data.head(2))\n",
    "\n",
    "#Clean Data\n",
    "FamaFrench_Data[\"DateAdj\"] = FamaFrench_Data.apply(lambda x: pd.to_datetime(str(int(x[\"Date\"])), format='%Y-%m-%d'), axis=1)\n",
    "    #(convert to decimals)\n",
    "FamaFrench_Data[[col for col in FamaFrench_Data.columns if col not in [\"Date\",\"DateAdj\"]]] = FamaFrench_Data[[col for col in FamaFrench_Data.columns if col not in [\"Date\",\"DateAdj\"]]]/100\n",
    "display(FamaFrench_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc30f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Join all Data (returns & sentiment & 3FF)\n",
    "Invest_FullData = pd.merge(InvestData,FamaFrench_Data, how=\"left\", on=['DateAdj'])\n",
    "display(Invest_FullData.head(2))\n",
    "#Duplicate check\n",
    "Invest_FullData[Invest_FullData.duplicated(subset=['DateAdj','RIC'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac852bb6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_output = input(\"Save Investment Data: \")\n",
    "if save_output == \"yes\":\n",
    "    if SentProxyChosen == \"Original\":\n",
    "        with open(\"5)InvestmentStrat/InvestmentData_FULL\", \"wb\") as fp:   \n",
    "            pickle.dump(Invest_FullData, fp)\n",
    "        print(\"OG\")\n",
    "    elif SentProxyChosen == \"RelRobust\":\n",
    "        with open(\"5)InvestmentStrat/InvestmentData_FULL_RelRobust\", \"wb\") as fp:   \n",
    "            pickle.dump(Invest_FullData, fp)\n",
    "        print(\"R1\")\n",
    "    elif SentProxyChosen == \"AbsRobust\":\n",
    "        with open(\"5)InvestmentStrat/InvestmentData_FULL_AbsRobust\", \"wb\") as fp:   \n",
    "            pickle.dump(Invest_FullData, fp)\n",
    "        print(\"R2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a808f3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Aligned Pivot Tables for Portfolios Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd64cf7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SentProxyChosen = \"AbsRobust\"    # Original   RelRobust   AbsRobust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e2427",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with IntraDay Sentiment classifications\n",
    "if SentProxyChosen == \"Original\":\n",
    "    with open(\"5)InvestmentStrat/InvestmentData_FULL\", \"rb\") as fp:  \n",
    "        Invest_FullData = pickle.load(fp)\n",
    "    print(\"OG\")\n",
    "elif SentProxyChosen == \"RelRobust\":\n",
    "    with open(\"5)InvestmentStrat/InvestmentData_FULL_RelRobust\", \"rb\") as fp:  \n",
    "        Invest_FullData = pickle.load(fp)\n",
    "    print(\"R1\")\n",
    "elif SentProxyChosen == \"AbsRobust\":\n",
    "    with open(\"5)InvestmentStrat/InvestmentData_FULL_AbsRobust\", \"rb\") as fp:  \n",
    "        Invest_FullData = pickle.load(fp)\n",
    "    print(\"R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e230c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#For Each Eliminate rows if All NULL\n",
    "    #Return by date and company\n",
    "Ret_date_company = Invest_FullData.pivot(index='DateAdj', columns='RIC', values='RET')  \n",
    "print(\"Ret PIVOT\", len(Ret_date_company), \" Dropped Dates\", list(Ret_date_company[Ret_date_company.isnull().all(axis=1)].index))\n",
    "Ret_date_company = Ret_date_company[~Ret_date_company.isnull().all(axis=1)].copy()\n",
    "    #Lagged sent by date and company\n",
    "LagSent_date_company = Invest_FullData.pivot(index='DateAdj', columns='RIC', values='DailyAvgSent(t-1)') \n",
    "print(\"L Sent PIVOT\", len(LagSent_date_company), \" Dropped Dates\", list(LagSent_date_company[LagSent_date_company.isnull().all(axis=1)].index))\n",
    "LagSent_date_company = LagSent_date_company[~LagSent_date_company.isnull().all(axis=1)].copy()\n",
    "    #Lagged Market Cap by date and company\n",
    "LagMktCap_date_company = Invest_FullData.pivot(index='DateAdj', columns='RIC', values='MktCap(t-1)') \n",
    "print(\"L MktCap PIVOT\", len(LagMktCap_date_company), \" Dropped Dates\", list(LagMktCap_date_company[LagMktCap_date_company.isnull().all(axis=1)].index))\n",
    "LagMktCap_date_company = LagMktCap_date_company[~LagMktCap_date_company.isnull().all(axis=1)].copy()\n",
    "\n",
    "#Keep only dates in common\n",
    "common_dates = sorted(list(set(set(Ret_date_company.index).intersection(LagSent_date_company.index)).intersection(LagMktCap_date_company.index)))\n",
    "Ret_date_company = Ret_date_company.loc[common_dates]\n",
    "LagSent_date_company = LagSent_date_company.loc[common_dates]\n",
    "LagMktCap_date_company = LagMktCap_date_company.loc[common_dates]\n",
    "Invest_FullData_Limited = Invest_FullData[Invest_FullData[\"DateAdj\"].isin(common_dates)].drop_duplicates(subset=[\"DateAdj\"])\n",
    "print(\"Final Size\", len(Ret_date_company), len(LagSent_date_company), len(LagMktCap_date_company))\n",
    "print(\"Slimmed Stats\", len(Invest_FullData_Limited))\n",
    "\n",
    "#Save all in Dict Variable\n",
    "Invest_Pivots = {\"Ret\":Ret_date_company,\n",
    "                 \"LagSent\":LagSent_date_company,\n",
    "                 \"LagMktCap\":LagMktCap_date_company,\n",
    "                 \"InvestData\":Invest_FullData_Limited}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e965bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_output = input(\"Save Investment Data: \")\n",
    "if save_output == \"yes\":\n",
    "    if SentProxyChosen == \"Original\":\n",
    "        with open(\"5)InvestmentStrat/InvestmentData_PivotsDict\", \"wb\") as fp:   \n",
    "            pickle.dump(Invest_Pivots, fp)\n",
    "        print(\"OG\")\n",
    "    elif SentProxyChosen == \"RelRobust\":\n",
    "        with open(\"5)InvestmentStrat/InvestmentData_PivotsDict_RelRobust\", \"wb\") as fp:   \n",
    "            pickle.dump(Invest_Pivots, fp)\n",
    "        print(\"R1\")\n",
    "    elif SentProxyChosen == \"AbsRobust\":\n",
    "        with open(\"5)InvestmentStrat/InvestmentData_PivotsDict_AbsRobust\", \"wb\") as fp:   \n",
    "            pickle.dump(Invest_Pivots, fp)\n",
    "        print(\"R2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d45e8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initiate NOCost Data Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92ddda",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SentProxyChosenInv = \"AbsRobust\"    # Original   RelRobust   AbsRobust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104cd439",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#SAVE DATA\n",
    "save_data_file_nocost = input(\"Create File to Save the Data?\")\n",
    "if save_data_file_nocost ==\"yes\":\n",
    "    datestring_time = datetime.strftime(datetime.now(),\"%m_%d_%Y\")\n",
    "    if SentProxyChosenInv==\"Original\":\n",
    "        NoCost_writer = pd.ExcelWriter(\"3)Tables_PLots/\" + str(datestring_time) + \"__Investment_Strat_NoCOST.xlsx\", engine='xlsxwriter')\n",
    "        file_name = \"InvestmentData_PivotsDict\"\n",
    "    elif SentProxyChosenInv==\"RelRobust\":\n",
    "        NoCost_writer = pd.ExcelWriter(\"3)Tables_PLots/\" + str(datestring_time) + \"__Investment_Strat_NoCOST_RelRobust.xlsx\", engine='xlsxwriter')\n",
    "        file_name = \"InvestmentData_PivotsDict_RelRobust\"\n",
    "    elif SentProxyChosenInv==\"AbsRobust\":\n",
    "        NoCost_writer = pd.ExcelWriter(\"3)Tables_PLots/\" + str(datestring_time) + \"__Investment_Strat_NoCOST_AbsRobust.xlsx\", engine='xlsxwriter')\n",
    "        file_name = \"InvestmentData_PivotsDict_AbsRobust\"\n",
    "        \n",
    "save_data_nocost = input(\"Save the Data?\")\n",
    "if save_data_nocost ==\"yes\":\n",
    "    NoCost_writer.save()\n",
    "    \n",
    "save_data_nocost = \"yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7a27e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Benchmark\n",
    "Buy and Hold S&P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781833b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663ef47",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Get Full data of returns for the period for which the sent startegy is applicable\n",
    "Buy_Hold = Invest_FullData.drop_duplicates(subset=[\"DateAdj\"])\n",
    "Buy_Hold = Buy_Hold.sort_values(by='DateAdj')\n",
    "#Cumulative\n",
    "Buy_Hold[\"Cumulative\"] = (Buy_Hold[\"sprtrn\"]+1).cumprod()\n",
    "#Visual\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(Buy_Hold[\"DateAdj\"],Buy_Hold[\"Cumulative\"])\n",
    "#Table snippet\n",
    "display(Buy_Hold[['DateAdj','sprtrn', 'Cumulative', 'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF', 'Mom']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba024d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Data\n",
    "df = paper_stats_table(\"Buy & Hold\", \"sprtrn\",\"Cumulative\",  Buy_Hold)\n",
    "display(df)\n",
    "strat_data = pd.DataFrame()\n",
    "col_list = list(df.columns)\n",
    "for col in col_list:\n",
    "    for index, row in df.iterrows():\n",
    "        value = df.loc[index,col]\n",
    "        if not(pd.isnull(value)):\n",
    "            strat_data.loc[\"Buy-Hold\",f\"{col}{index}\"] = value\n",
    "#Save\n",
    "if save_data_nocost.lower() == \"yes\":\n",
    "    strat_data.to_excel(NoCost_writer, sheet_name='S&P',startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afb5dd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Basic Main Strats\n",
    "\n",
    "**No Transaction Costs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458652bf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Equal Weighted\n",
    "L, S, L-S   <>  Max 100 stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5f210",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98e264",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Construct List of top and bottom companies for portfolio creation\n",
    "#(auxiliary)\n",
    "n_companies = 100\n",
    "unique_rics = list(LagSent_date_company.columns)\n",
    "Return_by_Strat_date = pd.DataFrame()  \n",
    "#Iterate over each type of strat and compute statistics\n",
    "for strat_ in [\"L\", \"S\", \"L-S\"]:\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"] = LagSent_date_company.apply(lambda x: get_ric_pos_dict(strat_,n_companies,x.name,x,[Ret_date_company])  , axis=1)  \n",
    "    Return_by_Strat_date.sort_index(inplace=True)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")NCompanies\"] = Return_by_Strat_date.apply(lambda x:  {\"L\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == 1),\"S\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == -1)} , axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RET_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"] = Return_by_Strat_date.apply(lambda x: get_universe_weight_dict(unique_rics, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], None),  axis=1)    \n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_List\"] = Return_by_Strat_date.apply(lambda x: [x[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"][key] for key in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"]], axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Turnover\"] = Return_by_Strat_date.apply(lambda x: get_daily_turnover(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"],Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_] = Return_by_Strat_date.apply(lambda x: sum(ret * w for ret, w in zip(x[strat_+\"(\"+str(n_companies)+\")RET_List\"], x[strat_+\"(\"+str(n_companies)+\")Weight_List\"])),  axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Cumulative\"] =  (Return_by_Strat_date[strat_]+1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f3342",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Data with info for Stats\n",
    "Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),Invest_FullData[[\"DateAdj\",'MktCap(t-1)','Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']], how=\"inner\", on=\"DateAdj\")\n",
    "print(\"Data Final\", len(Portfolio_Stats_Data), \"Data Port\", len(Return_by_Strat_date), \"Data Baseline\", len(Invest_FullData))\n",
    "display(Portfolio_Stats_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac6d55",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Strat Stats\n",
    "strat_data = pd.DataFrame()\n",
    "plt.figure(figsize=(20, 5))\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index_s, strat in enumerate(['L', 'S', 'L-S']):  \n",
    "    #Data\n",
    "    df = paper_stats_table(strat, strat, strat+\" Cumulative\",  Portfolio_Stats_Data)\n",
    "    df.loc[strat, \"Turnover (%)\"] = np.mean(Portfolio_Stats_Data[strat+\" Turnover\"])*100\n",
    "    display(df)\n",
    "    col_list = list(df.columns)\n",
    "    for col in col_list:\n",
    "        for index, row in df.iterrows():\n",
    "            value = df.loc[index,col]\n",
    "            if not(pd.isnull(value)):\n",
    "                index_name = \"\" if index in  ['L', 'S', 'L-S'] else index\n",
    "                strat_data.loc[strat,f\"{col}{index_name}\"] = value\n",
    "    plt.plot(Portfolio_Stats_Data[\"DateAdj\"],Portfolio_Stats_Data[strat+\" Cumulative\"], label = strat, color='tab:'+color[index_s])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"EW Base Startegies\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Save\n",
    "if save_data_nocost.lower() == \"yes\":\n",
    "    strat_data.to_excel(NoCost_writer, sheet_name='EW',startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb014c51",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Value Weighted\n",
    "L, S, L-S   <>  Max 100 stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d3274",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f5632",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Construct List of top and bottom companies for portfolio creation\n",
    "#(auxiliary)\n",
    "n_companies = 100\n",
    "unique_rics = list(LagSent_date_company.columns)\n",
    "Return_by_Strat_date = pd.DataFrame()  \n",
    "#Iterate over each type of strat and compute statistics\n",
    "for strat_ in [\"L\", \"S\", \"L-S\"]:\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"] = LagSent_date_company.apply(lambda x: get_ric_pos_dict(strat_,n_companies,x.name,x,[Ret_date_company, LagMktCap_date_company])  , axis=1)  \n",
    "    Return_by_Strat_date.sort_index(inplace=True)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")NCompanies\"] = Return_by_Strat_date.apply(lambda x:  {\"L\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == 1),\"S\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == -1)} , axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RET_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")MKT_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], LagMktCap_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"] = Return_by_Strat_date.apply(lambda x: get_universe_weight_dict(unique_rics, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], x[strat_+\"(\"+str(n_companies)+\")MKT_List\"]),  axis=1)    \n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_List\"] = Return_by_Strat_date.apply(lambda x: [x[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"][key] for key in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"]], axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Turnover\"] = Return_by_Strat_date.apply(lambda x: get_daily_turnover(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"],Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_] = Return_by_Strat_date.apply(lambda x: sum(ret * w for ret, w in zip(x[strat_+\"(\"+str(n_companies)+\")RET_List\"], x[strat_+\"(\"+str(n_companies)+\")Weight_List\"])),  axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Cumulative\"] =  (Return_by_Strat_date[strat_]+1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc214a05",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Data with info for Stats\n",
    "Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),Invest_FullData[[\"DateAdj\",'MktCap(t-1)','Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']], how=\"inner\", on=\"DateAdj\")\n",
    "print(\"Data Final\", len(Portfolio_Stats_Data), \"Data Port\", len(Return_by_Strat_date), \"Data Baseline\", len(Invest_FullData))\n",
    "display(Portfolio_Stats_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf58d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Strat Stats\n",
    "strat_data = pd.DataFrame()\n",
    "plt.figure(figsize=(20, 5))\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index_s, strat in enumerate(['L', 'S', 'L-S']):  \n",
    "    #Data\n",
    "    df = paper_stats_table(strat, strat, strat+\" Cumulative\",  Portfolio_Stats_Data)\n",
    "    df.loc[strat, \"Turnover (%)\"] = np.mean(Portfolio_Stats_Data[strat+\" Turnover\"])*100\n",
    "    display(df)\n",
    "    col_list = list(df.columns)\n",
    "    for col in col_list:\n",
    "        for index, row in df.iterrows():\n",
    "            value = df.loc[index,col]\n",
    "            if not(pd.isnull(value)):\n",
    "                index_name = \"\" if index in  ['L', 'S', 'L-S'] else index\n",
    "                strat_data.loc[strat,f\"{col}{index_name}\"] = value\n",
    "    plt.plot(Portfolio_Stats_Data[\"DateAdj\"],Portfolio_Stats_Data[strat+\" Cumulative\"], label = strat, color='tab:'+color[index_s])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"VW Base Startegies\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Save\n",
    "if save_data_nocost.lower() == \"yes\":\n",
    "    strat_data.to_excel(NoCost_writer, sheet_name='VW',startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd858dbf",
   "metadata": {},
   "source": [
    "## Return Response on Shifted Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c02e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]   #return at t\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]\n",
    "#Additional needed\n",
    "Sent_date_company = LagSent_date_company.shift(periods=-1)  #sentiment at t\n",
    "PreviousRet_date_company = Ret_date_company.shift(periods=1) #ret at t-1\n",
    "NextRet_date_company = Ret_date_company.shift(periods=-1) #ret at t+1\n",
    "# FF\n",
    "FF_Data = Invest_FullData[[\"DateAdj\",'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']].set_index(\"DateAdj\")\n",
    "PreviousFF_Data = FF_Data.shift(periods=1)\n",
    "NextFF_Data = FF_Data.shift(periods=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea25c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct List of top and bottom companies for portfolio creation\n",
    "#(auxiliary)\n",
    "n_companies = 100\n",
    "unique_rics = list(Sent_date_company.columns)\n",
    "Return_by_Strat_date = pd.DataFrame()  \n",
    "# Loop over different Lagges\n",
    "for RET_Data, id_ in zip([PreviousRet_date_company,Ret_date_company,NextRet_date_company],[\"_D-1\",\"_D0\",\"_D+1\"]):\n",
    "    print(\"Computing stats for\", id_)\n",
    "    #Iterate over each type of strat and compute statistics\n",
    "    for strat_ in [\"L\", \"S\", \"L-S\"]:\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"+id_] = Sent_date_company.apply(lambda x: get_ric_pos_dict(strat_,n_companies,x.name,x,[RET_Data])  , axis=1).replace('None',np.NaN).fillna(method=\"ffill\")  \n",
    "        Return_by_Strat_date.sort_index(inplace=True)\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")NCompanies\"+id_] = Return_by_Strat_date.apply(lambda x:  {\"L\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"+id_].values() if v == 1),\"S\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"+id_].values() if v == -1)} , axis=1)\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RET_List\"+id_] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"+id_], RET_Data), axis=1)\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")MKT_List\"+id_] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"+id_], LagMktCap_date_company), axis=1)\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"+id_] = Return_by_Strat_date.apply(lambda x: get_universe_weight_dict(unique_rics, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"+id_], None),  axis=1)     \n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_List\"+id_] = Return_by_Strat_date.apply(lambda x: [x[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"+id_][key] for key in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"+id_]], axis=1)\n",
    "        Return_by_Strat_date[strat_+id_+\" Turnover\"] = Return_by_Strat_date.apply(lambda x: get_daily_turnover(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"+id_],RET_Data), axis=1)\n",
    "        Return_by_Strat_date[strat_+id_] = Return_by_Strat_date.apply(lambda x: sum(ret * w for ret, w in zip(x[strat_+\"(\"+str(n_companies)+\")RET_List\"+id_], x[strat_+\"(\"+str(n_companies)+\")Weight_List\"+id_])),  axis=1)\n",
    "        Return_by_Strat_date[strat_+\" Cumulative\"+id_] =  (Return_by_Strat_date[strat_+id_]+1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with info for Stats\n",
    "Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),Invest_FullData[[\"DateAdj\",'MktCap(t-1)','Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']], how=\"inner\", on=\"DateAdj\")\n",
    "print(\"Data Final\", len(Portfolio_Stats_Data), \"Data Port\", len(Return_by_Strat_date), \"Data Baseline\", len(Invest_FullData))\n",
    "display(Portfolio_Stats_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strat Stats\n",
    "plt.figure(figsize=(20, 5))\n",
    "color = [\"grey\",\"red\",\"blue\"]\n",
    "strat_type = ['L', 'S', 'L-S']\n",
    "line_type = ['-.','-','dotted']\n",
    "shift = [\"_D-1\",\"_D0\",\"_D+1\"]\n",
    "for strat in ['L', 'S', 'L-S']:\n",
    "    for t in [\"_D-1\",\"_D0\",\"_D+1\"]:\n",
    "        strat_var = strat+\" Cumulative\"+t\n",
    "        plt.plot(Portfolio_Stats_Data[\"DateAdj\"],Portfolio_Stats_Data[strat_var], label = strat_var, linestyle=line_type[shift.index(t)], color='tab:'+color[strat_type.index(strat)])\n",
    "plt.xlabel('Date')\n",
    "#plt.title(\"Shifted Sent Impact\")\n",
    "plt.legend()\n",
    "if save_data_nocost.lower() == \"yes\":\n",
    "    plt.savefig('3)Tables_PLots/SpeedInfoAssimilation_'+SentProxyChosenInv+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7cd29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_data = pd.DataFrame()\n",
    "for t, famafrench_df in zip([\"_D-1\",\"_D0\",\"_D+1\"],[PreviousFF_Data, FF_Data, NextFF_Data]):\n",
    "    famafrench_df.dropna(how='all', inplace=True)\n",
    "    Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),famafrench_df.reset_index(), how=\"inner\", on=\"DateAdj\")\n",
    "    for strat in ['L', 'S', 'L-S']:\n",
    "        df = paper_stats_table(strat+t, strat+t, strat+\" Cumulative\"+t,  Portfolio_Stats_Data)\n",
    "        df.loc[strat+t, \"Turnover (%)\"] = np.mean(Portfolio_Stats_Data[strat+t+\" Turnover\"])*100\n",
    "        display(df)\n",
    "        col_list = list(df.columns)\n",
    "        for col in col_list:\n",
    "            for index, row in df.iterrows():\n",
    "                value = df.loc[index,col]\n",
    "                if not(pd.isnull(value)):\n",
    "                    index_name = \"\" if index == strat+t else index\n",
    "                    strat_data.loc[strat+t,f\"{col}{index_name}\"] = value\n",
    "        \n",
    "        \n",
    "#Save stats\n",
    "if save_data_nocost.lower() == \"yes\": \n",
    "    # Position the dataframes in the worksheet.\n",
    "    strat_data.to_excel(NoCost_writer, sheet_name='InfoAssimilation'+strat,startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2f7de",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Alternative Strats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0b51a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 200 stocks (EW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8b47b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ff133b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Construct List of top and bottom companies for portfolio creation\n",
    "#(auxiliary)\n",
    "n_companies = 200\n",
    "unique_rics = list(LagSent_date_company.columns)\n",
    "Return_by_Strat_date = pd.DataFrame()  \n",
    "#Iterate over each type of strat and compute statistics\n",
    "for strat_ in [\"L\", \"S\", \"L-S\"]:\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"] = LagSent_date_company.apply(lambda x: get_ric_pos_dict(strat_,n_companies,x.name,x,[Ret_date_company])  , axis=1)  \n",
    "    Return_by_Strat_date.sort_index(inplace=True)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")NCompanies\"] = Return_by_Strat_date.apply(lambda x:  {\"L\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == 1),\"S\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == -1)} , axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RET_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"] = Return_by_Strat_date.apply(lambda x: get_universe_weight_dict(unique_rics, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], None),  axis=1)    \n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_List\"] = Return_by_Strat_date.apply(lambda x: [x[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"][key] for key in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"]], axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Turnover\"] = Return_by_Strat_date.apply(lambda x: get_daily_turnover(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"],Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_] = Return_by_Strat_date.apply(lambda x: sum(ret * w for ret, w in zip(x[strat_+\"(\"+str(n_companies)+\")RET_List\"], x[strat_+\"(\"+str(n_companies)+\")Weight_List\"])),  axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Cumulative\"] =  (Return_by_Strat_date[strat_]+1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fde7bf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Data with info for Stats\n",
    "Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),Invest_FullData[[\"DateAdj\",'MktCap(t-1)','Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']], how=\"inner\", on=\"DateAdj\")\n",
    "print(\"Data Final\", len(Portfolio_Stats_Data), \"Data Port\", len(Return_by_Strat_date), \"Data Baseline\", len(Invest_FullData))\n",
    "display(Portfolio_Stats_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30963d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Strat Stats\n",
    "strat_data = pd.DataFrame()\n",
    "plt.figure(figsize=(20, 5))\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index_s, strat in enumerate(['L', 'S', 'L-S']):  \n",
    "    #Data\n",
    "    df = paper_stats_table(strat, strat, strat+\" Cumulative\",  Portfolio_Stats_Data)\n",
    "    df.loc[strat, \"Turnover (%)\"] = np.mean(Portfolio_Stats_Data[strat+\" Turnover\"])*100\n",
    "    display(df)\n",
    "    col_list = list(df.columns)\n",
    "    for col in col_list:\n",
    "        for index, row in df.iterrows():\n",
    "            value = df.loc[index,col]\n",
    "            if not(pd.isnull(value)):\n",
    "                index_name = \"\" if index in  ['L', 'S', 'L-S'] else index\n",
    "                strat_data.loc[strat,f\"{col}{index_name}\"] = value\n",
    "    plt.plot(Portfolio_Stats_Data[\"DateAdj\"],Portfolio_Stats_Data[strat+\" Cumulative\"], label = strat, color='tab:'+color[index_s])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"200 stocks Base Startegies\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Save\n",
    "if save_data_nocost.lower() == \"yes\":\n",
    "    strat_data.to_excel(NoCost_writer, sheet_name='Stocks200EW',startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd49157",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 200 stocks (VW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47712db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca48e0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Construct List of top and bottom companies for portfolio creation\n",
    "#(auxiliary)\n",
    "n_companies = 200\n",
    "unique_rics = list(LagSent_date_company.columns)\n",
    "Return_by_Strat_date = pd.DataFrame()  \n",
    "#Iterate over each type of strat and compute statistics\n",
    "for strat_ in [\"L\", \"S\", \"L-S\"]:\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"] = LagSent_date_company.apply(lambda x: get_ric_pos_dict(strat_,n_companies,x.name,x,[Ret_date_company, LagMktCap_date_company])  , axis=1)  \n",
    "    Return_by_Strat_date.sort_index(inplace=True)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")NCompanies\"] = Return_by_Strat_date.apply(lambda x:  {\"L\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == 1),\"S\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == -1)} , axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RET_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")MKT_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], LagMktCap_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"] = Return_by_Strat_date.apply(lambda x: get_universe_weight_dict(unique_rics, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], x[strat_+\"(\"+str(n_companies)+\")MKT_List\"]),  axis=1)    \n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_List\"] = Return_by_Strat_date.apply(lambda x: [x[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"][key] for key in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"]], axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Turnover\"] = Return_by_Strat_date.apply(lambda x: get_daily_turnover(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"],Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_] = Return_by_Strat_date.apply(lambda x: sum(ret * w for ret, w in zip(x[strat_+\"(\"+str(n_companies)+\")RET_List\"], x[strat_+\"(\"+str(n_companies)+\")Weight_List\"])),  axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Cumulative\"] =  (Return_by_Strat_date[strat_]+1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394284c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Data with info for Stats\n",
    "Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),Invest_FullData[[\"DateAdj\",'MktCap(t-1)','Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']], how=\"inner\", on=\"DateAdj\")\n",
    "print(\"Data Final\", len(Portfolio_Stats_Data), \"Data Port\", len(Return_by_Strat_date), \"Data Baseline\", len(Invest_FullData))\n",
    "display(Portfolio_Stats_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6ee10",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Strat Stats\n",
    "strat_data = pd.DataFrame()\n",
    "plt.figure(figsize=(20, 5))\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index_s, strat in enumerate(['L', 'S', 'L-S']):  \n",
    "    #Data\n",
    "    df = paper_stats_table(strat, strat, strat+\" Cumulative\",  Portfolio_Stats_Data)\n",
    "    df.loc[strat, \"Turnover (%)\"] = np.mean(Portfolio_Stats_Data[strat+\" Turnover\"])*100\n",
    "    display(df)\n",
    "    col_list = list(df.columns)\n",
    "    for col in col_list:\n",
    "        for index, row in df.iterrows():\n",
    "            value = df.loc[index,col]\n",
    "            if not(pd.isnull(value)):\n",
    "                index_name = \"\" if index in  ['L', 'S', 'L-S'] else index\n",
    "                strat_data.loc[strat,f\"{col}{index_name}\"] = value\n",
    "    plt.plot(Portfolio_Stats_Data[\"DateAdj\"],Portfolio_Stats_Data[strat+\" Cumulative\"], label = strat, color='tab:'+color[index_s])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"200 stocks Base Startegies\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Save\n",
    "if save_data_nocost.lower() == \"yes\":\n",
    "    strat_data.to_excel(NoCost_writer, sheet_name='Stocks200VW',startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45f018f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Follow the Loser (EW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28368593",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f0aa4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Construct List of top and bottom companies for portfolio creation\n",
    "#(auxiliary)\n",
    "n_companies = 100\n",
    "unique_rics = list(LagSent_date_company.columns)\n",
    "Return_by_Strat_date = pd.DataFrame()  \n",
    "#Iterate over each type of strat and compute statistics\n",
    "for strat_ in [\"L\", \"S\", \"L-S\"]:\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict_FW\"] = LagSent_date_company.apply(lambda x: get_ric_pos_dict(strat_,n_companies,x.name,x,[Ret_date_company])  , axis=1)  \n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"] = Return_by_Strat_date.apply(lambda x: {key:value*(-1) for key, value in x [strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict_FW\"].items()}, axis=1) \n",
    "    Return_by_Strat_date.sort_index(inplace=True)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")NCompanies\"] = Return_by_Strat_date.apply(lambda x:  {\"L\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == 1),\"S\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == -1)} , axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RET_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"] = Return_by_Strat_date.apply(lambda x: get_universe_weight_dict(unique_rics, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], None),  axis=1)    \n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_List\"] = Return_by_Strat_date.apply(lambda x: [x[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"][key] for key in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"]], axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Turnover\"] = Return_by_Strat_date.apply(lambda x: get_daily_turnover(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"],Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_] = Return_by_Strat_date.apply(lambda x: sum(ret * w for ret, w in zip(x[strat_+\"(\"+str(n_companies)+\")RET_List\"], x[strat_+\"(\"+str(n_companies)+\")Weight_List\"])),  axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Cumulative\"] =  (Return_by_Strat_date[strat_]+1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f11767",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Data with info for Stats\n",
    "Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),Invest_FullData[[\"DateAdj\",'MktCap(t-1)','Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']], how=\"inner\", on=\"DateAdj\")\n",
    "print(\"Data Final\", len(Portfolio_Stats_Data), \"Data Port\", len(Return_by_Strat_date), \"Data Baseline\", len(Invest_FullData))\n",
    "display(Portfolio_Stats_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006d69a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Strat Stats\n",
    "strat_data = pd.DataFrame()\n",
    "plt.figure(figsize=(20, 5))\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index_s, strat in enumerate(['L', 'S', 'L-S']):  \n",
    "    #Data\n",
    "    df = paper_stats_table(strat, strat, strat+\" Cumulative\",  Portfolio_Stats_Data)\n",
    "    df.loc[strat, \"Turnover (%)\"] = np.mean(Portfolio_Stats_Data[strat+\" Turnover\"])*100\n",
    "    display(df)\n",
    "    col_list = list(df.columns)\n",
    "    for col in col_list:\n",
    "        for index, row in df.iterrows():\n",
    "            value = df.loc[index,col]\n",
    "            if not(pd.isnull(value)):\n",
    "                index_name = \"\" if index in  ['L', 'S', 'L-S'] else index\n",
    "                strat_data.loc[strat,f\"{col}{index_name}\"] = value\n",
    "    plt.plot(Portfolio_Stats_Data[\"DateAdj\"],Portfolio_Stats_Data[strat+\" Cumulative\"], label = strat, color='tab:'+color[index_s])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"200 stocks Base Startegies\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Save\n",
    "if save_data_nocost.lower() == \"yes\":\n",
    "    strat_data.to_excel(NoCost_writer, sheet_name='FollowLoserEW',startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec61606",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Follow the Loser (VW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4259a8d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32c70d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Construct List of top and bottom companies for portfolio creation\n",
    "#(auxiliary)\n",
    "n_companies = 100\n",
    "unique_rics = list(LagSent_date_company.columns)\n",
    "Return_by_Strat_date = pd.DataFrame()  \n",
    "#Iterate over each type of strat and compute statistics\n",
    "for strat_ in [\"L\", \"S\", \"L-S\"]:\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict_FW\"] = LagSent_date_company.apply(lambda x: get_ric_pos_dict(strat_,n_companies,x.name,x,[Ret_date_company, LagMktCap_date_company])  , axis=1)  \n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"] = Return_by_Strat_date.apply(lambda x: {key:value*(-1) for key, value in x [strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict_FW\"].items()}, axis=1) \n",
    "    Return_by_Strat_date.sort_index(inplace=True)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")NCompanies\"] = Return_by_Strat_date.apply(lambda x:  {\"L\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == 1),\"S\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == -1)} , axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RET_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")MKT_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], LagMktCap_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"] = Return_by_Strat_date.apply(lambda x: get_universe_weight_dict(unique_rics, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], x[strat_+\"(\"+str(n_companies)+\")MKT_List\"]),  axis=1)    \n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_List\"] = Return_by_Strat_date.apply(lambda x: [x[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"][key] for key in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"]], axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Turnover\"] = Return_by_Strat_date.apply(lambda x: get_daily_turnover(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"],Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_] = Return_by_Strat_date.apply(lambda x: sum(ret * w for ret, w in zip(x[strat_+\"(\"+str(n_companies)+\")RET_List\"], x[strat_+\"(\"+str(n_companies)+\")Weight_List\"])),  axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Cumulative\"] =  (Return_by_Strat_date[strat_]+1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8deed6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Data with info for Stats\n",
    "Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),Invest_FullData[[\"DateAdj\",'MktCap(t-1)','Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']], how=\"inner\", on=\"DateAdj\")\n",
    "print(\"Data Final\", len(Portfolio_Stats_Data), \"Data Port\", len(Return_by_Strat_date), \"Data Baseline\", len(Invest_FullData))\n",
    "display(Portfolio_Stats_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0f3b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Strat Stats\n",
    "strat_data = pd.DataFrame()\n",
    "plt.figure(figsize=(20, 5))\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index_s, strat in enumerate(['L', 'S', 'L-S']):  \n",
    "    #Data\n",
    "    df = paper_stats_table(strat, strat, strat+\" Cumulative\",  Portfolio_Stats_Data)\n",
    "    df.loc[strat, \"Turnover (%)\"] = np.mean(Portfolio_Stats_Data[strat+\" Turnover\"])*100\n",
    "    display(df)\n",
    "    col_list = list(df.columns)\n",
    "    for col in col_list:\n",
    "        for index, row in df.iterrows():\n",
    "            value = df.loc[index,col]\n",
    "            if not(pd.isnull(value)):\n",
    "                index_name = \"\" if index in  ['L', 'S', 'L-S'] else index\n",
    "                strat_data.loc[strat,f\"{col}{index_name}\"] = value\n",
    "    plt.plot(Portfolio_Stats_Data[\"DateAdj\"],Portfolio_Stats_Data[strat+\" Cumulative\"], label = strat, color='tab:'+color[index_s])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"200 stocks Base Startegies\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Save\n",
    "if save_data_nocost.lower() == \"yes\":\n",
    "    strat_data.to_excel(NoCost_writer, sheet_name='FollowLoserVW',startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044ff35",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initiate NoCost Data Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118fc9a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SentProxyChosenInvCOST = \"AbsRobust\"    # Original   RelRobust   AbsRobust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb43ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#SAVE DATA\n",
    "save_data_file_cost = input(\"Create File to Save the Data?\")\n",
    "if save_data_file_cost ==\"yes\":\n",
    "    datestring_time = datetime.strftime(datetime.now(),\"%m_%d_%Y\")\n",
    "    if SentProxyChosenInvCOST==\"Original\":\n",
    "        Cost_writer = pd.ExcelWriter(\"3)Tables_PLots/\" + str(datestring_time) + \"__Investment_Strat_COST.xlsx\", engine='xlsxwriter')\n",
    "        file_name = \"InvestmentData_PivotsDict\"\n",
    "    elif SentProxyChosenInvCOST==\"RelRobust\":\n",
    "        Cost_writer = pd.ExcelWriter(\"3)Tables_PLots/\" + str(datestring_time) + \"__Investment_Strat_COST_RelRobust.xlsx\", engine='xlsxwriter')\n",
    "        file_name = \"InvestmentData_PivotsDict_RelRobust\"\n",
    "    elif SentProxyChosenInvCOST==\"AbsRobust\":\n",
    "        Cost_writer = pd.ExcelWriter(\"3)Tables_PLots/\" + str(datestring_time) + \"__Investment_Strat_COST_AbsRobust.xlsx\", engine='xlsxwriter')\n",
    "        file_name = \"InvestmentData_PivotsDict_AbsRobust\"\n",
    "        \n",
    "save_data_cost = input(\"Save the Data?\")\n",
    "if save_data_cost ==\"yes\":\n",
    "    Cost_writer.save()\n",
    "    \n",
    "save_data_cost = \"yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72fbf8",
   "metadata": {},
   "source": [
    "## Economic Viability Portfolios (Basic)\n",
    "\n",
    "**Transaction Costs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd716a0a",
   "metadata": {},
   "source": [
    "### Applied to EW Basic Strats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c52fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d409c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct List of top and bottom companies for portfolio creation\n",
    "#(auxiliary)\n",
    "n_companies = 100\n",
    "unique_rics = list(LagSent_date_company.columns)\n",
    "Return_by_Strat_date = pd.DataFrame()  \n",
    "#Iterate over each type of strat and compute statistics\n",
    "for strat_ in [\"L\", \"S\", \"L-S\"]:\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"] = LagSent_date_company.apply(lambda x: get_ric_pos_dict(strat_,n_companies,x.name,x,[Ret_date_company])  , axis=1)  \n",
    "    Return_by_Strat_date.sort_index(inplace=True)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")NCompanies\"] = Return_by_Strat_date.apply(lambda x:  {\"L\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == 1),\"S\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == -1)} , axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RET_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"] = Return_by_Strat_date.apply(lambda x: get_universe_weight_dict(unique_rics, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], None),  axis=1)    \n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_List\"] = Return_by_Strat_date.apply(lambda x: [x[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"][key] for key in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"]], axis=1)\n",
    "    Return_by_Strat_date[strat_] = Return_by_Strat_date.apply(lambda x: sum(ret * w for ret, w in zip(x[strat_+\"(\"+str(n_companies)+\")RET_List\"], x[strat_+\"(\"+str(n_companies)+\")Weight_List\"])),  axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Cumulative\"] =  (Return_by_Strat_date[strat_]+1).cumprod()\n",
    "    Return_by_Strat_date[strat_+\" Costs\"] = Return_by_Strat_date.apply(lambda x: get_transaction_costs(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"]), axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Net\"] =  Return_by_Strat_date[strat_] - Return_by_Strat_date[strat_+\" Costs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f423e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with info for Stats\n",
    "Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),Invest_FullData[[\"DateAdj\",'MktCap(t-1)','Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']], how=\"inner\", on=\"DateAdj\")\n",
    "print(\"Data Final\", len(Portfolio_Stats_Data), \"Data Port\", len(Return_by_Strat_date), \"Data Baseline\", len(Invest_FullData))\n",
    "display(Portfolio_Stats_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c392301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strat Stats\n",
    "strat_data = pd.DataFrame()\n",
    "for strat in ['L', 'S', 'L-S']:\n",
    "    print( \"\\t Strat: \",strat)\n",
    "    df = paper_stats_table_Costs(strat, strat, strat+\" Net\",  Portfolio_Stats_Data)\n",
    "    display(df)\n",
    "    col_list = list(df.columns)\n",
    "    for col in col_list:\n",
    "        for index, row in df.iterrows():\n",
    "            index_name = index if index not in  ['L', 'S', 'L-S'] else \"\"\n",
    "            strat_data.loc[strat,f\"{col}{index_name}\"] = df.loc[index,col]\n",
    "#Save\n",
    "strat_data.to_excel(Cost_writer, sheet_name=str(n_companies)+'EWCost',startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c755798",
   "metadata": {},
   "source": [
    "### Applied to VW Basic Strats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab125c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86395a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct List of top and bottom companies for portfolio creation\n",
    "#(auxiliary)\n",
    "n_companies = 100\n",
    "unique_rics = list(LagSent_date_company.columns)\n",
    "Return_by_Strat_date = pd.DataFrame()  \n",
    "#Iterate over each type of strat and compute statistics\n",
    "for strat_ in [\"L\", \"S\", \"L-S\"]:\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"] = LagSent_date_company.apply(lambda x: get_ric_pos_dict(strat_,n_companies,x.name,x,[Ret_date_company, LagMktCap_date_company])  , axis=1)  \n",
    "    Return_by_Strat_date.sort_index(inplace=True)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")NCompanies\"] = Return_by_Strat_date.apply(lambda x:  {\"L\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == 1),\"S\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"].values() if v == -1)} , axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RET_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], Ret_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")MKT_List\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], LagMktCap_date_company), axis=1)\n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"] = Return_by_Strat_date.apply(lambda x: get_universe_weight_dict(unique_rics, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"], x[strat_+\"(\"+str(n_companies)+\")MKT_List\"]),  axis=1)    \n",
    "    Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_List\"] = Return_by_Strat_date.apply(lambda x: [x[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"][key] for key in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict\"]], axis=1)\n",
    "    Return_by_Strat_date[strat_] = Return_by_Strat_date.apply(lambda x: sum(ret * w for ret, w in zip(x[strat_+\"(\"+str(n_companies)+\")RET_List\"], x[strat_+\"(\"+str(n_companies)+\")Weight_List\"])),  axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Cumulative\"] =  (Return_by_Strat_date[strat_]+1).cumprod()\n",
    "    Return_by_Strat_date[strat_+\" Costs\"] = Return_by_Strat_date.apply(lambda x: get_transaction_costs(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe\"]), axis=1)\n",
    "    Return_by_Strat_date[strat_+\" Net\"] =  Return_by_Strat_date[strat_] - Return_by_Strat_date[strat_+\" Costs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8111524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with info for Stats\n",
    "Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),Invest_FullData[[\"DateAdj\",'MktCap(t-1)','Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']], how=\"inner\", on=\"DateAdj\")\n",
    "print(\"Data Final\", len(Portfolio_Stats_Data), \"Data Port\", len(Return_by_Strat_date), \"Data Baseline\", len(Invest_FullData))\n",
    "display(Portfolio_Stats_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strat Stats\n",
    "strat_data = pd.DataFrame()\n",
    "for strat in ['L', 'S', 'L-S']:\n",
    "    print( \"\\t Strat: \",strat)\n",
    "    df = paper_stats_table_Costs(strat, strat, strat+\" Net\",  Portfolio_Stats_Data)\n",
    "    display(df)\n",
    "    col_list = list(df.columns)\n",
    "    for col in col_list:\n",
    "        for index, row in df.iterrows():\n",
    "            index_name = index if index not in  ['L', 'S', 'L-S'] else \"\"\n",
    "            strat_data.loc[strat,f\"{col}{index_name}\"] = df.loc[index,col]\n",
    "#Save\n",
    "strat_data.to_excel(Cost_writer, sheet_name=str(n_companies)+'VWCost',startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524c319",
   "metadata": {},
   "source": [
    "## Economic Viability Portfolios (OPTIMMIZED)\n",
    "\n",
    "**Transaction Costs**\n",
    "Fixed percentage adjusted each time (theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786dcb15",
   "metadata": {},
   "source": [
    "### Start Based on 1st Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4791c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf532a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct List of top and bottom companies for portfolio creation  \n",
    "#(auxiliary)\n",
    "n_companies = 100\n",
    "start_type_ = [\"current\",n_companies,True]   # first element can be  \"current\"   or \"random\"\n",
    "unique_rics = list(LagSent_date_company.columns)\n",
    "Return_by_Strat_date = pd.DataFrame()  \n",
    "# Loop over different lambdas (percentage of positions to change) max 1 (= change everthing) and min 0 (=change nothing)\n",
    "for λ_ in range(1,10):\n",
    "    λ_/= 10\n",
    "    #Iterate over each type of strat and compute statistics\n",
    "    for strat_ in [\"L\", \"S\", \"L-S\"]:\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"] = LagSent_date_company.apply(lambda x: get_ric_pos_dict(strat_,n_companies,x.name,x,[Ret_date_company])  , axis=1)  \n",
    "        Return_by_Strat_date.sort_index(inplace=True)\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")NCompanies[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x:  {\"L\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"].values() if v == 1),\"S\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"].values() if v == -1)} , axis=1)\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RET_List[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"], Ret_date_company), axis=1)\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x: get_universe_weight_dict(unique_rics, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"], None),  axis=1)    \n",
    "        \n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Optimal_Weight_Universe[λ=\"+str(λ_)+\"]\"] = getFULL_optimal_universe_weight_dict(Return_by_Strat_date,n_companies,   strat_,start_type_, λ_) \n",
    "\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_List[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x: [x[strat_+\"(\"+str(n_companies)+\")Optimal_Weight_Universe[λ=\"+str(λ_)+\"]\"][key] for key in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"]], axis=1)\n",
    "        Return_by_Strat_date[strat_+\"[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x: sum(ret * w for ret, w in zip(x[strat_+\"(\"+str(n_companies)+\")RET_List[λ=\"+str(λ_)+\"]\"], x[strat_+\"(\"+str(n_companies)+\")Weight_List[λ=\"+str(λ_)+\"]\"])),  axis=1)\n",
    "        Return_by_Strat_date[strat_+\"[λ=\"+str(λ_)+\"]\"+\" Turnover\"] = Return_by_Strat_date.apply(lambda x: get_daily_turnover(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Optimal_Weight_Universe[λ=\"+str(λ_)+\"]\"],Ret_date_company), axis=1)\n",
    "        Return_by_Strat_date[strat_+\" Cumulative[λ=\"+str(λ_)+\"]\"] =  (Return_by_Strat_date[strat_+\"[λ=\"+str(λ_)+\"]\"]+1).cumprod()\n",
    "        Return_by_Strat_date[strat_+\" Costs[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x: get_transaction_costs(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Optimal_Weight_Universe[λ=\"+str(λ_)+\"]\"]), axis=1)\n",
    "        Return_by_Strat_date[strat_+\" Net[λ=\"+str(λ_)+\"]\"] =  Return_by_Strat_date[strat_+\"[λ=\"+str(λ_)+\"]\"] - Return_by_Strat_date[strat_+\" Costs[λ=\"+str(λ_)+\"]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with info for Stats\n",
    "Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),Invest_FullData[[\"DateAdj\",'MktCap(t-1)','Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']], how=\"inner\", on=\"DateAdj\")\n",
    "print(\"Data Final\", len(Portfolio_Stats_Data), \"Data Port\", len(Return_by_Strat_date), \"Data Baseline\", len(Invest_FullData))\n",
    "display(Portfolio_Stats_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1403bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_check = []\n",
    "#Strat Stats\n",
    "for strat in ['L', 'S', 'L-S']:\n",
    "    strat_data = pd.DataFrame()\n",
    "    for λ_ in range(1,10):\n",
    "        λ_/= 10\n",
    "        strat_ = strat+\"[λ=\"+str(λ_)+\"]\"\n",
    "        print( \"\\t Strat: \",strat_)    # name to show, column of returns, column of net returns, dataframe\n",
    "        df = paper_stats_table_Costs(strat_, strat_, strat+\" Net[λ=\"+str(λ_)+\"]\",  Portfolio_Stats_Data)\n",
    "        df.loc[strat_, \"Turnover (%)\"] = np.mean(Portfolio_Stats_Data[strat_+\" Turnover\"].iloc[1:])*100\n",
    "        display(df)\n",
    "        col_list = list(df.columns)\n",
    "        for col in col_list:\n",
    "            for index, row in df.iterrows():\n",
    "                index_name = index if index[:index.find(\"[\")] not in  ['L', 'S', 'L-S'] else \"\"\n",
    "                strat_data.loc[strat_,f\"{col}{index_name}\"] = df.loc[index,col]\n",
    "    data_check.append(strat_data)\n",
    "    #Save stats\n",
    "    # Position the dataframes in the worksheet.\n",
    "    strat_data.to_excel(Cost_writer, sheet_name=str(n_companies)+'OP_Costs_SentStart>'+strat,startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2b209",
   "metadata": {},
   "source": [
    "### Start EW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Saved Dataframe with Daily Sentiment classifications\n",
    "with open(\"5)InvestmentStrat/\"+file_name, \"rb\") as fp:  \n",
    "    Invest_FullData_Limited = pickle.load(fp)\n",
    "# Separate Data\n",
    "Ret_date_company = Invest_FullData_Limited[\"Ret\"]\n",
    "LagSent_date_company = Invest_FullData_Limited[\"LagSent\"]\n",
    "LagMktCap_date_company = Invest_FullData_Limited[\"LagMktCap\"]\n",
    "Invest_FullData = Invest_FullData_Limited[\"InvestData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct List of top and bottom companies for portfolio creation  \n",
    "#(auxiliary)\n",
    "n_companies = 100\n",
    "unique_rics = list(LagSent_date_company.columns)\n",
    "start_type_ = [\"current\",unique_rics,True]   # first element can be  \"current\"   or \"random\"\n",
    "Return_by_Strat_date = pd.DataFrame()  \n",
    "# Loop over different lambdas (percentage of positions to change) max 1 (= change everthing) and min 0 (=change nothing)\n",
    "for λ_ in range(1,10):\n",
    "    λ_/= 10\n",
    "    #Iterate over each type of strat and compute statistics\n",
    "    for strat_ in [\"L\", \"S\", \"L-S\"]:\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"] = LagSent_date_company.apply(lambda x: get_ric_pos_dict(strat_,n_companies,x.name,x,[Ret_date_company])  , axis=1)  \n",
    "        Return_by_Strat_date.sort_index(inplace=True)\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")NCompanies[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x:  {\"L\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"].values() if v == 1),\"S\":sum(1 for v in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"].values() if v == -1)} , axis=1)\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")RET_List[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x: get_ric_value_dict(x.name, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"], Ret_date_company), axis=1)\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_Universe[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x: get_universe_weight_dict(unique_rics, x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"], None),  axis=1)    \n",
    "                \n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Optimal_Weight_Universe[λ=\"+str(λ_)+\"]\"] = getFULL_optimal_universe_weight_dict(Return_by_Strat_date,n_companies,   strat_,start_type_, λ_) \n",
    "\n",
    "        Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Weight_List[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x: [x[strat_+\"(\"+str(n_companies)+\")Optimal_Weight_Universe[λ=\"+str(λ_)+\"]\"][key] for key in x[strat_+\"(\"+str(n_companies)+\")RIC_Pos_Dict[λ=\"+str(λ_)+\"]\"]], axis=1)\n",
    "        Return_by_Strat_date[strat_+\"[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x: sum(ret * w for ret, w in zip(x[strat_+\"(\"+str(n_companies)+\")RET_List[λ=\"+str(λ_)+\"]\"], x[strat_+\"(\"+str(n_companies)+\")Weight_List[λ=\"+str(λ_)+\"]\"])),  axis=1)\n",
    "        Return_by_Strat_date[strat_+\" Cumulative[λ=\"+str(λ_)+\"]\"] =  (Return_by_Strat_date[strat_+\"[λ=\"+str(λ_)+\"]\"]+1).cumprod()\n",
    "        Return_by_Strat_date[strat_+\" Costs[λ=\"+str(λ_)+\"]\"] = Return_by_Strat_date.apply(lambda x: get_transaction_costs(x.name, Return_by_Strat_date[strat_+\"(\"+str(n_companies)+\")Optimal_Weight_Universe[λ=\"+str(λ_)+\"]\"]), axis=1)\n",
    "        Return_by_Strat_date[strat_+\" Net[λ=\"+str(λ_)+\"]\"] =  Return_by_Strat_date[strat_+\"[λ=\"+str(λ_)+\"]\"] - Return_by_Strat_date[strat_+\" Costs[λ=\"+str(λ_)+\"]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with info for Stats\n",
    "Portfolio_Stats_Data = pd.merge(Return_by_Strat_date.reset_index(),Invest_FullData[[\"DateAdj\",'MktCap(t-1)','Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF','Mom']], how=\"inner\", on=\"DateAdj\")\n",
    "print(\"Data Final\", len(Portfolio_Stats_Data), \"Data Port\", len(Return_by_Strat_date), \"Data Baseline\", len(Invest_FullData))\n",
    "display(Portfolio_Stats_Data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b976963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strat Stats\n",
    "for strat in ['L', 'S', 'L-S']:\n",
    "    strat_data = pd.DataFrame()\n",
    "    for λ_ in range(1,10):\n",
    "        λ_/= 10\n",
    "        strat_ = strat+\"[λ=\"+str(λ_)+\"]\"\n",
    "        print( \"\\t Strat: \",strat_)    # name to show, column of returns, column of net returns, dataframe\n",
    "        df = paper_stats_table_Costs(strat_, strat_, strat+\" Net[λ=\"+str(λ_)+\"]\",  Portfolio_Stats_Data)\n",
    "        display(df)\n",
    "        col_list = list(df.columns)\n",
    "        for col in col_list:\n",
    "            for index, row in df.iterrows():\n",
    "                index_name = index if index[:index.find(\"[\")] not in  ['L', 'S', 'L-S'] else \"\"\n",
    "                strat_data.loc[strat_,f\"{col}{index_name}\"] = df.loc[index,col]\n",
    "    #Save stats\n",
    "    # Position the dataframes in the worksheet.\n",
    "    strat_data.to_excel(Cost_writer, sheet_name=str(n_companies)+'OP_Costs_EWStart>'+strat,startrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638335fc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Investment Strategy Testing Based on Other Paper Ideas\n",
    "\n",
    "\n",
    "<u> **1**) Strategies based on daily `relative` classification [3 types just long(**L**), just short(**S**) and long-short(**L-S**]:</u>\n",
    "> **Relative_Strat_1>** Each day go long(short) on top(bottom) 20% ranked stocks (=100 stocks) if there are signals for that amount. Meaning startegy can include less than 100 stocks but not more. Signal is based purely on previous day Sentiment (**1 var**)\n",
    "> **Relative_Strat_2>** Each day go long(short) on top(bottom) 20% ranked stocks (=100 stocks) if there are signal for that amount. Meaning startegy can include less than 100 stocks but not more. Signal is based on the alignment of Previous day sentiment, sentiment shock and sentiment trend. (**3 var**)\n",
    "> **Relative_Strat_2>** Each day go long(short) on top(bottom) 20% ranked stocks (=100 stocks) if there are signal for that amount. Meaning startegy can include less than 100 stocks but not more. Signal is based on the alignment of sentiment shock and sentimetn trend. (**2 var**)\n",
    "\n",
    "<u> **2**) Strategies based on daily `absolute` classification [3 types just long(**L**), just short(**S**) and long-short(**L-S**]:</u>\n",
    "> **Absolute_Strat_1>** Pick top winners (losers) (capped at 100) based on previous day sentiment (**1 var**)\n",
    "\n",
    "<u> **2**) Strategies Adjusted Less Frequently [3 types just long(**L**), just short(**S**) and long-short(**L-S**]:</u>\n",
    "> **Passive_Strat_1>** \n",
    "\n",
    "\n",
    "**Key** assumptions:\n",
    "> **Transaction Costs>** 20bp for round trip (average between Tetlock and News Based Traiding Reference Paper)\n",
    "> **No signal** assumption: If no signal is given then get risk free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afa032",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f71bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def stats_table(Strat_Returns):\n",
    "    #Main Metrics\n",
    "    Strat_Statistics.loc[\"Count\"] = len(Strat_Returns)\n",
    "    Strat_Statistics.loc[\"\"] = \"\"\n",
    "    Strat_Statistics.loc[\"Mean %\"] = np.average(Strat_Returns)*100*12\n",
    "    Strat_Statistics.loc[\"(mean) t-stat\"] = np.average(Strat_Returns) / (np.std(Strat_Returns)/ (Strat_Statistics.loc[\"Count\"].values[0]**0.5)) \n",
    "    Strat_Statistics.loc[\"(mean) p-value\"] = (1-norm.cdf(Strat_Statistics.loc[\"(mean) t-stat\"].values[0]))*2\n",
    "    Strat_Statistics.loc[\"-\"] = \"-\"\n",
    "    Strat_Statistics.loc[\"Std. Dev. (%)\"] = np.round(np.std(Strat_Returns)*100*(12**0.5),2)\n",
    "    Strat_Statistics.loc[\"--\"] = \"--\"\n",
    "    Strat_Statistics.loc[\"Sharpe ratio\"] = Strat_Statistics.loc[\"Mean %\"]/ Strat_Statistics.loc[\"Std. Dev. (%)\"]\n",
    "    Strat_Statistics.loc[\"(SR) t-stat\"] = (Strat_Statistics.loc[\"Sharpe ratio\"].values[0]/(12**0.5))/np.sqrt((1+0.5*(Strat_Statistics.loc[\"Sharpe ratio\"].values[0]/(12**0.5))**2)/Strat_Statistics.loc[\"Count\"].values[0])\n",
    "    Strat_Statistics.loc[\"(SR) p-value\"] = (1-norm.cdf(Strat_Statistics.loc[\"(SR) t-stat\"].values[0]))*2\n",
    "    Strat_Statistics.loc[\"---\"] = \"---\"\n",
    "    Strat_Statistics.loc[\"Skewness\"] = skew(Strat_Returns)\n",
    "    Strat_Statistics.loc[\"Kurtosis\"] = kurtosis(Strat_Returns)\n",
    "    Strat_Statistics.loc[\"----\"] = \"----\"\n",
    "    Strat_Statistics.loc[\"JB test statistic\"] = (Strat_Statistics.loc[\"Count\"].values[0]/6)*(Strat_Statistics.loc[\"Skewness\"].values[0]**2+0.25*(Strat_Statistics.loc[\"Kurtosis\"].values[0]**0.5))\n",
    "    Strat_Statistics.loc[\"(JB) p-value\"] = 1-chi2.cdf(Strat_Statistics.loc[\"JB test statistic\"].values[0], 2)\n",
    "    Strat_Statistics.loc[\"-----\"] = \"-----\"\n",
    "    Strat_Statistics.loc[\"Minimum (%)\"] = Strat_Returns.min()\n",
    "    Strat_Statistics.loc[\"Percentile 25 (%)\"] = np.percentile(Strat_Returns, 25)\n",
    "    Strat_Statistics.loc[\"Median (%)\"] = Strat_Returns.median()\n",
    "    Strat_Statistics.loc[\"Percentile 75 (%)\"] = np.percentile(Strat_Returns, 75)\n",
    "    Strat_Statistics.loc[\"Maximum (%)\"] = Strat_Returns.max()\n",
    "    Strat_Statistics.loc[\"------\"] = \"------\"\n",
    "    Strat_Statistics.loc[\"AR(1) (%)\"] = Strat_Returns.corr(Strat_Returns.shift(1))\n",
    "    Strat_Statistics.loc[\"p-value\"] = 2*(1-norm.cdf(Strat_Statistics.loc[\"Count\"].values[0]**0.5*np.abs(Strat_Statistics.loc[\"AR(1) (%)\"].values[0])))\n",
    "    return Strat_Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99baa845",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def FamaFrench(data, y_var):\n",
    "    X_Fama3Factors = data[['Mkt-RF', 'SMB', 'HML']]\n",
    "    Y_ExcessReturns = data[y_var] - data['RF']\n",
    "    X = sm.add_constant(X_Fama3Factors)\n",
    "    ff_model = sm.OLS(Y_ExcessReturns, X).fit()\n",
    "    print(ff_model.summary())\n",
    "    intercept, b1, b2, b3 = ff_model.params\n",
    "    return intercept, b1, b2, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3379c11c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Relative and Absolute Strats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921594f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### (1) Relative Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097186b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def decile(x,n_deciles):\n",
    "    try:\n",
    "        return pd.qcut(x, n_deciles, labels=False)\n",
    "    except Exception as e:\n",
    "        if x.isnull().all():\n",
    "            return x\n",
    "        print(e,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be70940",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def returns_adj_transaction_costs(strat_row,strat_data,sinal_type,cost):\n",
    "    date = strat_row.name\n",
    "    strat_date_iloc = strat_data.index.get_loc(date)\n",
    "    current_company_list = strat_data.iloc[strat_date_iloc][sinal_type+\"RIC_List\"] \n",
    "    \n",
    "    #Dealing with cases where no ranking provided\n",
    "    if len(current_company_list)==0:\n",
    "        return pd.Series([0,0], index=[sinal_type+\"NTransactions\", sinal_type+\"Cost\"])\n",
    "    else:\n",
    "        #(exception of first case where all positions have costs)\n",
    "        if strat_date_iloc==0:\n",
    "            n_transactions = len(current_company_list)\n",
    "            return pd.Series([n_transactions,cost], index=[sinal_type+\"NTransactions\", sinal_type+\"Cost\"])\n",
    "        else:\n",
    "            old_company_list = strat_data.iloc[strat_date_iloc-1][sinal_type+\"RIC_List\"]\n",
    "            n_unchanged_companies = len(list(set(old_company_list).intersection(current_company_list)))\n",
    "            n_transactions = (len(current_company_list)-n_unchanged_companies)\n",
    "            costs = n_transactions*cost/len(current_company_list) \n",
    "            return pd.Series([n_transactions,costs], index=[sinal_type+\"NTransactions\", sinal_type+\"Cost\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3e5af",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Relative_Strat_1\n",
    "(Just Previous Day Sentiment Variable)\n",
    "\n",
    "*For 100 (200 & 50) companies*\n",
    ">With Costs\n",
    "**Best Performers**: Long Strategy is profitable BUT Everything performs badly, highly driven by transaction costs.\n",
    "**Notes**: Not even reversal intuition can help to achieve good perforance\n",
    "\n",
    ">Without  Costs\n",
    "**Best Performers**: Long strategy is similar to benchmark, others result in negative returns. The opposite of short outperforms benchmark (see notes for hypothesis).\n",
    "**Notes**: It appears that negative sentiment creates panic but then bounces back within the next day so buying becomes more profitable (supports market reversal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d543485",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #Return by date and company\n",
    "Ret_date_company = Invest_FullData.pivot(index='Date', columns='RIC', values='RET')    \n",
    "Ret_date_company = Ret_date_company.iloc[1:]\n",
    "    #Lagged sent by date and company\n",
    "LagSent_date_company = Invest_FullData.pivot(index='Date', columns='RIC', values='Avg_Confidence(t-1)') \n",
    "LagSent_date_company = LagSent_date_company.iloc[1:]\n",
    "\n",
    "    #safety check\n",
    "dates_not_common = set(LagSent_date_company.index).symmetric_difference(set(Ret_date_company.index))\n",
    "print(f\"The following dates are not available on both sides: {dates_not_common}\")\n",
    "      \n",
    "    #Get Ranking of only negative signals (1st ranking = lowest sentiment | Lower number = bigger negative signal)\n",
    "NEG_LagSent_date_company = LagSent_date_company.copy()\n",
    "NEG_LagSent_date_company[NEG_LagSent_date_company>=0.5] = np.nan\n",
    "NEG_RANK_date_company_LagSent = NEG_LagSent_date_company.apply(lambda x: x.rank(method='first', ascending=True), axis=1) \n",
    "    #Get Ranking of only negative signals (1st ranking = highest sentiment | Lower number = bigger positive signal)\n",
    "POS_LagSent_date_company = LagSent_date_company.copy()\n",
    "POS_LagSent_date_company[POS_LagSent_date_company<=0.5] = np.nan\n",
    "POS_RANK_date_company_LagSent = POS_LagSent_date_company.apply(lambda x: x.rank(method='first', ascending=False), axis=1) \n",
    "      \n",
    "    #Construct lists of companies according to top and bottom signals\n",
    "        #(auxiliary)\n",
    "n_companies = 200\n",
    "Return_by_Strat_date = pd.DataFrame()\n",
    "    #(companies list, Nº of companies, Average return, N of transactions, Average Costs)\n",
    "for df, sinal_type in zip([NEG_RANK_date_company_LagSent,POS_RANK_date_company_LagSent],[\"Neg(\"+str(n_companies)+\")\",\"Pos(\"+str(n_companies)+\")\"]):\n",
    "        Return_by_Strat_date[sinal_type+\"RIC_List\"] = df.apply(lambda x: list(x[x<=n_companies].index), axis=1)\n",
    "        Return_by_Strat_date[sinal_type+\"NCompanies\"] = Return_by_Strat_date.apply(lambda x: len(x[sinal_type+\"RIC_List\"]), axis=1)\n",
    "        Return_by_Strat_date[sinal_type+\"Avg_Ret\"] = Return_by_Strat_date.apply(lambda x: np.nanmean(Ret_date_company.loc[x.name][Ret_date_company.loc[x.name].index.isin(x[sinal_type+\"RIC_List\"])]), axis=1)\n",
    "        Return_by_Strat_date[[sinal_type+\"NTransactions\", sinal_type+\"Cost\"]] =  Return_by_Strat_date.apply(lambda x: returns_adj_transaction_costs(x,Return_by_Strat_date,sinal_type,transaction_cost) , axis=1)\n",
    "\n",
    "# Strategy returns\n",
    "    #(daily return)\n",
    "Return_by_Strat_date[\"L\"] = (1) *Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Avg_Ret\"] - Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Cost\"]\n",
    "Return_by_Strat_date[\"S\"] = (-1) * Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Avg_Ret\"] - Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Cost\"]\n",
    "Return_by_Strat_date[\"L-S\"] = ((1)*Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Avg_Ret\"] + (-1)*Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Avg_Ret\"]) -(Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Cost\"]+Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Cost\"]) \n",
    "    #(cumulative return)\n",
    "Return_by_Strat_date.sort_index(inplace=True)\n",
    "Return_by_Strat_date[\"L Cumulative\"] =  (Return_by_Strat_date[\"L\"]+1).cumprod()\n",
    "Return_by_Strat_date[\"S Cumulative\"] = (Return_by_Strat_date[\"S\"]+1).cumprod()\n",
    "Return_by_Strat_date[\"L-S Cumulative\"] = (Return_by_Strat_date[\"L-S\"]+1).cumprod()\n",
    "Return_by_Strat_date.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf2acd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Baseline\n",
    "baseline = Buy_Hold[Buy_Hold[\"Date\"].isin(Return_by_Strat_date.index)]\n",
    "plt.plot(baseline[\"Date\"],baseline[\"Cumulative\"], label = \"Buy & Hold\", color='tab:red')\n",
    "#Strategy\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index, strat in enumerate([col for col in Return_by_Strat_date if \"Cumulative\" in col]):\n",
    "    plt.plot(Return_by_Strat_date.index,Return_by_Strat_date[strat], label = strat, color='tab:'+color[index])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"Relative strategy Based on Average_Set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93edfd2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Statistics Table\n",
    "statistics_table = pd.DataFrame()\n",
    "for column in [\"L\",\"S\",\"L-S\"]:\n",
    "    statistics_table[column] = stats_table(Return_by_Strat_date[column])\n",
    "display(statistics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa734b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Relevant data + Fama French\n",
    "StratReturn_and_Fama = pd.merge(Return_by_Strat_date.reset_index(),FF3_Data, how=\"left\", on=['Date'])\n",
    "FamaFrench(StratReturn_and_Fama, \"L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b9c2a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Relative_Strat_2\n",
    "(Previous Day Sent and Sentiment Shock and Trend Variables as signals)\n",
    "\n",
    ">With Costs\n",
    "**Best Performers**: Both Long and short have very similar performance, being positive despote the transaction costs. However they are below the Benchmark.\n",
    "**Notes**: \n",
    "\n",
    ">Without  Costs\n",
    "**Best Performers**: (With 7 days based shock and trend) Both Long and Short Startegies outpereform benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b3126",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #Return by date and company\n",
    "Ret_date_company = Invest_FullData.pivot(index='Date', columns='RIC', values='RET')    \n",
    "Ret_date_company = Ret_date_company.iloc[1:]\n",
    "    #Lagged sent by date and company\n",
    "LagSent_date_company = Invest_FullData.pivot(index='Date', columns='RIC', values='Avg_Confidence(t-1)') \n",
    "LagSent_date_company = LagSent_date_company.iloc[1:]\n",
    "    #Lagged sent shock by date and company\n",
    "shock_var = [col for col in Invest_FullData.columns if \"_Shock\" in col][0]\n",
    "LagSentShock_date_company = Invest_FullData.pivot(index='Date', columns='RIC', values=shock_var) \n",
    "LagSentShock_date_company = LagSentShock_date_company.iloc[1:]\n",
    "    #Lagged sent Trend by date and company\n",
    "trend_var = [col for col in Invest_FullData.columns if \"_Trend\" in col][0]\n",
    "LagSentTrend_date_company = Invest_FullData.pivot(index='Date', columns='RIC', values=trend_var) \n",
    "LagSentTrend_date_company = LagSentTrend_date_company.iloc[1:]\n",
    "\n",
    "    #safety check\n",
    "dates_not_common = set(LagSent_date_company.index).symmetric_difference(set(Ret_date_company.index))\n",
    "print(f\"The following dates are not available on both sides: {dates_not_common}\")\n",
    "    \n",
    "# Previous DaY Sentiment\n",
    "    #Get Ranking of only negative signals (1st ranking = lowest sentiment | Lower number = bigger negative signal)\n",
    "NEG_LagSent_date_company = LagSent_date_company.copy()\n",
    "NEG_LagSent_date_company[NEG_LagSent_date_company>=0.5] = np.nan\n",
    "NEG_RANK_date_company_LagSent = NEG_LagSent_date_company.apply(lambda x: x.rank(method='first', ascending=True), axis=1) \n",
    "    #Get Ranking of only negative signals (1st ranking = highest sentiment | Lower number = bigger positive signal)\n",
    "POS_LagSent_date_company = LagSent_date_company.copy()\n",
    "POS_LagSent_date_company[POS_LagSent_date_company<=0.5] = np.nan\n",
    "POS_RANK_date_company_LagSent = POS_LagSent_date_company.apply(lambda x: x.rank(method='first', ascending=False), axis=1) \n",
    "\n",
    "# Schock Sentiment\n",
    "    #Get Ranking of only negative signals (1st ranking = lowest sentiment | Lower number = bigger negative signal)\n",
    "NEG_LagSentShock_date_company = LagSentShock_date_company.copy()\n",
    "NEG_LagSentShock_date_company[NEG_LagSentShock_date_company>=0] = np.nan\n",
    "NEG_RANK_date_company_LagSentShock = NEG_LagSentShock_date_company.apply(lambda x: x.rank(method='first', ascending=True), axis=1) \n",
    "    #Get Ranking of only negative signals (1st ranking = highest sentiment | Lower number = bigger positive signal)\n",
    "POS_LagSentShock_company = LagSentShock_date_company.copy()\n",
    "POS_LagSentShock_company[POS_LagSentShock_company<=0] = np.nan\n",
    "POS_RANK_date_company_LagSentShock = POS_LagSentShock_company.apply(lambda x: x.rank(method='first', ascending=False), axis=1) \n",
    "\n",
    "# Trend Sentiment\n",
    "    #Get Ranking of only negative signals (1st ranking = lowest sentiment | Lower number = bigger negative signal)\n",
    "NEG_LagSentTrend_date_company = LagSentTrend_date_company.copy()\n",
    "NEG_LagSentTrend_date_company[NEG_LagSentTrend_date_company>=0] = np.nan\n",
    "NEG_RANK_date_company_LagSentTrend = NEG_LagSentTrend_date_company.apply(lambda x: x.rank(method='first', ascending=True), axis=1) \n",
    "    #Get Ranking of only negative signals (1st ranking = highest sentiment | Lower number = bigger positive signal)\n",
    "POS_LagSentTrend_company = LagSentTrend_date_company.copy()\n",
    "POS_LagSentTrend_company[POS_LagSentTrend_company<=0] = np.nan\n",
    "POS_RANK_date_company_LagSentTrend = POS_LagSentTrend_company.apply(lambda x: x.rank(method='first', ascending=False), axis=1) \n",
    "\n",
    "    #Joined Ranking (based on mean of all rankings)\n",
    "NegRANK_date_company = pd.concat([NEG_RANK_date_company_LagSent,NEG_RANK_date_company_LagSentShock,NEG_RANK_date_company_LagSentTrend]).groupby(\"Date\").agg(np.nanmean)\n",
    "POSRANK_date_company = pd.concat([POS_RANK_date_company_LagSent,POS_RANK_date_company_LagSentShock,POS_RANK_date_company_LagSentTrend]).groupby(\"Date\").agg(np.nanmean)\n",
    "\n",
    "\n",
    "    #Construct lists of companies according to top and bottom signals\n",
    "        #(auxiliary)\n",
    "n_companies = 200\n",
    "Return_by_Strat_date = pd.DataFrame()\n",
    "    #(companies list, Nº of companies, Average return, N of transactions, Average Costs)\n",
    "for df, sinal_type in zip([NegRANK_date_company,POSRANK_date_company],[\"Neg(\"+str(n_companies)+\")\",\"Pos(\"+str(n_companies)+\")\"]):\n",
    "        Return_by_Strat_date[sinal_type+\"RIC_List\"] = df.apply(lambda x: list(x[x<=n_companies].index), axis=1)\n",
    "        Return_by_Strat_date[sinal_type+\"NCompanies\"] = Return_by_Strat_date.apply(lambda x: len(x[sinal_type+\"RIC_List\"]), axis=1)\n",
    "        Return_by_Strat_date[sinal_type+\"Avg_Ret\"] = Return_by_Strat_date.apply(lambda x: np.nanmean(Ret_date_company.loc[x.name][Ret_date_company.loc[x.name].index.isin(x[sinal_type+\"RIC_List\"])]), axis=1)\n",
    "        Return_by_Strat_date[[sinal_type+\"NTransactions\", sinal_type+\"Cost\"]] =  Return_by_Strat_date.apply(lambda x: returns_adj_transaction_costs(x,Return_by_Strat_date,sinal_type,transaction_cost) , axis=1)\n",
    "\n",
    "# Strategy returns\n",
    "    #(daily return)\n",
    "Return_by_Strat_date[\"L\"] = (1) *Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Avg_Ret\"] - Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Cost\"]\n",
    "Return_by_Strat_date[\"S\"] = (1) * Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Avg_Ret\"] - Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Cost\"]\n",
    "Return_by_Strat_date[\"L-S\"] = ((1)*Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Avg_Ret\"] + (-1)*Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Avg_Ret\"]) -(Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Cost\"]+Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Cost\"]) \n",
    "    #(cumulative return)\n",
    "Return_by_Strat_date.sort_index(inplace=True)\n",
    "Return_by_Strat_date[\"L Cumulative\"] =  (Return_by_Strat_date[\"L\"]+1).cumprod()\n",
    "Return_by_Strat_date[\"S Cumulative\"] = (Return_by_Strat_date[\"S\"]+1).cumprod()\n",
    "Return_by_Strat_date[\"L-S Cumulative\"] = (Return_by_Strat_date[\"L-S\"]+1).cumprod()\n",
    "Return_by_Strat_date.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae7eb3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Baseline\n",
    "baseline = Buy_Hold[Buy_Hold[\"Date\"].isin(Return_by_Strat_date.index)]\n",
    "plt.plot(baseline[\"Date\"],baseline[\"Cumulative\"], label = \"Buy & Hold\", color='tab:red')\n",
    "#Strategy\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index, strat in enumerate([col for col in Return_by_Strat_date if \"Cumulative\" in col]):\n",
    "    plt.plot(Return_by_Strat_date.index,Return_by_Strat_date[strat], label = strat, color='tab:'+color[index])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"Relative strategy Based on Average_Set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df01716c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Statistics Table\n",
    "statistics_table = pd.DataFrame()\n",
    "for column in [\"L\",\"S\",\"L-S\"]:\n",
    "    statistics_table[column] = stats_table(Return_by_Strat_date[column])\n",
    "display(statistics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb486e2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Relevant data + Fama French\n",
    "StratReturn_and_Fama = pd.merge(Return_by_Strat_date.reset_index(),FF3_Data, how=\"left\", on=['Date'])\n",
    "FamaFrench(StratReturn_and_Fama, \"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17bf80",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Relative_Strat_3\n",
    "(Sentiment Shock and Trend Variables as signals)\n",
    "\n",
    ">With Costs\n",
    "**Best Performers**: \n",
    "**Notes**: \n",
    "\n",
    ">Without  Costs\n",
    "**Best Performers**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b2413",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #Return by date and company\n",
    "Ret_date_company = Invest_FullData.pivot(index='Date', columns='RIC', values='RET')    \n",
    "Ret_date_company = Ret_date_company.iloc[1:]\n",
    "    #Lagged sent shock by date and company\n",
    "shock_var = [col for col in Invest_FullData.columns if \"_Shock\" in col][0]\n",
    "LagSentShock_date_company = Invest_FullData.pivot(index='Date', columns='RIC', values=shock_var) \n",
    "LagSentShock_date_company = LagSentShock_date_company.iloc[1:]\n",
    "    #Lagged sent Trend by date and company\n",
    "trend_var = [col for col in Invest_FullData.columns if \"_Trend\" in col][0]\n",
    "LagSentTrend_date_company = Invest_FullData.pivot(index='Date', columns='RIC', values=trend_var) \n",
    "LagSentTrend_date_company = LagSentTrend_date_company.iloc[1:]\n",
    "\n",
    "    #safety check\n",
    "dates_not_common = set(LagSent_date_company.index).symmetric_difference(set(Ret_date_company.index))\n",
    "print(f\"The following dates are not available on both sides: {dates_not_common}\")\n",
    "\n",
    "\n",
    "# Schock Sentiment\n",
    "    #Get Ranking of only negative signals (1st ranking = lowest sentiment | Lower number = bigger negative signal)\n",
    "NEG_LagSentShock_date_company = LagSentShock_date_company.copy()\n",
    "NEG_LagSentShock_date_company[NEG_LagSentShock_date_company>=0] = np.nan\n",
    "NEG_RANK_date_company_LagSentShock = NEG_LagSentShock_date_company.apply(lambda x: x.rank(method='first', ascending=True), axis=1) \n",
    "    #Get Ranking of only negative signals (1st ranking = highest sentiment | Lower number = bigger positive signal)\n",
    "POS_LagSentShock_company = LagSentShock_date_company.copy()\n",
    "POS_LagSentShock_company[POS_LagSentShock_company<=0] = np.nan\n",
    "POS_RANK_date_company_LagSentShock = POS_LagSentShock_company.apply(lambda x: x.rank(method='first', ascending=False), axis=1) \n",
    "\n",
    "# Trend Sentiment\n",
    "    #Get Ranking of only negative signals (1st ranking = lowest sentiment | Lower number = bigger negative signal)\n",
    "NEG_LagSentTrend_date_company = LagSentTrend_date_company.copy()\n",
    "NEG_LagSentTrend_date_company[NEG_LagSentTrend_date_company>=0] = np.nan\n",
    "NEG_RANK_date_company_LagSentTrend = NEG_LagSentTrend_date_company.apply(lambda x: x.rank(method='first', ascending=True), axis=1) \n",
    "    #Get Ranking of only negative signals (1st ranking = highest sentiment | Lower number = bigger positive signal)\n",
    "POS_LagSentTrend_company = LagSentTrend_date_company.copy()\n",
    "POS_LagSentTrend_company[POS_LagSentTrend_company<=0] = np.nan\n",
    "POS_RANK_date_company_LagSentTrend = POS_LagSentTrend_company.apply(lambda x: x.rank(method='first', ascending=False), axis=1) \n",
    "\n",
    "    #Joined Ranking (based on mean of all rankings)\n",
    "NegRANK_date_company = pd.concat([NEG_RANK_date_company_LagSentShock,NEG_RANK_date_company_LagSentTrend]).groupby(\"Date\").agg(np.nanmean)\n",
    "POSRANK_date_company = pd.concat([POS_RANK_date_company_LagSentShock,POS_RANK_date_company_LagSentTrend]).groupby(\"Date\").agg(np.nanmean)\n",
    "\n",
    "\n",
    "    #Construct lists of companies according to top and bottom signals\n",
    "        #(auxiliary)\n",
    "n_companies = 200\n",
    "Return_by_Strat_date = pd.DataFrame()\n",
    "    #(companies list, Nº of companies, Average return, N of transactions, Average Costs)\n",
    "for df, sinal_type in zip([NegRANK_date_company,POSRANK_date_company],[\"Neg(\"+str(n_companies)+\")\",\"Pos(\"+str(n_companies)+\")\"]):\n",
    "        Return_by_Strat_date[sinal_type+\"RIC_List\"] = df.apply(lambda x: list(x[x<=n_companies].index), axis=1)\n",
    "        Return_by_Strat_date[sinal_type+\"NCompanies\"] = Return_by_Strat_date.apply(lambda x: len(x[sinal_type+\"RIC_List\"]), axis=1)\n",
    "        Return_by_Strat_date[sinal_type+\"Avg_Ret\"] = Return_by_Strat_date.apply(lambda x: np.nanmean(Ret_date_company.loc[x.name][Ret_date_company.loc[x.name].index.isin(x[sinal_type+\"RIC_List\"])]), axis=1)\n",
    "        Return_by_Strat_date[[sinal_type+\"NTransactions\", sinal_type+\"Cost\"]] =  Return_by_Strat_date.apply(lambda x: returns_adj_transaction_costs(x,Return_by_Strat_date,sinal_type,transaction_cost) , axis=1)\n",
    "\n",
    "# Strategy returns\n",
    "    #(daily return)\n",
    "Return_by_Strat_date[\"L\"] = (1) *Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Avg_Ret\"] - Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Cost\"]\n",
    "Return_by_Strat_date[\"S\"] = (1) * Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Avg_Ret\"] - Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Cost\"]\n",
    "Return_by_Strat_date[\"L-S\"] = ((1)*Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Avg_Ret\"] + (-1)*Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Avg_Ret\"]) -(Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Cost\"]+Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Cost\"]) \n",
    "    #(cumulative return)\n",
    "Return_by_Strat_date.sort_index(inplace=True)\n",
    "Return_by_Strat_date[\"L Cumulative\"] =  (Return_by_Strat_date[\"L\"]+1).cumprod()\n",
    "Return_by_Strat_date[\"S Cumulative\"] = (Return_by_Strat_date[\"S\"]+1).cumprod()\n",
    "Return_by_Strat_date[\"L-S Cumulative\"] = (Return_by_Strat_date[\"L-S\"]+1).cumprod()\n",
    "Return_by_Strat_date.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1960fb0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Baseline\n",
    "baseline = Buy_Hold[Buy_Hold[\"Date\"].isin(Return_by_Strat_date.index)]\n",
    "plt.plot(baseline[\"Date\"],baseline[\"Cumulative\"], label = \"Buy & Hold\", color='tab:red')\n",
    "#Strategy\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index, strat in enumerate([col for col in Return_by_Strat_date if \"Cumulative\" in col]):\n",
    "    plt.plot(Return_by_Strat_date.index,Return_by_Strat_date[strat], label = strat, color='tab:'+color[index])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"Relative strategy Based on Average_Set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf4d7d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Statistics Table\n",
    "statistics_table = pd.DataFrame()\n",
    "for column in [\"L\",\"S\",\"L-S\"]:\n",
    "    statistics_table[column] = stats_table(Return_by_Strat_date[column])\n",
    "display(statistics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bd11a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Relevant data + Fama French\n",
    "StratReturn_and_Fama = pd.merge(Return_by_Strat_date.reset_index(),FF3_Data, how=\"left\", on=['Date'])\n",
    "FamaFrench(StratReturn_and_Fama, \"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4dad7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### (2) Absolute Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804e805",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Absolute_Strat_1\n",
    "(Just Previous Day Sentiment Variable)\n",
    "\n",
    "*For 100 companies*\n",
    ">With Costs\n",
    "**Best Performers**: All perform BAd. However short startegy starts with a relatively good performance\n",
    "\n",
    ">Without  Costs\n",
    "**Best Performers**: Long strategy, very similar to benchmark. The reversal of short startegy has best results.\n",
    "**Notes**: Appear to be much more volatile than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a4ab2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #Return by date and company\n",
    "Ret_date_company = Invest_FullData.pivot(index='Date', columns='RIC', values='RET')    \n",
    "Ret_date_company = Ret_date_company.iloc[1:]\n",
    "    #Lagged sent by date and company\n",
    "LagSent_date_company = Invest_FullData.pivot(index='Date', columns='RIC', values='Avg_Confidence(t-1)') \n",
    "LagSent_date_company = LagSent_date_company.iloc[1:]\n",
    "\n",
    "    #safety check\n",
    "dates_not_common = set(LagSent_date_company.index).symmetric_difference(set(Ret_date_company.index))\n",
    "print(f\"The following dates are not available on both sides: {dates_not_common}\")\n",
    "      \n",
    "    #Get Ranking of only top negative signals (1st ranking = lowest sentiment | Lower number = bigger negative signal)\n",
    "neg_threshold = 0.2\n",
    "NEG_LagSent_date_company = LagSent_date_company.copy()\n",
    "NEG_LagSent_date_company[NEG_LagSent_date_company>neg_threshold] = np.nan\n",
    "NEG_RANK_date_company_LagSent = NEG_LagSent_date_company.apply(lambda x: x.rank(method='first', ascending=True), axis=1) \n",
    "    #Get Ranking of only top negative signals (1st ranking = highest sentiment | Lower number = bigger positive signal)\n",
    "pos_threshold = 0.80\n",
    "POS_LagSent_date_company = LagSent_date_company.copy()\n",
    "POS_LagSent_date_company[POS_LagSent_date_company<pos_threshold] = np.nan\n",
    "POS_RANK_date_company_LagSent = POS_LagSent_date_company.apply(lambda x: x.rank(method='first', ascending=False), axis=1) \n",
    "      \n",
    "    #Construct lists of companies according to top and bottom signals\n",
    "        #(auxiliary)\n",
    "n_companies = 50\n",
    "Return_by_Strat_date = pd.DataFrame()\n",
    "    #(companies list, Nº of companies, Average return, N of transactions, Average Costs)\n",
    "for df, sinal_type in zip([NEG_RANK_date_company_LagSent,POS_RANK_date_company_LagSent],[\"Neg(\"+str(n_companies)+\")\",\"Pos(\"+str(n_companies)+\")\"]):\n",
    "        Return_by_Strat_date[sinal_type+\"RIC_List\"] = df.apply(lambda x: list(x[x<=n_companies].index), axis=1)\n",
    "        Return_by_Strat_date[sinal_type+\"NCompanies\"] = Return_by_Strat_date.apply(lambda x: len(x[sinal_type+\"RIC_List\"]), axis=1)\n",
    "        Return_by_Strat_date[sinal_type+\"Avg_Ret\"] = Return_by_Strat_date.apply(lambda x: np.nanmean(Ret_date_company.loc[x.name][Ret_date_company.loc[x.name].index.isin(x[sinal_type+\"RIC_List\"])]), axis=1)\n",
    "        Return_by_Strat_date[[sinal_type+\"NTransactions\", sinal_type+\"Cost\"]] =  Return_by_Strat_date.apply(lambda x: returns_adj_transaction_costs(x,Return_by_Strat_date,sinal_type,transaction_cost) , axis=1)\n",
    "        #For no allocation put on risk free (for this time period is zero)\n",
    "        Return_by_Strat_date[sinal_type+\"Avg_Ret\"].fillna(0, inplace=True)\n",
    "        \n",
    "# Strategy returns\n",
    "    #(daily return)\n",
    "Return_by_Strat_date[\"L\"] = (1) *Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Avg_Ret\"] - Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Cost\"]\n",
    "Return_by_Strat_date[\"S\"] = (1) * Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Avg_Ret\"] - Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Cost\"]\n",
    "Return_by_Strat_date[\"L-S\"] = ((1)*Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Avg_Ret\"] + (-1)*Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Avg_Ret\"]) #-(Return_by_Strat_date[\"Pos(\"+str(n_companies)+\")Cost\"]+Return_by_Strat_date[\"Neg(\"+str(n_companies)+\")Cost\"]) \n",
    "    #(cumulative return)\n",
    "Return_by_Strat_date.sort_index(inplace=True)\n",
    "Return_by_Strat_date[\"L Cumulative\"] =  (Return_by_Strat_date[\"L\"]+1).cumprod()\n",
    "Return_by_Strat_date[\"S Cumulative\"] = (Return_by_Strat_date[\"S\"]+1).cumprod()\n",
    "Return_by_Strat_date[\"L-S Cumulative\"] = (Return_by_Strat_date[\"L-S\"]+1).cumprod()\n",
    "Return_by_Strat_date.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc370fd8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Baseline\n",
    "baseline = Buy_Hold[Buy_Hold[\"Date\"].isin(Return_by_Strat_date.index)]\n",
    "plt.plot(baseline[\"Date\"],baseline[\"Cumulative\"], label = \"Buy & Hold\", color='tab:red')\n",
    "#Strategy\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index, strat in enumerate([col for col in Return_by_Strat_date if \"Cumulative\" in col]):\n",
    "    plt.plot(Return_by_Strat_date.index,Return_by_Strat_date[strat], label = strat, color='tab:'+color[index])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"Relative strategy Based on Average_Set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146166b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Statistics Table\n",
    "statistics_table = pd.DataFrame()\n",
    "for column in [\"L\",\"S\",\"L-S\"]:\n",
    "    statistics_table[column] = stats_table(Return_by_Strat_date[column])\n",
    "display(statistics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1aa76",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Relevant data + Fama French\n",
    "StratReturn_and_Fama = pd.merge(Return_by_Strat_date.reset_index(),FF3_Data, how=\"left\", on=['Date'])\n",
    "FamaFrench(StratReturn_and_Fama, \"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f92f4c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d783c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad84748",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7ff1712",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1b333",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #Return by date and company\n",
    "Ret_date_company = Invest_Data.pivot(index='Date', columns='RIC', values='RET')     \n",
    "\n",
    "    #Lagged sent by date and company\n",
    "LagSent_date_company = Invest_Data.pivot(index='Date', columns='RIC', values='Avg_Confidence(t-1)') \n",
    "LagSent_date_company.dropna(how=\"all\", inplace=True)\n",
    "#LagSent_date_company.fillna(0.5, inplace=True)  #0.5 is the proxy to neutral\n",
    "\n",
    "    #Kepp only dates in common\n",
    "relevant_dates = Ret_date_company.index.intersection(LagSent_date_company.index)\n",
    "Ret_date_company = Ret_date_company.loc[relevant_dates]\n",
    "LagSent_date_company = LagSent_date_company.loc[relevant_dates]\n",
    "\n",
    "    #Average returns for companies > threshold\n",
    "threshold_start = 0.8\n",
    "Return_long_threshold = pd.DataFrame()\n",
    "for t in range(int(threshold_start*100),100,5):\n",
    "        threshold = t/100\n",
    "        Return_long_threshold[\"Long>=\"+str(threshold)] =  Ret_date_company.apply(lambda x: np.average(x[x.index.isin(list(LagSent_date_company.loc[x.name][LagSent_date_company.loc[x.name]>=threshold].index))]) , axis=1)\n",
    "        Return_long_threshold.sort_index(inplace=True)\n",
    "        Return_long_threshold[\"Long>=\"+str(threshold) + \" Cumulative\"] = (Return_long_threshold[\"Long>=\"+str(threshold)]+1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18d9bb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Statistics Table\n",
    "statistics_table = pd.DataFrame()\n",
    "for column in [col for col in Return_long_threshold.columns if (\"Cumulative\" not in col)]:\n",
    "    statistics_table[column] = stats_table(Return_long_threshold[column])\n",
    "display(statistics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db81b978",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Baseline\n",
    "baseline = Buy_Hold[Buy_Hold[\"Date\"].isin(Return_by_decile_date.index)]\n",
    "plt.plot(baseline[\"Date\"],baseline[\"Cumulative\"], label = \"Buy & Hold\", color='tab:red')\n",
    "#Strategy\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index, t in enumerate(range(int(threshold_start*100),100,5)):\n",
    "    threshold = t/100\n",
    "    plt.plot(Return_long_threshold.index,Return_long_threshold[\"Long>=\"+str(threshold) + \" Cumulative\"], label = \"Long>\"+str(threshold), color='tab:'+color[index])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"Cumulative  Buy & Hold  VS  Strategy 1\")\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc5dec",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Startegy 2\n",
    ">Better Results if Long Worst News\n",
    "\n",
    ">Hypothesis: Hype Dies Down (market rallys as a good opportunity to acquire a company expected to go back up in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b90ea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "  #Return by date and company\n",
    "Ret_date_company = Invest_Data.pivot(index='Date', columns='RIC', values='RET')     \n",
    "\n",
    "    #Lagged sent by date and company\n",
    "LagSent_date_company = Invest_Data.pivot(index='Date', columns='RIC', values='Avg_Confidence(t-1)') \n",
    "LagSent_date_company.dropna(how=\"all\", inplace=True)\n",
    "#LagSent_date_company.fillna(0.5, inplace=True)  #0.5 is the proxy to neutral\n",
    "\n",
    "    #Kepp only dates in common\n",
    "relevant_dates = Ret_date_company.index.intersection(LagSent_date_company.index)\n",
    "Ret_date_company = Ret_date_company.loc[relevant_dates]\n",
    "LagSent_date_company = LagSent_date_company.loc[relevant_dates]\n",
    "\n",
    "    #Average returns for companies > threshold\n",
    "threshold_start = 0.2\n",
    "Return_short_threshold = pd.DataFrame()\n",
    "for t in range(int(threshold_start*100),0,-5):\n",
    "        threshold = t/100\n",
    "        Return_short_threshold[\"Short<=\"+str(threshold)] =  Ret_date_company.apply(lambda x: -np.average(x[x.index.isin(list(LagSent_date_company.loc[x.name][LagSent_date_company.loc[x.name]<=threshold].index))]) , axis=1)\n",
    "        Return_short_threshold.sort_index(inplace=True)\n",
    "        Return_short_threshold.fillna(0, inplace=True) #Put returns as zero (change to risk free)\n",
    "        Return_short_threshold[\"Short<=\"+str(threshold) + \" Cumulative\"] = (Return_short_threshold[\"Short<=\"+str(threshold)]+1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f187838",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Check Nans\n",
    "Return_short_threshold[Return_short_threshold.isna().sum(axis=1)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96149418",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Baseline\n",
    "baseline = Buy_Hold[Buy_Hold[\"Date\"].isin(Return_by_decile_date.index)]\n",
    "plt.plot(baseline[\"Date\"],baseline[\"Cumulative\"], label = \"Buy & Hold\", color='tab:red')\n",
    "#Strategy\n",
    "color = [\"grey\",\"blue\",\"green\",\"cyan\",\"magenta\",\"yellow\",\"black\"]\n",
    "for index, t in enumerate(range(int(threshold_start*100),0,-5)):\n",
    "    threshold = t/100\n",
    "    plt.plot(Return_short_threshold.index,Return_short_threshold[\"Short<=\"+str(threshold) + \" Cumulative\"], label = \"Short<=\"+str(threshold), color='tab:'+color[index])\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"Cumulative  Buy & Hold  VS  Strategy 2\")\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f364756",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Startegy 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0236b817",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #Return by date and company\n",
    "Ret_date_company = Invest_Data.pivot(index='Date', columns='RIC', values='RET')     \n",
    "\n",
    "    #Lagged sent by date and company\n",
    "LagSent_date_company = Invest_Data.pivot(index='Date', columns='RIC', values='Avg_Confidence(t-1)') \n",
    "LagSent_date_company.dropna(how=\"all\", inplace=True)\n",
    "#LagSent_date_company.fillna(0.5, inplace=True)  #0.5 is the proxy to neutral\n",
    "\n",
    "    #Company decile rank by day \n",
    "n_deciles = 20\n",
    "RANK_date_company_LagSent = LagSent_date_company.apply(lambda x: pd.qcut(x, n_deciles, labels=False), axis=1) \n",
    "\n",
    "    #Kepp only dates in common\n",
    "relevant_dates = Ret_date_company.index.intersection(RANK_date_company_LagSent.index)\n",
    "Ret_date_company = Ret_date_company.loc[relevant_dates]\n",
    "RANK_date_company_LagSent = RANK_date_company_LagSent.loc[relevant_dates]\n",
    "\n",
    "    #Average returns per decile\n",
    "Return_by_decile_date = pd.DataFrame()\n",
    "for i in range(0,n_deciles):\n",
    "    #For specific date gets list of companies at given rank and then get for the same day the returns of said companies and averages them\n",
    "        Return_by_decile_date[\"Rank(\"+ str(i) +\")\"] =  Ret_date_company.apply(lambda x: np.average(x[x.index.isin(list(RANK_date_company_LagSent.loc[x.name][RANK_date_company_LagSent.loc[x.name]==i].index))]) , axis=1)\n",
    "# Stretgy returns\n",
    "Return_by_decile_date[\"Long-Short\"] = Return_by_decile_date[\"Rank(\"+ str(n_deciles-1) + \")\"]  - Return_by_decile_date[\"Rank(0)\"] \n",
    "Return_by_decile_date[\"Short-Long\"] = Return_by_decile_date[\"Rank(0)\"]  - Return_by_decile_date[\"Rank(\"+ str(n_deciles-1) + \")\"]  \n",
    "Return_by_decile_date.sort_index(inplace=True)\n",
    "Return_by_decile_date[\"Long-Short Cumulative\"] = (Return_by_decile_date[\"Long-Short\"]+1).cumprod()\n",
    "Return_by_decile_date[\"Short-Long Cumulative\"] = (Return_by_decile_date[\"Short-Long\"]+1).cumprod()\n",
    "Return_by_decile_date.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139ac3e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Baseline\n",
    "baseline = Buy_Hold[Buy_Hold[\"Date\"].isin(Return_by_decile_date.index)]\n",
    "plt.plot(baseline[\"Date\"],baseline[\"Cumulative\"], label = \"Buy & Hold\", color='tab:red')\n",
    "#Strategy\n",
    "plt.plot(Return_by_decile_date.index,Return_by_decile_date[\"Long-Short Cumulative\"], label = \"Long-Short\", color='tab:blue')\n",
    "plt.plot(Return_by_decile_date.index,Return_by_decile_date[\"Short-Long Cumulative\"], label = \"Short-Long\", color='tab:grey')\n",
    "plt.xlabel('Date')\n",
    "plt.title(\"Cumulative  Buy & Hold  VS  Strategy 3\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754932ba",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Strat_Statistics = pd.DataFrame(columns=[\"Stats\"])\n",
    "Strat_Returns = Return_by_decile_date[\"Long-Short\"].copy()\n",
    "#Main Metrics\n",
    "Strat_Statistics.loc[\"Count\"] = len(Strat_Returns)\n",
    "Strat_Statistics.loc[\"\"] = \"\"\n",
    "Strat_Statistics.loc[\"Mean %\"] = np.average(Strat_Returns)*100*12\n",
    "Strat_Statistics.loc[\"(mean) t-stat\"] = np.average(Strat_Returns) / (np.std(Strat_Returns)/ (Strat_Statistics.loc[\"Count\"].values[0]**0.5)) \n",
    "Strat_Statistics.loc[\"(mean) p-value\"] = (1-norm.cdf(Strat_Statistics.loc[\"(mean) t-stat\"].values[0]))*2\n",
    "Strat_Statistics.loc[\"-\"] = \"-\"\n",
    "Strat_Statistics.loc[\"Std. Dev. (%)\"] = np.round(np.std(Strat_Returns)*100*(12**0.5),2)\n",
    "Strat_Statistics.loc[\"--\"] = \"--\"\n",
    "Strat_Statistics.loc[\"Sharpe ratio\"] = Strat_Statistics.loc[\"Mean %\"]/ Strat_Statistics.loc[\"Std. Dev. (%)\"]\n",
    "Strat_Statistics.loc[\"(SR) t-stat\"] = (Strat_Statistics.loc[\"Sharpe ratio\"].values[0]/(12**0.5))/np.sqrt((1+0.5*(Strat_Statistics.loc[\"Sharpe ratio\"].values[0]/(12**0.5))**2)/Strat_Statistics.loc[\"Count\"].values[0])\n",
    "Strat_Statistics.loc[\"(SR) p-value\"] = (1-norm.cdf(Strat_Statistics.loc[\"(SR) t-stat\"].values[0]))*2\n",
    "Strat_Statistics.loc[\"---\"] = \"---\"\n",
    "Strat_Statistics.loc[\"Skewness\"] = skew(Strat_Returns)\n",
    "Strat_Statistics.loc[\"Kurtosis\"] = kurtosis(Strat_Returns)\n",
    "Strat_Statistics.loc[\"----\"] = \"----\"\n",
    "Strat_Statistics.loc[\"JB test statistic\"] = (Strat_Statistics.loc[\"Count\"].values[0]/6)*(Strat_Statistics.loc[\"Skewness\"].values[0]**2+0.25*(Strat_Statistics.loc[\"Kurtosis\"].values[0]**0.5))\n",
    "Strat_Statistics.loc[\"(JB) p-value\"] = 1-chi2.cdf(Strat_Statistics.loc[\"JB test statistic\"].values[0], 2)\n",
    "Strat_Statistics.loc[\"-----\"] = \"-----\"\n",
    "Strat_Statistics.loc[\"Minimum (%)\"] = Strat_Returns.min()\n",
    "Strat_Statistics.loc[\"Percentile 25 (%)\"] = np.percentile(Strat_Returns, 25)\n",
    "Strat_Statistics.loc[\"Median (%)\"] = Strat_Returns.median()\n",
    "Strat_Statistics.loc[\"Percentile 75 (%)\"] = np.percentile(Strat_Returns, 75)\n",
    "Strat_Statistics.loc[\"Maximum (%)\"] = Strat_Returns.max()\n",
    "Strat_Statistics.loc[\"------\"] = \"------\"\n",
    "Strat_Statistics.loc[\"AR(1) (%)\"] = Strat_Returns.corr(Strat_Returns.shift(1))\n",
    "Strat_Statistics.loc[\"p-value\"] = 2*(1-norm.cdf(Strat_Statistics.loc[\"Count\"].values[0]**0.5*np.abs(Strat_Statistics.loc[\"AR(1) (%)\"].values[0])))\n",
    "#Displaying\n",
    "display(Strat_Statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0a7eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
